%% Use the hmcposter class with the thesis document-class option.
\documentclass[thesis]{hmcposter}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{booktabs}
\usepackage{subfig}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{url}

\usepackage{enumitem}
\setitemize{labelsep=0.4em,leftmargin=1em}
\setlength{\parindent}{35pt}


\usepackage{util}
\usepackage{math}
\usepackage{kbordermatrix}

%\definecolor{cterm}{cmyk}{0,.80,1,0}
\definecolor{cterm}{rgb}{0,0.45,0.2}

\newcommand{\mterm}[1]{\textbf{\textcolor{cterm}{#1}}}
%\newcommand*{\mterm}[1]{\textbf{#1}}



\newcommand*{\STB}{\::\:}
\newcommand*{\mle}{\mathrm{mle}}
\newcommand*{\halfspace}{\vspace{0.5\baselineskip}}
\newcommand*{\nhalfspace}{\vspace{-0.5\baselineskip}}
\newcommand*{\nquarterspace}{\vspace{-0.25\baselineskip}}


\author{Aaron Pribadi}
\posteryear{2012}
\title{Algebraic Methods for Log-Linear Models}
\class{Math 197: Senior Thesis}

% Advisor(s) name or names.  Separate with \and.
\advisor{Michael Orrison}
% Reader(s) name or names.  Separate with \and.
\reader{Weiqing Gu}

%\copyrightholder{}


%% Define the \BibTeX command, used in our example document.
\providecommand{\bibtex}{{\rmfamily B\kern-.05em%
    \textsc{i\kern-.025em b}\kern-.08em%
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\pagestyle{fancy}

\begin{document}

\begin{poster}

\section{Introduction}
\nhalfspace

Problems concerning genetics, image recognition, natural language processing,
voting, and product recommendation hinge on making sense of discrete data.

Such problems pose challenges, but also offer rich mathematical structures.  We
investigate log-linear models, a type that interacts particularly well with
methods from geometry and algebra.

\nhalfspace
\section{Discrete Data}
\nhalfspace

Let $\xs$ be a finite sample space.  

Observations $X_1, X_2, \ldots, X_m$ are i.i.d. random variables
from a true distribution $X_i \sim p$.

\halfspace
\begin{figure}
    \centering
    \small
\begin{verbatim}
1) democrat   n y y n y y n n n n n n y y y y
2) republican n y n y y y n n n n n y y y n y
3) democrat   y y y n n n y y y n y n n n y y
4) democrat   y y y n n n y y y n n n n n y y
\end{verbatim}
    \caption{UCI Congressional Voting Records
    Data}
\end{figure}
\halfspace
\noindent If $\xs$ is large, we need many samples or a model.

\nhalfspace
\subsection{Models}
\nhalfspace
The \mterm{simplex} $\Delta_{n-1}$ contains as points all 
distributions over $\xs = \{x_1, \ldots, x_n\}$.
\[
    \Delta_{n-1} = 
    \left\{(p_1, \ldots, p_{n}) \in \R^n \STB
    \sum_{i=1}^{n} p_i= 1, p_i \ge 0 \right\} 
\]
\noindent A \mterm{statistical model} $\ms$ is a subset of the simplex.  \\
A \mterm{parametrization} is a surjective map $\Theta \to \ms$. \\
A \mterm{maximum likelihood estimate} $p_\mle$ minimizes 
\[
    -l(\theta; Z) = -\sum_{i=1}^m \log p_\theta(z_i).
\]
given data $Z = \{z_1, \ldots, z_m\}$.

A model can be selected with a criterion
\[
    -l(p_\mle) + \pi(\ms)
\]
that penalizes `complex' models.

\halfspace
\columnbreak


\section{Log-Linear Models}
\nhalfspace

A \mterm{log-linear} model is of the form
\[
    \ms_{V,h} = \cdel[\big]{p \in \Delta_{n-1} \STB \log p \in V + h},
\]
that is, its log-probabilities fall in an affine space.  Sparse models with
$\dim V$ small are effective.

A model can be specified with a matrix.
\[
    \ms_\as = \cdel[\big]{ p \in \Delta_{N-1}
    \:: \log p  \in \mathrm{rowspace}(\as)}
\]

\noindent {\bf Example} With $\xs = \{0, 1\}^2$
\[
        \as = \kbordermatrix{
            & 00 & 01 & 10 & 11 \\
            &  1 & 1  & 0  & 0 \\
            &  0 & 0  & 1  & 1 \\
            &  1 & 0  & 1  & 0 \\
            &  0 & 1  & 0  & 1
        }
\]
$\ms_\as$ makes the two bits independent.


\nhalfspace
\subsection{$L_1$ Regularization}
\nquarterspace

Let $\{b_1, \ldots, b_n\}$ be a basis for $L(\xs)$.  Choose
\[
    \log p = \sum_{j=1}^n \beta_j b_j
\]
to minimize
\[
    -\sum_{i=1}^m \log p(z_i) + \lambda \sum_{j=1}^n |\beta_j|
\]
where $\lambda > 0$ is a hyperparameter.

The $L_1$ penalty encourages a sparse structure.  This algorithm depends on the
choice of basis.

\nhalfspace
\subsection{Conditional Monte Carlo Sampling}
\nquarterspace
The vanishing ideal of $\ms_\as \subset \C^n$ is generated by
\[
    I(\ms_\as) = \adel[\big]{
        p^u - p^v \::
        \text{$u, v \in \mathbb N^d$ and $\as u = \as v$}
    }.
\]
{\bf Theorem} (Markov Basis)
A set of moves $\{b_1, \ldots, b_k\}$ is a Markov basis iff $p^{b_i^+} -
p^{b_i^-}$ generates the ideal $I(\ms_\as)$.

The Metropolis-Hastings algorithm takes samples conditioned on the statistic
$\as U = \as u$.  With samples, a $\chi^2$ test can be applied for the model.

\halfspace
\columnbreak

\section{Invariance Principles}
\nhalfspace

Permutations $\sigma: \xs \to \xs$ rename outcomes.  

Under certain $\sigma$, we want the results of statistical procedures be
invariant.  The \mterm{automorphism group} $G$ of $\xs$ consists of the $\sigma$
that `preserve its structure'.

\nhalfspace
\subsection{Isotypic Decomposition of $L(\xs)$}
\nquarterspace

Say that $V \subset L(\xs)$ is important for the majority of the data that one
encounters for $\xs$.  Then $V$ should be invariant under the action of $G$.

As a representation, $L(\xs)$ decomposes into \mterm{isotypic} components
\[
    L(\xs) = \bigoplus_{\rho \in \widehat G} m_\rho W_\rho.
\]
If $L(\xs)$ is multiplicity-free, then the decomposition of
$L(\xs)$ into irreducibles is unique.

\nhalfspace
\subsection{Homogeneous Spaces}
\nquarterspace

Let the action of $G$ on $\xs$ be transitive.  Then $\xs$ is a
\mterm{homogeneous space} and $\xs \cong G / K$ where $K$ is
the stabilizer of some $x_0 \in \xs$.

If $(G, K)$ is a Gelfand pair, then $L(\xs)$ is multiplicity free.  The series
of groups $K \le G$ induces \mterm{Gelfand-Tsetlin} bases for the $K$-invariant
vectors in each isotypic compnent.

\nhalfspace
\section{Binary Models}
\nquarterspace

Let $\xs = \{0, 1\}^k$.  We want invariance under
\begin{itemize}
    \item flips of bits, and
    \item rearrangements of bits.
\end{itemize}
These make the \mterm{hyperoctahedral} group \mbox{$S_2 \wr S_n$}.

The irreducibles of $S_2 \wr S_n$ in $L(\xs)$ are the eigenspaces of the
Laplacian matrix of the hypercube graph.  They represent $k$th-order effects.

\halfspace


\section{Boltzmann Machines}
\nhalfspace

A \mterm{Boltzmann Machine} is a model on $\{0, 1\}^n$ contained in the
eigenspaces for $\lambda = 0,2,4$.  More concretely, it is a log-linear model
governed by at most $2$nd-order interactions.
\begin{gather*}
    H(x) = -\sum_{i < j} \beta_{ij} x_i x_j - \sum_i \gamma_i x_i \\
    p(x) \propto \exp(-H(x))
\end{gather*}

A derived model used in machine learning is the \mterm{Restricted Boltzmann
Machine}.
\begin{gather*}
    H(x, h) = -\sum_{i < j} \beta_{ij} x_i h_j - \sum_i \gamma_i x_i - \sum_j
    \delta_j h_j\\
    p(x) \propto \sum_{h \in \{0, 1\}^k}\exp(-H(x, h))
\end{gather*}
It contains hidden and visible nodes, with interactions only between hidden
and visible nodes.  The visible distribution is the marginalization.

In general, making \mterm{mixture models} is a powerful way to combine
log-linear models.  The geometry of the situation, where weighted sums occur
before and after the exponential map, is incompletely understood.

%\bibliographystyle{hmcmath}
%\bibliography{thesis}

\section{Acknowledgments}
\nhalfspace

I would like to acknowledge Professor Michael Orrison for the introduction to
the field of algebraic statistics, and for his encouragement and forbearance
while I worked on my thesis.

\section{For Further Information}
\nhalfspace

\begin{itemize}
\item Contact the author at \\\mbox{\url{aaron.pribadi@gmail.com}}.
\item The poster and thesis document are available electronically at
\url{http://www.math.hmc.edu/~apribadi/thesis/}.
\end{itemize}


\end{poster}

\end{document}

 
