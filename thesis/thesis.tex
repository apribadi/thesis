% Copyright (c) 2012 by Aaron Pribadi. 
%
% This work is made available under the terms of the Creative Commons
% Attribution-NonCommercial-ShareAlike 3.0 license,
% http://creativecommons.org/licenses/by-nc-sa/3.0/.

\documentclass[cclicense]{hmcthesis}

% custom packages
\usepackage{util}
\usepackage{math}

% not on CTAN
\usepackage{kbordermatrix}

% macros specific to this document
\newcommand*{\x}[1]{\ensuremath{X^{(#1)}}}
\newcommand*{\mle}{\mathrm{mle}}
\newcommand*{\emp}{\mathrm{emp}}

% numbering
\numberwithin{equation}{chapter}
\numberwithin{ucounter}{chapter}

\title{Algebraic Methods for Log-Linear Models}
\author{Aaron Pribadi}
\thesisyear{2012}
\advisor{Michael Orrison}
\reader{Weiqing Gu}

\begin{document}

\frontmatter

\maketitle

\tableofcontents


\chapter{Abstract}
    This will be an abstract.

\chapter{Acknowledgements}
    There will be acknowledgements.

\mainmatter

\chapter{Introduction}

    Discrete observations model the world in almost the simplest way possible.
    Things are labelled and put into categories: ``This ice cream is green and
    has sprinkles.''  Given enough observations, one can begin to make
    predictions.
    
    Though on the surface this procedure appears to be quite simple, there
    exists a deep well of problems that are both difficult and intensely
    practical.  The pervasive influence of digital storage and computation has
    only increased the flood of readily available data.  In many cases, we
    cannot use data as well as we would like.  Problems surrounding 
    \begin{itemize}\noparspace
    \item genomic sequences,
    \item image recognition, 
    \item natural language processing,
    \item voting data, 
    \item product recommendation, 
    \end{itemize}
    and so forth are active areas of research driven by real-world concerns.

    Questions arising from the analysis of discrete data pose challenges, but at
    the same time offer rich mathematical ground.  Our hope is that ideas
    from `pure' mathematics, particularly geometry and algebra, can bring
    guiding principles and effective techniques for the analysis of data.  In
    return, advances suggested by the needs of practical problems are often of
    intrinsic mathematical interest.
    
    In this thesis, we examine log-linear models for discrete data.  Methods
    from both representation theory and algebraic geometry have been employed
    for model selection with log-linear models.  Our ideas are similar to those
    in the book by \citet{GRPS}, which also discusses decomposing a space of
    log-linear models with help from a group action.  The inspiration for our
    approach, however, was the recent appearance of what has been termed
    `algebraic statistics', i.e. the effort to apply techniques from algebraic
    geometry to statistics.  The lecture notes by \citet{DSS08} are currently
    the best introduction to the subject.  
    
    The approach to statistics taken here is influenced by the machine learning
    community; it emphasizes algorithmic methods, large data sets, and models
    which make few assumptions about the underlying random process.  An
    excellent introduction to machine learning from the statistical point of
    view is the book by \citet{EOSL}.  
    
    Log-linear models conform to a rigid structure while at the same time
    encompassing standard techniques for the analysis of discrete data.  As is
    often the case, the presence of a linear structure makes many questions more
    tractable.  The linear structure also allows concerns of symmetry and
    invariance to be more readily exploited.  Objects that are important across
    different data sets on the same sample space should be invariant under the
    symmetries of the sample space; it is easier to describe invariant objects
    that are linear.

    We pay special attention to models on the sample space $\{0, 1\}^n$ of
    binary strings of a fixed length.  This space is a straightforward way to
    represent finite data compactly, and as such is often employed as a generic
    encoding (e.g. in digital electronics).  The space of binary strings also
    has a large amount of symmetry, and is a homogeneous space with respect to
    its natural automorphism group.  We introduce a parametrization for
    log-linear models on a finite homogeneous space $\xs$ under several
    assumptions, including that the representation $L(\xs)$ is
    multiplicity-free.  This parametrization respects the isotypic decomposition
    of $L(\xs)$.  We then construct this parametrization explicitly for the case
    $\xs = \{0, 1\}^n$.  Such a parametrization is one of several for $\{0,
    1\}^n$, and can be used in algorithms for finding sparse models.

    While the constructions described here for models, algorithms, and bases
    offer a procedure for the analysis of binary data, we hope that the greater
    importance of our investigations will be to illuminate the terrain where
    algebra and statistics interact.

\chapter{Discrete Models}

    In this chapter, we outline an approach to the statistics of discrete data.
    We then introduce log-linear models, a class which is central to our
    investigation, and highlight the importance of sparsity.  From this
    exposition, it will become clear that statistical procedures require a few
    somewhat arbitrary choices.  Algebraic concerns, introduced in the following
    chapter, can help clarify such decisions.

    \section{Discrete Data}

    A random variable is (from an elementary standpoint) either
    \begin{itemize}\noparspace
    \item discrete, when its sample space is countable, or
    \item continuous, when its sample space is a subset of $\R^n$.
    \end{itemize}
    We consider the first case.  In particular, we usually take the relevant
    sample space to be finite.  Because we can then deal with
    finite-dimensional spaces, this assumption simplifies many ideas.

    Another fundamental assumption is that repeated observations are independent and
    identically distributed random variables.  That is, the observations $\x 1,
    \ldots, \x m$ are random variables, and each is distributed according to the
    same underlying probability distribution, $\x i \sim p$.  Given an observed
    set of values for the $\x i$, a basic objective is to estimate the
    underlying probability distribution $p$.

    Let each $\x i$ take an observed value from the finite sample space $\xs$.
    Because the order of the observations does not matter, we can summarize the
    series of observations with the counts
    \begin{equation*}
        u(x) = \pdel{\text{the number of $i$ for which $\x i = x$}}
    \end{equation*}
    for $x \in \xs$.  If the number of possible outcomes $|\xs|$ is small
    relative to the number of samples $m$, then the empirical distribution
    \begin{equation}
        p_\emp(x) = \frac{u(x)}{m}
        \label{eq:empirical}
    \end{equation}
    is a useful estimate of the true distribution $\x i \sim p$.

    It is sometimes the case that $|\xs|$ is very large.  Without a
    correspondingly large number of samples, the empirical distribution $p_\emp$
    may not adequately capture the underlying distribution $p$.  Such sample
    spaces arise naturally in a variety of situations, especially when
    combinatorial processes are involved.  Some examples follow.
    \begin{itemize}
    \item Multivariate data occurs when multiple things are observed at once.
    The sample space is a cartesian product
    \[
        \xs = \xs_1 \times \cdots \times \xs_k
    \]
    of several finite sets.  If $k$ labels are assigned to an observation, then
    the component space $\xs_i$ could be the set of possible values for the
    $i$th label.  For example, a person can have an eye color, a handedness, a
    gender, a political affiliation, and many more characteristics. In that
    case, $\xs_1$ could be a set of colors, et cetera.
    
    If each variate has approximately the same number of possible values, then
    the size of the sample space is exponential in the number of variates.  

    A comprehensive reference for the statistical analysis of multivariate data
    is the book by \citet{DMA}.

    \item Group-valued data occurs when discussing arrangements of something.
    For example, voters in an election could be asked to rank $n$ candidates.
    The sample space is then the symmetric group $\xs = S_n$.  Clearly, it is
    rather easy to construct simple examples with large numbers of possible
    outcomes, e.g.  $n!$ for the symmetric group.

    \end{itemize}
    For the empirical distribution to produce a good estimate of the underlying
    distribution, a prohibitively large number of samples is required.

    In order to analyze data with a large number of states, it is often fruitful
    to restrict which distributions we consider.  Ideally, we would use the
    structure of the underlying sample space in order to decide what
    restrictions to make.

    \begin{example}[Binary Multivariate Data]
        The UCI database contains a large number of data sets useful for the
        evaluation of machine learning techniques \citep{UCIData}.  The
        Congressional Voting Records data set contains Congressional voting
        records on a number of key issues.
        \begin{figure}[H]
            \centering
            \begin{verbatim}
            1) n y y n y y n n n n n n y y y y
            2) n y n y y y n n n n n y y y n y
            3) y y y n n n y y y n y n n n y y
            4) y y y n n n y y y n n n n n y y
            5) y n y n n n y y y y n n n n y y
            \end{verbatim}
            \vspace{-1.5\baselineskip}
            \caption{A few lines from the Congressional Voting Records
            data set}
        \end{figure}
        \noindent For this data set, each variate has two possible values,
        either the set $\{\texttt{yes}, \texttt{no}\}$ or $\{\texttt{democrat},
        \texttt{republican}\}$.  (This ignores missing values, e.g.  where a
        representative did not vote.)  The sample space may be written as $\xs =
        \{0, 1\}^{16}$, the set of binary strings with 16 bits.  
        \label{ex:binary-voting}
    \end{example}
        
    In the above example, the size of the sample space is quite large, with
    $|\xs| = 2^{16} = 65536$.  There are $435$ samples in the data set.  In
    order to make sense of the data, some assumptions about potential
    distributions are needed.  One rather simplistic assumption would be to
    assume that that each variate is independent from the others.

\section{The Simplex and Statistical Models}

    We take a geometric view of statistical models.  The formulation of
    statistical objects in terms of ideas borrowed from other parts of
    mathematics allows statistical problems to be attacked with a large range of
    useful tools.  The adoption of this perspective is in large part influenced
    by the lecture notes by \citet{DSS08}, which summarize recent progress in
    using algebraic geometry for statistics.

    A probability distribution on a finite set $\xs$ can be thought of as a
    real-valued function $p \in L(\xs)$ subject to the restrictions that
    $\sum_{x\in \xs} p(x) = 1$ and $p(x) \ge 0$ for all $x \in \xs$.  The
    function $p$ is the probability mass function.  Finite sets are particularly
    convenient because we can always work with a probability mass function, and
    because the probability mass function is always embedded in an ambient
    finite-dimensional vector space.  The space of all distributions on $\xs$ is
    a geometric object.
    
    \begin{definition} 
        \index{simplex}
        The \emph{standard simplex} of dimension $n$ is the subset
        \[
            \Delta_n = 
            \left\{(p_1, \ldots, p_{n+1}) \in \R^{n+1} \STB
            \sum_{i=1}^{n+1} p_i= 1, p_i \ge 0 \right\} 
        \]
        of $\R^{n+1}$.  If the appropriate dimension is either clear in context
        or irrelevant, then we may write $\Delta$, omitting the subscript.
    \end{definition}

    The simplex is a generalization of an equilateral triangle.  Low-dimensional
    simplices are familiar shapes: $\Delta_0$ is a point, $\Delta_1$ is a line
    segment, $\Delta_2$ is a triangle, and $\Delta_3$ is a tetrahedron.

    \begin{figure}[H]
        \centering
        \includegraphics[scale=1]{images/2-simplex.pdf}
        \caption{The 2-simplex.}
    \end{figure}

    There is a one-to-one correspondence between probability distributions on
    the set $\xs = \{x_1, \ldots, x_{n+1}\}$ and points in the simplex
    $\Delta_n$; the probability $p(i)$ is equal to the value of the coordinate
    $p_i$.  We therefore identify the two concepts with each other.
    Because the space of all probability distributions is a geometric
    object, it is easy to talk about specific families of probability
    distributions.

    \begin{definition}
    A \emph{statistical model} is a subset $\ms \subset \Delta$ of the
    probability simplex.  A \emph{parametrized model} $\ms$ with parameter space
    $\Theta$ is specified by a surjective map $\Theta \to \ms$.  We usually
    write $p_\theta$, with $\theta \in \Theta$, to denote a distribution from a
    parametrized model.
    \end{definition}

    Many statistical models have been studied and employed for the analysis of
    data, and the selection of an appropriate model is a delicate question.  In
    Section~\ref{sec:linear-models} we introduce a family of models with
    convenient properties.

    In an ideal situation, there is a hypothesis for an underlying mechanism
    producing the observable results.  In that case, a particular choice of
    model is easy to justify.

    \begin{example}[Binomial Model]

        The distribution $\mathrm{Binom}(N, \alpha)$ models the number of heads
        produced by $N$ independent `coin tosses', where \mbox{$0 \le \alpha \le
        1$} is the probability that a single toss produces a head.  It is
        defined by the probability density function
        \[
            p_\alpha(k) = {N \choose k} \alpha^k(1-\alpha)^{n-k}.
        \]
        for $k \in \{0, \ldots, N\}$.  
    \end{example}
        
    The map $\alpha \mapsto p_\alpha$ determines a parametrized statistical
    model.  The binomial model is a curve, i.e. a one-dimensional subset of the
    simplex.  In the case that $N=2$, the model $\mathrm{Binom}(2, \alpha)$
    simulates two coin flips.  The three coordinates of a point in the model
    measure the probabilities that zero, one, and two heads will occur,
    respectively.

    \begin{figure}[H]
        \centering
        \vspace*{-0.2cm}
        \scalebox{1}{ \includegraphics[scale=0.7]{images/binomial.pdf} }
        \vspace*{-0.5cm}
        \caption{The binomial model for $N=2$}
        \label{fig:binomial}
    \end{figure}

    \noindent As the parameter $\alpha$ varies over $[0,1]$, the statistical
    model traces out the curve 
    \[
        \alpha \longmapsto \big((1-\alpha)^2, 2\alpha(1-\alpha), \alpha^2\big)
    \]
    in the simplex $\Delta_2$ (see Figure~\ref{fig:binomial}).  If we knew that
    a coin was being flipped twice and did not know the odds of the coin, then
    the model $\alpha \mapsto \mathrm{Binom}(2, \alpha)$ would be an appropriate
    choice for the situation.

    \begin{example}[Multivariate Independence]
        The independence model on a sample space $\xs = \xs_1 \times \cdots
        \times \xs_k$ assumes that each variate is independent.  For a
        distribution $p$ on $\xs$, the variates are said to be independent if
        $p$ factors as
        \[
            p(x) = p_1(x) \cdots p_k(x)
            \qquad
            \text{where}
            \qquad
            p_i(x) = \sum_{y_i = x_i} p(y) % \stackrel{y \in \xs}{x_i = y_i}} p(y)
        \]
        for all $x = (x_1, \ldots, x_k) \in \xs$.  Each factor $p_i(x)$ is a
        function only of the $i$th component of $x$.  
        \label{ex:independence1}
    \end{example}
        
    Despite this simplistic assumption, the independence model is surprisingly
    effective in many situations.  (TODO Naive Bayes)
    The map 
    \begin{align*}
        \Delta_{|\xs_1|-1} \times \cdots \Delta_{|\xs_k|-1} 
        &\to \Delta_{|\xs| - 1} \\
        (p_1, \ldots, p_k) &\mapsto p_1 \cdots p_k
    \end{align*}
    gives a one-to-one parametrization of the independence model.  Counting
    dimensions indicates that the independence model imposes strict limitations
    on potential distributions.  The parameter space has dimension
    \[
        \dim 
        (\Delta_{|\xs_1|-1} \times \cdots \times \Delta_{|\xs_k|-1})
        =
        |\xs_1| + \cdots + |\xs_k| - k
    \]
    whereas the space of all distributions on $\xs$ has dimension 
    \[
        \dim(\Delta_{|\xs| - 1})
        =
        |\xs_1| \cdots |\xs_k| - 1.
    \]
    If the variates are binary, as in Example~\ref{ex:binary-voting}, then the
    model has dimension $k$ and the whole simplex has dimension $2^k - 1$.
    The latter grows much more rapidly with increasing $k$ than the former.
    
\section{Likelihood and Model Complexity}
    In the analysis of data, there is a tension between how well an explanation
    fits the observed data, and how well such an explanation can be expected to
    generalize to new data.  While we do not explore all nuances of this
    trade-off, we do introduce a few fundamental concepts.

    One way to measure how well a distribution matches data is to ask the
    question, ``How likely is the data, given the distribution?''  A maximum
    likelihood estimate quantifies whether a model contains distributions
    suitable in that way.

    Let $\ms = \{p_\theta : \theta \in \Theta\}$ be a parametrized statistical
    model.  Suppose that some data $Z = \{z_1, \ldots, z_m\}$ are observed.  It
    is generally assumed that the data has been drawn from independent and
    identically distributed samples following an unknown true distribution from
    the model.
    
    \begin{definition}
        The \emph{likelihood function} of $\theta \in \Theta$ given the data $Z$
        is
        \[
            L(\theta; Z) = \prod_{i=1}^n p_\theta(z_i).
        \]
        It is the probability that the observed data $Z$ would occur if the
        samples followed $p_\theta$.  Oftentimes, the \emph{negative
        log-likelihood}
        \begin{equation}
            -l(\theta; Z) = -\log L(\theta; Z) = -\sum_{i=1}^n \log p_\theta(z_i)
            \label{eq:negloglik}
        \end{equation}
        is used in lieu of the likelihood.  The negative log-likelihood is
        useful because the contributions to $-l(\theta; Z)$ from the
        observations $z_i$ are additive and each acts as a `loss' or `cost'.
        In machine learning, various considerations for decisions are often
        phrased as costs; this makes them easy to combine.
    \end{definition}
    \begin{definition}
    The \emph{maximum likelihood estimate} of the true parameter is the
    parameter 
    \[
        \theta_\mle = \argmax_{\theta \in \Theta} L(\theta; Z)
    \]
    that maximizes the likelihood of the data, or equivalently that minimizes
    the negative log-likelihood of the data.  In some cases, we identify the
    parameter $\theta$ with the distribution $p_\theta$. The term `maximum
    likelihood estimate' then refers to the distribution $p_\mle$ that maximizes
    the likelihood, given a choice of model.
    \end{definition}

    The fact that a maximum likelihood estimate might not exist or might not be
    unique is tacitly ignored.  Indeed $\{L(\theta; Z) \mid \theta \in \Theta\}$
    might be an open set, and the map $\theta \mapsto L(\theta; Z)$ might not be
    injective.  In applications with real data, it is often the case that a
    precise maximum likelihood estimate is not necessary, and that an
    approximate value is sufficient.

    The distribution $p_\mle$ from the maximum likelihood estimate is the best
    one possible from the model.  Notice that if $\ms \subset \ns$ are two
    nested statistical models, then the maximal likelihood from $\ms$ is less
    than or equal to that from $\ns$.  Thus the size of the model determines how
    good of a distribution is possible.

    \begin{example}
        Suppose that the model does not restrict distributions at all, i.e.
        that the model is the whole simplex.  Then the maximum likelihood
        estimate is the empirical distribution $p_\emp$ as in
        \eqref{eq:empirical}.
    \end{example}

    \begin{example}[Multivariate Independence]
        Recall the independence model from Example~\ref{ex:independence1}.  The
        maximum likelihood estimate for the independence model is
        \[
            p_\mle(x) = \frac{u_1(x_1)}{m} \times \cdots \times \frac{u_k(x_k)}{m}
        \]
        where $x = (x_1, \ldots, x_k)$, $m$ is the number of samples, and
        $u_i(x_i)$ is the number of times that the $i$th variate of the
        observation is $x_i$.  In other words, we get a maximum likelihood
        estimate of each variate separately with its empirical distribution, and
        take the product distribution.
    \end{example}

    A larger model is deemed to contain more `complex' distributions.  The
    trade-off is that a larger model allows for probability distributions that
    fit the observed data better, but might also allow for over-fitting.  When a
    model is over-fit, the resulting distribution matches the training data
    (i.e. the data with which the model was fit) very well, but generalizes
    (i.e. explain subsequently observed data) poorly.  The essential problem is
    that a small number of sample observations do not contain enough information
    to fit a complex model properly.  This trade-off between model complexity
    and predictive power can be approached in a number of ways, and is explored
    in the standard literature.  Chapter 7 of the book by \citet{EOSL} is one
    reference.  
    
    There are at least two common methods to limit model complexity. 
    \begin{itemize}
    \item We can require that the estimated distribution $p$ is contained within
    some small model $\ms \subset \Delta$.
    \item We can minimize $-l(p) + \pi(p)$, i.e. the negative log-likelihood
    with an additional penalty term measuring the complexity of $p$.
    \end{itemize}
    When the penalty term forces estimated distributions into smaller models,
    these two methods can produce similar results.  With a well-selected penalty
    term, however, the latter method can be scaled easily to favor more or less
    complex models by adjusting the penalty.

    For example, two criteria with complexity penalties are the Akaike
    information criterion and the Bayesian information criterion, defined
    \begin{align*}
        \mathrm{AIC} &= -2\,l(p_\mle) + 2d \\
        \mathrm{BIC} &= -2\,l(p_\mle) + (\log m) d
    \end{align*}
    where $-l(p_\mle)$ is the negative log-likelihood of the maximum likelihood
    estimate as in \eqref{eq:negloglik}, $d$ is the dimension of the parameter
    space $\Theta$, and $m$ is the number of samples \citep{EOSL}.  These two
    criteria have different motivations, which are described in the reference.
    The important point to notice, however, is that the dimension of the model
    is used as the measure of its size.

\section{Log-Linear Models}
    \label{sec:linear-models}
    
    The selection of distributions can be treated as a problem in function
    estimation.  From the data we construct the empirical distribution \mbox{$p_\emp
    \in L(X)$}, and we wish to find another function $p \in L(X)$ that
    approximates $p_\emp$ subject to some set of restrictions.  
    
    The structure of $L(X)$ as a linear space suggests one method of
    approximation.  One can expand $p_\emp$ in terms of a basis $B = \{v_1,
    \ldots, v_n\}$ of $L(X)$, $p_\emp = \sum_{i=1}^n \lambda_i v_i$.  Any subset
    of the basis elements yields an approximation of $p_\emp$ from the
    projection to the subspace spanned by our subset.  One can select the terms
    with the largest coefficients $\lambda_i$, or, if the basis has some natural
    ordering, one can simply truncate the series.

    This method of approximation has at least one drawback, namely that negative
    probabilities are possible.  Dealing instead with log-probabilities is often
    fruitful.  In fact, log-linear models, i.e. discrete exponential families,
    are especially prevalent in the analysis of discrete multivariate data; see,
    for example, \citet{DMA}.
    \begin{definition}
        A \emph{log-linear} model $\ms_{V\!,\,h}$ is a statistical model of the form
        \[
            \ms_{V\!,\,h} = \cdel[\big]{p \in \Delta_{n-1} \:: \log p = (\log p_1, \ldots,
            \log p_n) \in V + h}
        \]
        where $h \in \R^n$, $V$ is a linear subspace of $\R^n$, and $V + h$ is
        an affine subspace.
    \end{definition}

    In many useful situations $h = 0$, so the log-linear model is associated
    with a vector subspace of $L(\xs)$.
    The usual definition of an exponential family (which need not be over a
    finite sample space) is as follows.
    \begin{definition}
        An \emph{exponential family} over a sample space $\xs$ parametrized by
        $\Theta$ contains distributions of the form
        \[
            p_\theta(x) = 
            \frac 1 {Z(\theta)}
            \exp\pdel[\big]{
                \displaystyle \eta(\theta) \cdot T(x) + h(x)
            }
        \]
        where $\eta : \Theta \to \R^d$, $T:\xs \to \R^d$, and $h: \xs \to \R$
        are known functions, and $Z:\Theta \to \R$ is a normalizing constant
        known as the \emph{partition function}.
    \end{definition}
    An exponential family is a log-linear model when $\xs$ is finite and $\eta$
    is the identity map $\R^d \to \R^d$, as $p_\theta$ is constrained to lie in
    the affine space
    \[
        \spanset{
            (T_i(x_1), \ldots, T_i(x_n)) \:: i \in \{1, \ldots, d\}
        } + h
    \]
    where $h$ is interpreted as a vector in $\R^n$.  

    \begin{example}[Binary Independence]
        The independence model with strictly positive probabilities is a
        log-linear model.  We compute this explicitly for the case where the
        sample space is $\xs = \{0, 1\}^2$.  One can verify that the two bits
        are independent for a distribution $p$ on $\xs$ if and only if 
        the row span of the matrix
        \[
            \kbordermatrix{
                & 00 & 01 & 10 & 11 \\
                &  1 &  1 &  0 &  0 \\
                &  0 &  0 &  1 &  1 \\
                &  1 &  0 &  1 &  0 \\
                &  0 &  1 &  0 &  1
            }
        \]
        contains $\log p$.  Notice that the matrix has rank 3, so that the
        independence model is a proper subset of the full simplex.
    \end{example}

    There is another form in which log-linear models are often presented.
    Define an `energy' function $H: \xs \to \R$.  The corresponding
    \emph{Boltzmann} distribution is defined
    \[
        p(x) = \frac 1 Z \exp(-H(x))
    \]
    where
    \[
        Z = \sum_{x \in \xs} \exp(-H(x))
    \]
    is the normalizing constant.  If the possible
    values for $H$ in a model are all linear combinations of some basis
    functions, then the model is log-linear.  This form is motivated by examples
    from statistical mechanics, where the probability of a state is proportional
    to $e^{-E/(k_BT)}$, where $E$ is the energy of the state, $T$ is the
    temperature, and $k_B$ is the Boltzmann constant.

    \begin{example}
        A \emph{Boltzmann machine} on $\{0, 1\}^n$ has a distribution of the
        form
        \[
            p(x) \propto \exp\pdel*{-\sum_{i < j} \beta_{ij} x_i x_j  - \sum_i
            \gamma_i x_i}.
        \]
        The Boltzmann machine and derived models have recently had many
        important machine learning applications \citep{Hin07}.
        % For instance, the \emph{restricted Boltzmann machine}
        \label{ex:boltz}
    \end{example}
    % Example: Boltzmann machine

\section{Sparsity and Regularization}
    \label{sec:regularization}

    Why are we so keen to cut down the number of dimensions?  The reason, which
    arises in optimization and statistics, is colorfully termed the \emph{curse
    of dimensionality}.  It is easiest to illustrate the problem in another
    closely related setting.

    \begin{example}[Sampling and the Curse of Dimensionality]
        We describe a classification problem with $p$ real-valued features.
        Concretely, say that we want to label points in the $p$-dimensional cube
        $[0, 1]^p$ either \texttt{blue} or \texttt{orange}.  We can sample some
        points in the cube to check their color.  A reasonable assumption is
        that if a point $u$ is \texttt{blue}, then another nearby point $v$
        (where $\|u - v\|$ is small) is probably also \texttt{blue}.  This is
        known as a nearest-neighbor classification algorithm.  
        
        When $p$ becomes large, the volume of a neighborhood around a point
        becomes very small relative to the total volume of the space.  If we
        want every point of the space to contain a sampled point within a fixed
        radius $\delta > 0$, then we require a number of samples exponential in
        the dimension.
    \end{example}

    In statistical learning, this is also known as the `$p \gg N$' problem,
    where $p$ is the number of features and $N$ is the number of samples.  One
    method for battling the curse of dimensionality is to encourage
    \emph{sparsity}.  Roughly, this means that in a linear problem with many
    parameters, we want the majority of the parameters to be zero.
    
    \begin{example}[Linear Regression]
        The term `sparse' arises from linear regression.  In that setting one
        wishes to find a relation $y = Ax$, where $x \in \R^N$, $y \in \R$, and
        $A$ is a linear transformation.  A sparse solution is one where many
        entries of the matrix $A$ are zero, i.e. where $A$ is a sparse matrix.
    \end{example}
    
    Methods that encourage sparsity have been found to perform well in practice.
    In what they call the ``bet on sparsity'', \citet{EOSL} introduce a
    heuristic that explains the importance of sparse methods:
    \begin{quote}
        Use a procedure that does well in sparse problems, since no procedure
        does well in dense problems.
    \end{quote}
    \noindent We must assume that we can find important low-dimensional spaces,
    because otherwise the estimation of parameters becomes very difficult.

    We outline one approach to sparse modeling, the colorfully named Lasso, first
    introduced in a paper by \citet{LASSO}.  Consider a model with real
    parameters $\beta_1, \ldots, \beta_n$.  As before the setting is one of
    linear regression, fitting $y = \sum_{i=1}^n \beta_i x_i$.  The objective is to
    minimize
    \[
        \sum_{j=1}^m \pdel*{y^{(j)} - \sum_{i=1}^n \beta_i x_i^{(j)}}^2 +
        \lambda \sum_{i=1}^n |\beta_i|
    \]
    given data $\{(x^{(j)}, y^{(j)})\}_{j=1}^m$ and some hyperparameter $\lambda
    > 0$.  That is, it minimizes the residual sum of squares with an additional
    penalty term which is a multiple of the $L_1$-norm of the parameter vector.
    As the value of $\lambda$ is increased, this approach causes some parameters
    $\beta_i$ to go to exactly zero, yielding a sparse model.  The usage of the
    $L_1$ norm as a penalty is also known as $L_1$ regularization.  The
    procedure derived from using $L_2$ regularization instead is known as ridge
    regression; in contrast, it does not bring select parameters to zero, but
    instead reduces all parameters at the same time.

    The approach of $L_1$ regularization has also been applied directly to
    log-linear models, in the paper by \citet{SPEC}.  With data $\{z_1, \ldots,
    z_m\}$ and functions $\{f_1, \ldots, f_n\}$ that span $L(\xs)$, they
    minimize
    \[
        - \sum_{j=1}^m \log p(z_j; \beta) + \lambda
        \sum_{i=1}^n |\beta_i| 
    \]
    where $\lambda$ is a hyperparameter and the probability distributions are in
    a log-linear model
    \[
        \log p (z_j; \beta) = \sum_{i=1}^n
    \]
    with parameters $\beta_1, \ldots, \beta_n$.  That is, they minimize the
    negative log-likelihood with an $L_1$ penalty term.  This, again, yields a
    sparse representation.

    One should keep in mind that methods for finding sparse models often depend
    on selecting certain subspaces of $L(\xs)$ beforehand; $L_1$ regularization
    relies on the selection of a full set of basis vectors.  Because there
    are many possible choices for such subspaces, one would like to know that
    the outcome of the procedure is not too dependent on choices made
    arbitrarily, and that the selection of subspaces is somehow natural.  Under
    certain circumstances, especially when the sample space is highly
    structured, such natural choices are possible.

\chapter{Decompositions of $L(\xs)$}

    In this chapter, we describe algebraic ideas that can help make natural
    choices from among options posed by the statistical procedures described in
    the previous chapter.  Specifically, we use tools from representation theory
    to find natural subspaces and vectors in $L(\xs)$ that can be used for model
    selection in a log-linear framework.  As mentioned in the introduction, this
    approach is close to that explicated in the book by \citet{GRPS}.

\section{An Invariance Principle}
    
    The array of possible models and regularization procedures is large enough
    that some guiding principles are urgently needed.  Symmetries of the sample
    space, if they exist, suggest some constraints.  We shall say that we wish
    our statistical procedures to be invariant under reparametrization.  

    More precisely, depending on the nature of the sample space $\xs$, there are
    permutations $\sigma: \xs \to \xs$ that preserve the structure of $\xs$.
    Such permutations form the automorphism group $\Aut(\xs)$ of the sample
    space.  Essentially, the automorphism group describes the admissible ways to
    re-label the data.  A statistical procedure is invariant under $\sigma \in
    \Aut(\xs)$ if both
    \begin{itemize}\noparspace
    \item applying $\sigma$ to the data, then running the statistical procedure,
    and
    \item running the statistical procedure, then applying $\sigma$ to the
    results
    \end{itemize}
    produce the same output.

    \begin{example}[Binary Multivariate Data]
        Suppose that as in Example~\ref{ex:binary-voting} the data consist of
        voting records of individuals on $n$ binary issues.  The sample space is
        $\{0, 1\}^n$.  A natural choice for the automorphism group is the
        hyperoctahedral group.  The hyperoctahedral group and its interaction
        with binary data will be examined more closely in
        Section~\ref{sec:hyperoctahedral}, as it is out primary example.
    \end{example}
    
    Under the log-linear framework described in Section~\ref{sec:linear-models},
    a model consists of a subspace of $L(\xs)$ that is sufficient to adequately
    describe the data.  Suppose that one wishes to say that a particular
    subspace of $L(\xs)$ is important in general, i.e. that for the majority of
    data one encounters for the sample space $\xs$, this subspace is necessary.
    This claim occurs in practice.

    \begin{example}[Binary Multivariate Data]
        Let $\xs = \{0, 1\}^n$.  The space $L(\xs)$ contains subspaces of
        $k$th-order effects.  For any subset $S \subset \{1, \ldots, n\}$ and
        function $h: S \to \{0, 1\}$, define the function $f_{S,\,h} \in L(\xs)$
        as
        \[
            f_{S,\,h} (b_1 \cdots b_n) = \begin{cases}
                1 & \text{if $b_i = h(i)$ for all $i \in S$} \\
                0 & \text{otherwise}
            \end{cases}
        \]
        where the $b_i$ are bits.  The function $f_{S,\,h}$ is an indicator
        function for bit strings for which the bits indexed by $S$ take on
        particular values.  The space of $k$th-order effects is the subspace
        \[
            \spanset{f_{S,\,h} \STB S \subset \{1, \ldots, n\}, |S| = 
            k, f : S \to \{0, 1\}}
        \]
        of $L(\xs)$.
        The log-linear model associated with the space of $k$th-order effects
        only `cares' about the interaction between at most $k$ variates.
        Zeroth-order effects produce only the uniform distribution.  First-order
        effects produce the independence model (Example~\ref{ex:independence1}).
        Second-order effects produce the Boltzmann machine
        (Example~\ref{ex:boltz}).  The low order spaces are considered more
        essential than higher order ones.
    \end{example}

    Every permutation from the automorphism group of $\xs$ induces a linear
    automorphism of $L(\xs)$.  Thus the automorphism group has an induced group
    action on $L(\xs)$.  If a subspace of $L(\xs)$ is truly generically
    important, then it must be invariant under the action of the automorphism
    group.  The good news is that one can get a handle on the invariant
    subspaces of $L(\xs)$.


\section{The Isotypic Decomposition}
    The action of the automorphism group of $\xs$ on $L(\xs)$ is a permutation
    representation.  In order to explain this statement and its implications, we
    introduce a few ideas from representation theory.
    
    The goal of representation theory is to better understand the structure of a
    group (or other algebraic object) by transferring the problem to the domain
    of linear algebra.  The field of representation theory is very rich, so we
    limit ourselves to a few foundational concepts from the representation
    theory of finite groups.

    \begin{definition}
        A \emph{representation} of a group $G$ is a group homomorphism $\rho: G
        \to GL(V)$ from $G$ to the group of invertible linear transformations on
        a vector space $V$.  
    \end{definition}
    This group homomorphism `represents' group elements as invertible matrices.
    Here we make the simplifying assumptions that $G$ is a finite group and
    that $V$ is a finite-dimensional vector space.  It is common to identify the
    group representation $\rho$ with the vector space $V$; one then says that
    $V$ is the group representation.

    One subtlety is that it is possible to make certain strong statements only
    for complex representations, whereas for probabilities distributions we wish
    to deal with real vector spaces.  In some cases of practical interest,
    however, real representations behave similarly nicely.

    One source of representations, including the representations of interest to
    us for statistics, is through permutation groups.  
    \begin{definition}
        A \emph{permutation representation} of a group $G \le S_n$ on the space
        $L(\xs)$ of functions on $\{x_1, \ldots, x_n\}$ is defined by the action
        \[
            (\sigma f)(x_k) = f(x_{\inv \sigma k})
        \]
        for all $\sigma \in G$.
    \end{definition}

    Representations of finite groups are well-behaved in large part because they
    can be broken up into constituent parts.

    \begin{definition}
        An \emph{irreducible representation} is a nonzero representation $V$
        with no non-trivial (i.e. not $\{0\}$ or $V$) subspaces invariant under
        the group action.
    \end{definition}

    \begin{theorem}[Maschke]
        A complex finite-dimensional representation of a finite group can be
        written as a direct sum of irreducible representations.
    \end{theorem}

    \begin{theorem}
        A finite group has only finitely many irreducible complex
        representations, up to isomorphism.
    \end{theorem}

    As a result, every complex representation can be written in a particular
    canonical form.
    
    \begin{definition}
        The \emph{isotypic decomposition} of a complex representation $V$ of a
        finite group $G$ is the direct sum
        \[
            V = \bigoplus_{\rho \in \widehat{G}}  m_\rho W_\rho
        \]
        where $\widehat G$ is the collection of irreducible representations of
        $G$ and $m_\rho W_\rho$ is the direct sum of $m_\rho$ copies of the
        representation $\rho$.  If each $m_\rho$ is either $0$ or $1$, then the
        representation is called \emph{multiplicity-free}.
    \end{definition}

    Thus the automorphism group $\Aut(\xs)$ defines a decomposition of $L(\xs)$
    into a direct sum of invariant complex spaces.  For many cases of interest,
    including binary data, this decomposition also holds over the real field.

    %\begin{example}[Binary Multivariate Data]
        %It is possible for the isotypic decomposition to 
    %\end{example}

\section{Homogeneous Spaces and $K$-Invariant Vectors}

    Sometimes $\xs$ exhibits an exceptional amount of symmetry.  In the best
    case, `every point in $\xs$ is the same'.  The following formulation makes
    this last statement precise.
    \begin{definition}
        A \emph{homogeneous space} is a space $\xs$ together with a transitive
        action of a group $G$ on $\xs$.
    \end{definition}
    Homogeneous spaces usually turn up in the context of the action of a Lie
    group on a smooth manifold, in which case the group is required to act by
    diffeomorphisms.  For our purposes, we take $\xs$ to be finite and $G$ to be
    the automorphism group of $\xs$.

    Given a choice of any point $x_0 \in \xs$, let $K$ be the stabilizer of
    $x_0$.  Then we can identify the set $\xs$ with the coset space $G / K$.  If
    a different point $y = g x_0$ is selected, with $g \in G$, then the
    stabilizer of $y$ is related to $K$ by an inner automorphism.  Specifically,
    the stabilizer of $y$ is $g K\inv g$.  
    
    \begin{definition}
        Let $\xs = G / K$ be a homogeneous space.  If $L(\xs)$ is
        multiplicity-free, then we call $(G, K)$ a \emph{Gelfand pair}.
    \end{definition}
    There are in fact several equivalent characterizations of Gelfand pairs,
    the statements of which we do not fully explain \citep{Gelf}.
    \begin{itemize}\noparspace
    \item Convolution in the space $L(K\backslash G/K)$ of $K$-bi-invariant functions
    is commutative.
    \item $\mathrm{Hom}_G(L(\xs), L(\xs))$, the algebra of operators
    intertwining the permutation representation, is commutative.
    \item For all irreducible representations $(\rho, V)$ of $G$, we have
    $\dim V^K \le 1$, where $V^K$ is the subspace of all $K$-invariant
    vectors in $V$. 
    \end{itemize}

    In this setting, finding subspaces of $L(\xs)$ falls under the purview of
    the field of harmonic analysis.  We make use of the paper by \citet{Harm} on
    the subject.  Specifically, given this setup, we can find distinguished
    vectors in the isotypic spaces of $L(\xs)$.  The following theorem is a
    result in Section~2.5 of their paper.

    \begin{theorem}[\citet{Harm}]
        Let $(G, K)$ be a Gelfand pair, so that $L(G/K)$ is multiplicity-free.
        Let $V$ be an irreducible representation of $G$ contained in $L(G/K)$.
        Let $V^K = \cdel*{v \in V \STB kv = v \text{ for all $k \in
        K$}}$ be the space of $K$-invariant vectors in $V$.  Then $\dim V^K =
        1$.
    \end{theorem}

    \noindent Thus each bijection $\xs \cong G / K$ yields a unique
    $K$-invariant vector for each irreducible subspace, up to scaling.

    Recall that different selections of base point $x_0$ yield different
    stabilizers $\mathrm{fix}_G(x_0)$, related by inner automorphisms.  We claim
    that the vectors invariant under some $\mathrm{fix}_G(x_0)$, form spanning
    sets for each irreducible representation in $L(\xs)$.  We state this
    precisely, and give a proof.
    \begin{theorem}
        Let $G$ be a group acting transitively on a set $\xs$.  Let $L(\xs)$ be
        the space of complex-valued functions on $\xs$; it is a representation
        of $G$ with the induced action.  Fix $x_0 \in \xs$, and let $K =
        \mathrm{fix}_G (x_0)$ be the stabilizer of $x_0$.  Assume that $L(\xs)$
        is multiplicity-free, i.e. that $(G, K)$ is a Gelfand pair.  Let $L(\xs)
        = W_1 \oplus \cdots \oplus W_m$ be the isotypic decomposition of
        $L(\xs)$.  Fix any $1 \le d \le m$.  For each $g \in G$, there exists a
        $g K\inv g$-invariant vector $f_{d, g}$ in $W_d$, unique up to scaling.
        Then $\{f_{d, g} \STB g \in G\}$ is a spanning set for $W_d$.
        \label{thm:thm}
    \end{theorem}
    \begin{proof}
        Let $f$ be a $K$-invariant vector in $W_d$.  Because $W_d$ is an
        irreducible representation of $G$, it is sufficient to show that
        \[
            G f = \{g f \STB g \in G\} \subset \spanset{f_{d, g} \STB g
            \in G}.
        \]
        But for any $k \in K$,
        \begin{IEEEeqnarray*}{rClCl}
            ((g k \inv g)(g f))(x)
            &=&
            f(\inv g \inv{(g k \inv g)} x)
            &=&
            f(\inv g g \inv k \inv g x) \\
            &=&
            f(\inv k \inv g x) 
            &=&
            f(\inv g x) \\
            &=&
            (gf)(x)
        \end{IEEEeqnarray*}
        so $gf$ is $g K \inv g$-invariant.  The result follows.
    \end{proof}
    The significance of this theorem is that we have a spanning set of $L(\xs)$
    which depends (up to scaling) only on $\xs$ and its automorphism group, and
    which respects the isotypic decomposition.  From such a spanning set, a
    sparse log-linear model may be recovered using, for instance, the $L_1$
    regularization procedure explained in Section~\ref{sec:regularization}.

\chapter{Binary Data}
    \label{ch:binary}

    The constructions of the previous chapter are somewhat general.  In this
    chapter, we apply those constructions to the case of binary multivariate
    data.  It is first necessary to examine the structure of the hyperoctahedral
    group, the automorphism group of the set $\xs = \{0, 1\}^n$ of binary
    strings of a fixed length.  We then derive the isotypic decomposition for
    $L(\xs)$ and find the spanning set of $K$-invariant vectors.

\section{The Hyperoctahedral Group}
    \label{sec:hyperoctahedral}
    Let us devote our attention to the case of binary multivariate data.  As
    before, let the sample space be $\xs = \{0, 1\}^n$, the set of length $n$
    binary strings.
    \begin{figure}[H]
    \[
        \cdel*{
        \underbrace{00\cdots00}_\text{$n$ bits}\;,\;
        00\cdots01,\;
        00\cdots10,\;
        \ldots,\;
        11\cdots11
        }
    \]
    \caption{$2^n$ binary strings}
    \end{figure}
    If we do not assign particular meanings to any of the bits beforehand, then
    certain re-labellings of the sample space do not change its structure.  In
    particular, there are two natural operations:
    \begin{itemize}\nospace
    \item reordering the bits, and
    \item flipping a bit.
    \end{itemize}
    Each operation specifies a permutation of $\xs = \{0, 1\}^n$.  If $\mu \in
    S_n$ is a reordering of the bits, the corresponding permutation of the
    $\xs$  is
    \begin{align*}
        b_1 \cdots b_n &\mapsto b_{\mu(1)} \cdots b_{\mu(n)}.
    \end{align*}
    A flip of the $k$th bit is the permutation
    \begin{align*}
        b_1 \cdots 0 \cdots b_n 
        &\mapsto
        b_1 \cdots 1 \cdots b_n  \\
        b_1 \cdots 1 \cdots b_n 
        &\mapsto
        b_1 \cdots 0 \cdots b_n
    \end{align*}
    where the $k$th position is modified.  
    
    The permutations specified by reorderings and flips of bits are elements of
    an ambient group, the symmetric group on $\{0, 1\}^n$.  They generate the
    subgroup known as the hyperoctahedral group $S_2 \wr S_n$.  This group may
    be defined as the automorphism group of the $n$-hypercube or as the wreath
    product (the notation $S_2 \wr S_n$ comes from the second definition).  The
    first formulation is more concrete, so we use it.

    First we define a metric on the set of binary strings.
    \begin{definition}
    The \emph{Hamming distance} between two binary strings is the number of
    positions in which the two strings differ.  
    \end{definition}
    For example, the following pairs of binary strings have Hamming distances
    of 0, 1, and 2, respectively:
    \begin{align*}
        0010 &\xleftrightarrow{\quad\text{distance $0$}\quad} 0010 \\
        0010 &\xleftrightarrow{\quad\text{distance $1$}\quad} 0110 \\
        0010 &\xleftrightarrow{\quad\text{distance $2$}\quad} 0100
    \end{align*}
    It is straightforward to verify that the Hamming distance is in fact a
    metric.  The edges of the hypercube graph are determined by the Hamming
    metric.
    \begin{definition}\index{hypercube}
        The \emph{$n$-hypercube graph} has as its vertices the set $\{0, 1\}^n$.
        Two vertices form an edge if and only if their Hamming distance from
        each other is exactly 1, that is, if the two binary strings differ in
        exactly one bit.
    \end{definition}
    The graph distance between any two vertices of the hypercube graph, i.e. the
    minimal number of edges in a walk between those two vertices, is the same as
    their Hamming distance.  A look at a picture of a hypercube graph
    (Figure~\ref{fig:hypercube}) makes the origin of the graph's name clear.

    \begin{figure}[h]
        \centering
        % http://code.google.com/p/graph-theory-algorithms-book/
        % GPL Documentation Licence

        \begin{tikzpicture}
        [nodedecorate/.style={shape=circle,inner sep=2pt,draw,thick},%
          linedecorate/.style={-,thick}]
        %% nodes or vertices
        \foreach \nodename/\x/\y in {1/0/0, 2/2/0, 3/2/2, 4/0/2, 5/0.5/0.5,
          6/2.5/0.5, 7/2.5/2.5, 8/0.5/2.5}
        {
          \node (\nodename) at (\x,\y) [nodedecorate] {};
        }
        %% edges or lines
        \path
        \foreach \startnode/\endnode in {1/2, 2/3, 3/4, 4/1, 5/6, 6/7, 7/8,
          8/5, 1/5, 2/6, 3/7, 4/8}
        {
          (\startnode) edge[linedecorate] node {} (\endnode)
        };
        \end{tikzpicture}

        \caption{The $3$-hypercube graph}
        \label{fig:hypercube}
    \end{figure}
    
    The structure of a graph is determined entirely by its edges, so we get the
    following definition of a graph automorphism.
    \begin{definition}
        A \emph{graph automorphism} is a permutation of the vertices of a graph
        that preserves its edges.  That is, if a graph has vertices $S$ and
        edges $E$, where each edge is a two-element subset of $S$, a permutation
        $\sigma: S \to S$ is a graph automorphism if for every edge $\{u, v\}
        \in E$, we have \mbox{$\{\sigma(u), \sigma(v)\} \in E$}.  The automorphisms of
        a graph form a group under composition.
    \end{definition}

    \begin{definition}
        The \emph{hyperoctahedral group} $H_n$ is the automorphism group of the
        $n$-hypercube.
    \end{definition}

\section{Constructing the Isotypic Decomposition}
    \label{sec:construct-isotypic}
    The hypercube graph has a number of nice properties.  In particular, it is
    distance-transitive. 
    \begin{definition}
        A \emph{distance-transitive} graph is a graph such that given any two
        vertices $u_1$ and $v_1$ at distance $d$ and any other two vertices
        $u_2$ and $v_2$ also at distance $d$, there exists some automorphism
        $\sigma$ of the graph with $\sigma(u_1) = u_2$ and $\sigma(v_1) = v_2$.
    \end{definition}
    The fact that the hypercube graph is distance-transitive allows us to
    compute the isotypic decomposition of $L(\xs)$.  Recall that $\xs = \{0,
    1\}^n$ is the set of vertices of the hypercube graph and that the relevant
    group $S_2 \wr S_n$ is the automorphism group of the graph.  Furthermore,
    $L(\xs)$ is a representation of $S_2 \wr S_n$ with an action induced by the
    action on $\xs$.

    The representation $L(\xs)$ for a distance-transitive graph decomposes into
    the eigenspaces of a particular matrix.  
    \begin{definition}
        The \emph{Laplacian matrix} of a graph with vertices $\{v_1, \ldots,
        v_m\}$ is the $m\times m$ matrix $L = (l_{i,j})_{i,j=1}^m$ defined by
        \[
            l_{i,j} = \begin{cases}
                \mathrm{deg}(v_i) & \text{if $i=j$} \\
                -1 & \text{if $i \ne j$ and $v_i$ is adjacent to $v_j$} \\
                0 & \text{otherwise}
            \end{cases}
        \]
        It is also the difference between the degree matrix and the adjacency
        matrix of the graph.
    \end{definition}
    The Laplacian matrix of the $2$-hypercube graph is show below.
    \[
        \raisebox{-2.75em}{
        \begin{tikzpicture}
          \node (00) at (-1, 1) {00};
          \node (01) at (1, 1) {01};
          \node (10) at (-1, -1) {10};
          \node (11) at (1, -1) {11};
          \foreach \from/\to in {00/01, 01/11, 11/10, 10/00}
            \draw (\from) -- (\to);
        \end{tikzpicture}
        }
        \qquad\longrightarrow\qquad
        \kbordermatrix{
               & 00 & 01 & 10 & 11 \\
            00 &  2 & -1 & -1 &  0 \\
            01 & -1 &  2 &  0 & -1 \\
            10 & -1 &  0 &  2 & -1 \\
            11 &  0 & -1 & -1 &  2 \\
        }
    \]
    Of course, the Laplacian matrix may be viewed as a linear transformation on
    $L(\xs)$.  Objects like the Laplacian matrix arise in the field of
    \emph{spectral graph theory}.

    The following theorem makes precise the relationship between the Laplacian
    matrix and the isotypic decomposition of $L(\xs)$ in the case of a
    distance-transitive graph.  It is a close corollary of a result by
    \citeauthor{Sta84}, produced by translating a result for the adjacency
    matrix into a result for the Laplacian matrix.  Because a distance
    transitive graph is regular, the eigenspaces of the Laplacian matrix and of
    the adjacency matrix are the same.  If each vertex has degree $d$, then an
    eigenspace with eigenvalue $\lambda$ with respect to the adjacency matrix
    has eigenvalue $d - \lambda$ with respect to the Laplacian matrix.

    \begin{theorem}[\citet{Sta84}]
        Let $\xs$ be the vertices of a distance transitive graph, and let $G$ be
        the automorphism group of the graph, so that $L(\xs)$, the space of
        complex-valued functions on $\xs$, is a representation of $G$.  Then the
        isotypic decomposition of $L(\xs)$ is multiplicity-free, and is given by
        \[
            L(\xs) = W_{\lambda_1} \oplus \cdots \oplus W_{\lambda_k}
        \]
        where the $\lambda_i$ are distinct eigenvalues of the Laplacian matrix
        and $W_{\lambda_i}$ is the eigenspace corresponding to $\lambda_i$.  In
        particular, the eigenspaces of the Laplacian matrix are irreducible
        representations of $G$.
        \label{thm:dist-trans}
    \end{theorem}

    The eigenspaces for the Laplacian and adjacency matrices of the
    $n$-hypercube graphs have a recursively defined basis.
    \begin{theorem}[\cite{CW06}]
        Let $Q_n$ be the adjacency matrix of the $n$-hypercube graph.  If $v$
        is an eigenvector of $Q_{n-1}$ with eigenvalue $\lambda$, then the
        concatenated vectors $\adel{v_1, \ldots, v_{2^{n-1}},v_1, \ldots,
        v_{2^{n-1}}}$ and $\adel{v_1, \ldots, v_{2^{n-1}}, -v_1, \ldots,
        -v_{2^{n-1}}}$ are eigenvectors of $Q_n$ with eigenvalues $\lambda +1$
        and $\lambda - 1$ respectively.  All eigenvectors of $Q_n$ can be
        written in this form for some eigenvector of $Q_{n-1}$ when $n \ge 1$.
        \label{thm:eigs}
    \end{theorem}
    \begin{proof}
        Let $\Lambda_n$ be the set of eigenvalues of the adjacency matrix $Q_n$
        of the \mbox{$n$-hypercube graph}.  Because it has no edges, the
        adjacency matrix of the $0$-hypercube graph is \mbox{$Q_0 =
        \begin{bmatrix} 0 \end{bmatrix}$}, so $\Lambda_0 = \{0\}$.  From
        Theorem~\ref{thm:eigs}, 
        \[
            \Lambda_n = 
            \{\lambda + 1 \STB \lambda \in \Lambda_{n-1}\}
            \cup
            \{\lambda - 1 \STB \lambda \in \Lambda_{n-1}\}
        \]
        for all $n \ge 1$, so
        \[
            \Lambda_n = \{-n, -n + 2, -n + 4, \ldots, n\}
        \]
        for all $n \ge 0$.  Each vertex of the $n$-hypercube graph has $n$
        neighbors, so the eigenvalues of the Laplacian matrix are
        \[
            \{n -\lambda \STB \lambda \in \Lambda_n\}
            =
            \{0, 2, 4, \ldots, 2n\}
        \]
        as desired.
    \end{proof}

    These results give an explicit algorithm to compute the isotypic
    decomposition of $L(\xs)$ for $\xs = \{0, 1\}^n$.  Each irreducible
    representation contained in $L(\xs)$ is an eigenspace $W_{2k}$, where $2k$ is
    its eigenvalue with respect to the Laplacian matrix.  
    
    Begin with the vector $v = (1)$, which is an eigenvector of the matrix for
    the $0$-hypercube.  For $n$ steps, we replace $v$ with either $(v, v)$ for
    $(v, -v)$.  To get a vector with eigenvalue $2k$, in $n-k$ steps we need to
    replace $v$ with $(v, v)$ and in $k$ steps we need to replace $v$ with $(v,
    -v)$.  This procedure yields $n \choose k$ orthogonal eigenvectors in
    $W_{2k}$, and $2^n$ eigenvectors from all eigenspaces.

    The eigenvectors generated from this procedure are of a particular form.

    \begin{corollary}
        An eigenbasis of $L(\xs)$ with respect to the Laplacian matrix of the
        $n$-hypercube graph is given by the so-called Walsh functions on $\{0,
        1\}^n$.
        \label{cor:walsh}
    \end{corollary}

    \begin{example}
        The eigenspaces of the Laplacian matrix of the $3$-hypercube graph, or
        equivalently the irreducible representations in $L(\{0,1\}^3)$, have the
        following bases.
        \begin{figure}[H]
        \[
            W_0 = \adel*{
            \begin{rmatrix}
                1 \\ 1 \\ 1 \\ 1 \\ 1 \\ 1 \\ 1 \\ 1
            \end{rmatrix}
            }
            \qquad
            W_2 = \adel*{
            \begin{rmatrix}
                1 \\ -1 \\ 1 \\ -1 \\ 1 \\ -1 \\ 1 \\ -1
            \end{rmatrix}
            ,
            \begin{rmatrix}
                1 \\ 1 \\ -1 \\ -1 \\ 1 \\ 1 \\ -1 \\ -1
            \end{rmatrix}
            ,
            \begin{rmatrix}
                1 \\ 1 \\ 1 \\ 1 \\ -1 \\ -1 \\ -1 \\ -1
            \end{rmatrix}
            }
        \]
        \[
            W_4 = \adel*{
            \begin{rmatrix}
                1 \\ -1 \\ -1 \\ 1 \\ 1 \\ -1 \\ -1 \\ 1
            \end{rmatrix}
            ,
            \begin{rmatrix}
                1 \\ -1 \\ 1 \\ -1 \\ -1 \\ 1 \\ -1 \\ 1
            \end{rmatrix}
            ,
            \begin{rmatrix}
                1 \\ 1 \\ -1 \\ -1 \\ -1 \\ -1 \\ 1 \\ 1
            \end{rmatrix}
            }
            \qquad
            W_6 = \adel*{
            \begin{rmatrix}
                1 \\ -1 \\ -1 \\ 1 \\ -1 \\ 1 \\ 1 \\ -1
            \end{rmatrix}
            }
            \qquad
        \]
        \caption{Eigenbasis of $L(\{0, 1\}^3)$}
        \end{figure}
        \noindent A function $f: \{0, 1\}^n \to \R$ is represented as a column
        vector containing the values of $f$ evaluated on each binary string in
        lexicographic order.
        \label{ex:eigenbasis}
    \end{example}

\section{A Parametrization for Binary Log-Linear Models}
    
    We now give a parametrization for $L(\xs)$ in accordance with
    Theorem~\ref{thm:thm}.  Our explicit computations are for the case $n=3$,
    but the procedure is general.
    
    We can construct the isotypic decomposition 
    \[
        L(\xs) = W_0 \oplus W_2 \oplus \cdots \oplus W_{2n}
    \]
    by the algorithm described in Section~\ref{sec:construct-isotypic}.  Each
    $W_{2k}$ is the eigenspace of the Laplacian matrix with eigenvalue $2k$, and
    has a basis as in Example~\ref{ex:eigenbasis}.  
    

    Let the base point in $\xs$ be $x_0 = 00 \cdots 0$.  Then the stabilizer $K
    = \mathrm{fix}_G(x_0)$ in $S_2 \wr S_n$ is the copy of $S_n$ in $S_2 \wr
    S_n$ that rearranges the bits but does not flip any bit.  Notice that under
    the action of $S_n$, vectors in any one of the above bases are mapped to
    another vector in the basis.  It follows that the sum of the vectors in any
    basis is $K$-invariant.  Thus the vectors
    \[
        \begin{rmatrix}
            1 \\ 1 \\ 1 \\ 1 \\ 1 \\ 1 \\ 1 \\ 1
        \end{rmatrix}
        \in W_0,
        \quad
        \begin{rmatrix}
            3 \\ 1 \\ 1 \\ -1 \\ 1 \\ -1 \\ -1 \\ -3
        \end{rmatrix}
        \in W_2,
        \quad
        \begin{rmatrix}
            3 \\ -1 \\ -1 \\ -1 \\ -1 \\ -1 \\ -1 \\ 3
        \end{rmatrix}
        \in W_4,
        \quad
        \text{and }
        \begin{rmatrix}
            1 \\ -1 \\ -1 \\ 1 \\ -1 \\ 1 \\ 1 \\ -1
        \end{rmatrix}
        \in W_6
    \]
    are all $K$-invariant.  Acting on the vectors with $S_2 \wr S_n$, and
    identifying vectors which are scalar multiples of each other, we get the
    following spanning sets.
    \begin{figure}[H]
    \begin{gather*}
        W_0 = \adel*{
        \begin{rmatrix}
            1 \\ 1 \\ 1 \\ 1 \\ 1 \\ 1 \\ 1 \\ 1
        \end{rmatrix}
        }
        \qquad
        W_2 = \adel*{
        \begin{rmatrix}
            3 \\ 1 \\ 1 \\ -1 \\ 1 \\ -1 \\ -1 \\ -3
        \end{rmatrix}
        ,
        \begin{rmatrix}
            1 \\ 3 \\ -1 \\ 1 \\ -1 \\ 1 \\ -3 \\ -1
        \end{rmatrix}
        ,
        \begin{rmatrix}
            1 \\ -1 \\ 3 \\ 1 \\ -1 \\ -3 \\ 1 \\ -1
        \end{rmatrix}
        ,
        \begin{rmatrix}
            1 \\ -1 \\ -1 \\ -3 \\ 3 \\ 1 \\ 1 \\ -1
        \end{rmatrix}
        } 
        \\
        \nonumber
        W_4 = \adel*{
        \begin{rmatrix}
            3 \\ -1 \\ -1 \\ -1 \\ -1 \\ -1 \\ -1 \\ 3 
        \end{rmatrix}
        ,
        \begin{rmatrix}
            -1 \\ 3 \\ -1 \\ -1 \\ -1 \\ -1 \\ 3 \\ -1
        \end{rmatrix}
        ,
        \begin{rmatrix}
            -1 \\ -1 \\ 3 \\ -1 \\ -1 \\ 3 \\ -1 \\ -1
        \end{rmatrix}
        ,
        \begin{rmatrix}
            -1 \\ -1 \\ -1 \\ 3 \\ 3 \\ -1 \\ -1 \\ -1
        \end{rmatrix}
        }
        \qquad
        W_6 = \adel*{
        \begin{rmatrix}
            1 \\ -1 \\ -1 \\ 1 \\ -1 \\ 1 \\ 1 \\ -1
        \end{rmatrix}
        }
        \qquad
    \end{gather*}
    \caption{Spanning set of invariant vectors for $L(\{0, 1\}^3)$}
    \label{fig:spanning}
    \end{figure}

\section{Other Parametrizations}

    Some of the motivation for identifying a spanning set comes from the paper
    by \citet{SPEC}.  In that paper, the authors consider a number of spanning
    sets for $L(\xs)$ where $\xs = \{0, 1\}^n$.  With these spanning sets, the
    authors fit log-linear models to binary data using $L_1$ regularization as
    described in Section~\ref{sec:regularization} of this paper.

    The spanning sets considered in the paper by \citet{SPEC}, along with their
    names for them, are as follows.

    The \emph{full parametrization} contains functions of the form
    \[
        f_{S, g}(b_1 \cdots b_n) = \begin{cases}
            1 & \text{when $b_i = g(i)$ for all $i \in S$} \\
            0 & \text{otherwise}
        \end{cases}
    \]
    where $S$ is any subset of $\{1, \ldots, n\}$ and $g$ is any function $S \to
    \{0, 1\}$.  This parametrization contains $3^n$ functions.

    The \emph{canonical parametrization} contains functions of the form
    \[
        f_S(b_1 \cdots b_n) = \begin{cases}
            1 & \text{when $b_i = 1$ for all $i \in S$} \\
            0 & \text{otherwise}
        \end{cases}
    \]
    for all $S \subset \{1, \ldots, n\}$.  This parametrization contains $2^n$
    functions.

    The \emph{spectral parametrization} contains the $2^n$ Walsh functions on
    $\{0, 1\}^n$ as in Corollary~\ref{cor:walsh}.

    Our spanning set as in Figure~\ref{fig:spanning} shares with the spectral
    parametrization the properties that the set is invariant (as a set) under
    the action of $S_2 \wr S_n$, and that appropriate subsets of the set generate
    the irreducible representations in $L(\xs)$.  
    
    Additionally, the vectors generated by this abstract procedure seem to be
    amenable to admitting interpretation.  Each vector is invariant under some
    stabilizer subgroup, and heavily weights the outcomes fixed under that
    subgroup.  In this case, each stabilizer actually fixes two points of the
    sample space.  In the space of first-order effects, vectors concentrate
    probability around a single outcome.  In the space of second-order effects,
    vectors have a `polarizing' effect; an outcome and its opposite (e.g. $001$
    and $110$) are both heavily weighted.

\chapter{Conclusion}

\section{Summary}

    We have put forward an approach to finding sparse log-linear models on
    sample spaces which contain significant structure.  The advantage of an
    algebraic approach is that it reduces the number of arbitrary choices
    necessary for a model selection algorithm.

    The effect of Theorem~\ref{thm:thm} is that under certain conditions, we can
    construct a spanning set of $L(\xs)$ that is natural, in the sense that it
    depends only on $\xs$ and its automorphism group.  In the case of binary
    data, these vectors also seem to admit interpretation.  An algorithm such as
    that in Section~\ref{sec:regularization} can select a small subset of these
    vectors that together describe the observed data well, restricting model
    complexity.

\section{Future Work}

    Future work could more closely examine the results of this approach on other
    sample spaces.  For example, the symmetric group and quotient spaces of it
    have applications in voting theory.  While the interaction between
    representation theory and statistics has been examined before in this
    setting, applications to model construction from the statistical learning
    perspective have not been, to the author's knowledge.

    It is possible other settings are superior to $\{0, 1\}^n$ for this type of
    construction.  If the stabilizer of a base point is a smaller group, then
    there are more $K$-invariant vectors.  In that case, a chain of groups the
    Gelfand condition yields a basis for the $K$-invariant vectors; this is a
    \emph{Gelfand-Tsetlin basis} \citep{Harm}.  It is possible even that the
    stabilizer will is the trivial group, in which case a basis for all of
    $L(\xs)$ is recovered.  One potential issue with such an approach is that
    the basis will depend on the choice of subgroup chain.  

    The investigation into binary models was launched by questions about the
    geometry of a certain mixture model of log-linear binary models, known as
    the \emph{restricted Boltzmann machine}.  It is the model presented in
    Example~\ref{ex:boltz}, with the additional stipulations that some of the
    variates are hidden and that there are no interactions between visible and
    hidden variates.  The visible distribution is the marginalization over all
    hidden states.  There have recently been advances in understanding the
    geometry of this model with tools from algebraic geometry and tropical
    geometry \citep{CMS09}, but some questions remain open.  In general, the
    interaction between mixture models and log-linearity remains incompletely
    understood \citep{DSS08}.  It is possible that concerns of symmetry and
    sparsity could play a role in better understanding and using such models.

\appendix

% \nocitep{*}


\backmatter

\bibliographystyle{hmcmath}
\bibliography{thesis}

%\printindex


\end{document}






