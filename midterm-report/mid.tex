\documentclass[11pt,titlepage]{article}
\usepackage{pset}

\newcommand*{\X}{\mathfrak{X}}
\newcommand*{\Mod}{\mathcal{M}}
\newcommand*{\Bin}{\mathcal{B}}
\newcommand*{\vbar}{\;\big\vert\;}
\newcommand*{\mle}{\theta_{\text{mle}}}
\DeclareMathOperator{\Mixt}{Mixt}
\DeclareMathOperator{\intr}{int}
\DeclareMathOperator{\Sec}{Sec}
\DeclareMathOperator*{\argmax}{arg\ max}
\newcommand*{\parop}[1]{\frac{\partial}{\partial #1}}

\numberwithin{equation}{section}

\title{The Restricted Boltzmann Machine}
\author{Aaron Pribadi}
\date{November 2011}

\begin{document}

\maketitle

\tableofcontents

\pagebreak

\section{What is Algebraic Statistics?}

    Algebraic statistics is a relatively new field that examines statistical
    questions using algebraic geometry and commutative algebra.  Once a problem
    has been cast in the language of algebra, a number of computational tools
    can be brought to bear.  For example, one of the early papers in the field
    analyzed contingency tables using Monte Carlo sampling computed with Gröbner
    bases \cite{DS98}.

    Algebraic statistics also offers a geometric point of view on statistical
    models; the tendency is toward intrinsically defined objects in lieu of
    explicit coordinate systems.  In this light, algebraic statistics might be
    seen as in the tradition of information geometry, a field pioneered in the
    1980s that applied the techniques of Riemannian geometry to probability
    models (see for example \cite{Ama}).

    An introduction to the field of algebraic statistics may be found in the
    collection of lecture notes \cite{DSS08} and a brief history of the field
    may be found in the article \cite{Ric09}.  Algebraic statistics has also
    been applied to computational biology \cite{ASCB}.

    Our goal is to examine a particular statistical model, the Restricted
    Boltzmann Machine, from the point of view of algebraic statistics building
    from prior work in that direction, especially \cite{CMS09}.  The Restricted
    Boltzmann Machine has recently become the centerpiece of certain
    developments in the machine learning community \cite{Hin07}, and several
    basic questions about its geometry remain unanswered.

\section{The Geometry of Statistical Models}
    \subsection{The Probability Simplex}

    We consider probability distributions over finite sets.  The space of all
    such distributions corresponds to a geometric object.
    
    \begin{definition} The \emph{probability simplex} of dimension $N$ (also
    called the \emph{standard simplex}) is the space
    \[
        \Delta_N = 
        \left\{(x_1, \ldots, x_{N+1}) \vbar \sum_{i=1}^{N+1} x_i= 1, x_i \ge 0 \right\} 
        \subset
        \R^{N+1}.
    \]
    The subscript $N$ indicates the dimension of the simplex.
    \end{definition}
    A \emph{simplex} in general is the image of a standard simplex under any
    affine transformation.  Low dimensional simplices are familiar objects;
    $\Delta_0$ is a point, $\Delta_1$ is a line segment, $\Delta_2$ is a
    triangle, and $\Delta_3$ is a tetrahedron.
    \begin{center}
    \scalebox{1}{ \includegraphics[scale=0.5]{images/simplices.png} }
    \end{center}
    A random variable $X$ with $N+1$ possible values corresponds to a point $x =
    (x_1, \ldots, x_{N+1})$ contained in the simplex $\Delta_N$ in a natural
    way; we set $P(X = i) = x_i$.  We generally identify a probability
    distribution with its corresponding point in the probability simplex.
    
    Statisticians are often concerned with families of probability
    distributions.  We identify such families with geometric spaces.
    \begin{definition}
    By the phrase \emph{statistical model}, we mean a subset $\Mod \subset
    \Delta_N$ of a probability simplex.
    \end{definition}
    \noindent A statistical model may be parametrized by a map $f: U \to
    \Delta_N$, for some space $U$ of parameters.  Usually, the space of
    parameters is a subset of a real affine space $\R^d$, and parametrizations
    are differentiable almost everywhere.  In the context of algebraic
    statistics, the parametrization is usually a rational function.
    \begin{example}
    A random variable $X$ following a binomial distribution with size $N$ and
    parameter $\lambda \in [0,1]$ takes a value $k \in \{0,\ldots, N\}$ with
    probability
    \[
        P_\lambda[X = k] = {N \choose k} \lambda^k(1-\lambda)^{n-k}.
    \]
    This is the number of heads produced by $n$ `coin tosses', where $\lambda$
    is the probability of a head.  The map $\lambda \mapsto P_\lambda$
    determines a parametrized statistical model $\{P_\lambda : \lambda \in [0,1]
    \} \subset \Delta_N$. The statistical model is a curve, i.e. a
    one-dimensional subspace, of the simplex.
    \end{example}
    
\subsection{Maximum Likelihood Estimation}

    Given data from an unknown distribution and statistical model $\Mod \subset
    \Delta_N$ of candidate distributions, there is a natural way to select a
    distribution from the model.  We use discrete distributions throughout,
    though these definitions can be made more general.

    Let $\Mod = \{P_\theta : \theta \in U\}$ be a parametrized statistical
    model. A random variable $X$ distributed according to some $P_\theta$ has
    the probability mass function $p_\theta(x) = P_\theta[X = x]$.  Suppose that
    we have some data $Z = \{z_1, \ldots, z_n\}$.  The data are generally assumed to
    be independent and identically distributed according to some unknown true
    distribution in $\Mod$.
    
    \begin{definition}
    The \emph{likelihood function} of $\theta \in U$ given the data $Z$ is
    \[
        L(\theta; Z) = \prod_{i=1}^n p_\theta(z_i).
    \]
    It is the probability that the observed data $Z$ would occur if the
    observations were independent and identically distributed and following
    $P_\theta$.  The \emph{log-likelihood function} is
    \[
        l(\theta; Z) = \sum_{i=1}^n \log p_\theta(z_i)
    \]
    and is often used in place of the likelihood because it is additive.
    \end{definition}
    \begin{definition}
    The \emph{maximum likelihood estimate} of the true parameter is the
    parameter 
    \[
        \mle = \argmax_{\theta \in U} L(\theta; Z)
    \]
    that maximizes the likelihood (or equivalently the log-likelihood) of the
    data.
    \end{definition}

    Notice that the maximum likelihood estimate might not be unique and might
    not even exist.  In practice it is often the case that an exact value for
    $\mle$ is not computed, and instead a good approximation is found via
    numerical techniques.

\subsection{Solutions to the Likelihood Equations}

    Here, we give a flavor of how algebro-geometric techniques can be applied to
    maximum likelihood estimation.  Our exposition follows Section 3.3 of
    \cite{ASCB} and Section 2.1 of \cite{DSS08}.

    Suppose that $g: U \to \Delta_{N-1}$ is a rational parametrization of a
    statistical model, where $U$ is an open subset of $\R^d$.  By rational, we
    mean that each component of $g(\theta) = (g_1(\theta), \ldots, g_N(\theta))$
    is a rational function with rational coefficients in its argument $\theta
    \in U \subset \R^d$.  Maximum likelihood estimation then amounts to
    maximizing the function
    \[
        l(\theta) = \sum_{i=1}^N u_i \log g_i(\theta)
    \] 
    where $u_i$ is the number of times event $i$ has occurred in the observed
    data.

    Every local and global maximum $\theta \in U$ is a solution to the
    likelihood equations
    \begin{equation}\label{eq:lik}
        \frac{\partial l}{\partial\theta_j}
        =
        \sum_{i=1}^N 
        \frac{u_i}{g_i} 
        \cdot
        \frac{\partial g_i}{\partial \theta_j}
        = 0
        \qquad
        \text{for $j = 1,\ldots,d$}.
    \end{equation}
    The likelihood equations in \eqref{eq:lik} are again rational functions of
    $\theta$.  Solving the equations involves `clearing denominators' and
    solving the polynomial equations
    \begin{equation}\label{eq:lik-clear}
        \sum_{i=1}^N 
        u_i \cdot g_1 \cdots \widehat{g_i} \cdots g_N
        \frac{\partial g_i}{\partial \theta_j}
        = 0
        \qquad
        \text{for $j = 1, \ldots, d$}
    \end{equation}
    where $\widehat{g_i}$ indicates that the $i$-th factor is omitted.  The
    equations \eqref{eq:lik-clear}, however, introduce extraneous solutions, for
    example when $g_a(\theta) = g_b(\theta) = 0$ for any $a \ne b$.

    Algebraic geometry offers a principled way to solve the likelihood
    equations.  (For a brief introduction to affine and projective varieties, see
    Appendix \ref{sec:varieties}.)  Consider the ideal $I \subset \R[\theta_1,
    \ldots, \theta_d]$ generated by the polynomial expressions in \eqref{eq:lik-clear}
    \[
        I = \adil[\Bigg]{
            \sum_{i=1}^N u_i \cdot g_1 \cdots \widehat{g_i} \cdots g_N
            \frac{\partial g_i}{\partial \theta_j}
        }_{j=1}^d
        .
    \]
    Let $h$ be the product of all polynomials appearing in the denominators of
    the rational equations in \eqref{eq:lik}.  The \emph{saturation ideal} of
    $I$ with respect to $h$ is defined
    \[
        (I : h^\infty) = \cdil[\big]{
            f \in \R[\theta_1, \ldots, \theta_d]
            \;\big\vert\;
            fh^k \in I
            \quad
            \text{for some non-negative integer $k$}
        }.
    \]
    We can then consider the variety corresponding to the saturation ideal.  The
    points in $V(I : h^\infty) \cap U$ are all solutions to the likelihood
    equations.  In the case that there are only finitely many solutions, passing
    to the saturation ideal removes all extraneous solutions.

    The referenced works \cite{ASCB} and \cite{DSS08} include examples of
    computing the variety $V(I : h^\infty)$ with the software package
    \texttt{Singular}.  An overview of the techniques used to solve such
    polynomial equations, e.g. Gröbner bases, resultants, and elimination, is
    given in the text \cite{CLO05} on computational algebraic geometry.
% 
% \subsection{Implicit Models}

    % In the context of algebraic statistics, we usually look at statistical
    % models that are semialgebraic sets.

    % If a statistical model is a semi algebraic set, then we can pass to its
    % Zariski closure in affine space, or we can embed it in projective space
    % and takes its Zariski closure there.

    % A simplex is the locus of a finite collection of polynomial equations
    % and polynomial inequalities and therefore is a semialgebraic set.

    % Then we can solve the likelihood equations, given the defining ideal?


\section{The Restricted Boltzmann Machine}

    We now focus on a statistical model that has recently become important to
    the machine learning community in the pursuit of so-called `deep learning'
    architectures.  In particular, this model is the key component of the `Deep
    Belief Network' which has achieved considerable success at a number of
    machine learning tasks \cite{Hin07}.  For more about the context in which
    the Restricted Boltzmann Machine has become important, see the overview of
    machine learning in Appendix \ref{sec:ML}.

\subsection{The Model}
    \label{sec:rbm-def}

    \begin{definition}
    The \emph{Restricted Boltzmann Machine} (RBM) with $n$ visible units and $k$
    hidden units has binary states of the form $(v, h)$, where $v \in \{0,1\}^n$
    and $h \in \{0,1\}^k$ are binary vectors.  It has real parameters $w_{ij}$,
    $b_i$, and $c_j$, with indices ranging $1 \le i \le n$ and $1 \le j \le k$.
    Its unnormalized joint distribution is
    \begin{equation}\label{eq:rbm-unnorm}
        \psi(v, h) = \exp\pdil*{
            \sum_{ij} w_{ij} v_i h_j + \sum_i b_i v_i + \sum_j c_j h_j
        }
    \end{equation}
    and its actual joint distribution is 
    \[
        p(v, h) = \frac{\psi(v,h)}{Z}
    \] 
    where the normalizing constant $Z = \sum_{v,h} \psi(v, h)$ is the called the
    \emph{partition function}.  The distribution over its visible units is
    \[
        p(v) = \sum_{h \in \{0,1\}^k} p(v,h)
    \]
    a \emph{marginalization} over the hidden states.
    \end{definition}
    The reader familiar with statistical mechanics may recognize this as a
    Boltzmann distribution with energy $H(v,h) = - \log \psi(v, h)$.  The energy
    $H$ is a quadratic polynomial in the visible and hidden variables.  Because
    $H$ contains no terms with two visible or two hidden variables, the
    Restricted Boltzmann Machine is a \emph{graphical model} that factors
    according to the complete bipartite graph $K_{n,k}$ on the visible and
    hidden nodes.
    \begin{center}
    \scalebox{1.0}{ \includegraphics[scale=0.8]{images/rbm.png} }
    \end{center}
    Indeed, if we, for example, take the value $h$ of the hidden units to be
    fixed, then $v \mapsto H(v, h)$ is a linear function and the expression
    $e^{H(v,h)}$ decomposes into a product of functions, each dependent on a
    single component of $v = (v_1, \ldots, v_n)$.  It follows that conditioned
    on a fixed value of $h$, the components $v_i$ are independent random
    variables.  For a description of graphical models in general from an
    algebro-geometric viewpoint, refer to \cite{DSS08}.  A general (as opposed
    to restricted) Boltzmann machine would allow $H$ to be an arbitrary
    quadratic polynomial, and would only factor according to the complete graph
    on all units.
    
    
\subsection{The Geometry of the Restricted Boltzmann Machine}
    The recent paper \cite{CMS09} examines the Restricted Boltzmann Machine from
    an algebro-geometric perspective.  We outline a small portion of its
    approach.
    
    In the context of algebraic statistics, we want a rational parametrization
    of the model.  To that aim, we replace the parameters $w_{ij}$, $b_i$, and
    $c_j$ used in Section \ref{sec:rbm-def} with
    \[
        \omega_{ij} = \exp(w_{ij})
        \qquad
        \beta_i = \exp(b_i)
        \qquad
        \gamma_j = \exp(c_j)
    \]
    so that $\omega_{ij}$, $\beta_i$, and $\gamma_j$ range over the strictly
    positive real values.  That is, the parameter space is $\R_{>0}^{nk+n+k}$.
    Under this parametrization, the unnormalized probability distribution
    \eqref{eq:rbm-unnorm} becomes
    \[
        \psi(v, h) = 
            \pdil*{ \prod_{i=1}^k \prod_{j=1}^n \omega_{ij}^{h_i v_j} }
            \pdil*{ \prod_{i=1}^n \beta_i^{v_i} }
            \pdil*{ \prod_{j=1}^k \gamma_j^{h_j} }
    \]
    which is a polynomial for any particular value of the binary vectors
    $(v,h)$.  The marginalized distribution over for the visible variables
    factors as
    \begin{equation} \label{eq:rbm-alg-par}
        p(v) = \frac 1 Z
        \beta_1^{v_1} \beta_2^{v_2} \cdots \beta_n^{v_n} 
        \prod_{i=1}^k
        (1 + \gamma_i \omega_{i1}^{v_1} \omega_{i2}^{v_2} \cdots \omega_{in}^{v_n})
    \end{equation}
    where the factors $(1 + \gamma_i \omega_{i1}^{v_1} \omega_{i2}^{v_2} \cdots
    \omega_{in}^{v_n})$ correspond to the choice of whether $h_i = 0$ or $h_i =
    1$.  The partition function $Z = \sum_{v,h} \psi(v,h)$ is also a polynomial
    in the new parameters $\omega$, $\beta$, and $\gamma$,  so the full
    parametrization $\R_{>0}^{nk+n+k} \to \Delta_{2^n-1}$ of the model is a
    rational map.  

    Let $M_n^k \subset \Delta_{2^n-1}$ denote the image of the parametrization,
    and let $V_n^k$ denote the Zariski closure of $M_n^k$ in the complex
    projective space $\Proj^{2^n-1}$.  The model $M_n^k$ is a semialgebraic set.
    The referenced paper gives a structural result about the model $M_n^k$ and
    the variety $V_n^k$.
    \begin{theorem}[\cite{CMS09}] \label{thm:factor}
    The RBM variety and model factor as the Hadamard powers
    \[
        V_n^k = (V_n^1)^{[k]}
        \qquad\text{and}\qquad
        M_n^k = (M_n^1)^{[k]}
    \]
    of the variety and model with one hidden unit.
    \end{theorem}
    \noindent The Hadamard product of two statistical models $M, N \subset
    \Delta_{N-1}$ (consisting of strictly positive distributions) is defined to
    be
    \[
        M * N = \cdil[\big]{p * q \mid p \in M, q \in N}
        \qquad
        \text{where}
        \qquad
        (p * q)_i = \frac{p_i\,q_i}{\sum_{j=1}^N p_j\,q_j}.
    \]
    The Hadamard product $p*q$ may be thought of as the normalized
    coordinate-wise product of the two distributions.  The Hadamard product $X *
    Y$ of two subvarieties $X$ and $Y$ of a projective space $\Proj^{N-1}$ is
    defined to be the Zariski closure of the image of the rational map
    \[
        X \times Y \dashrightarrow \Proj^m,
        \quad
        (x, y) \mapsto [x_1y_1 : x_2y_2\cdots : x_N y_N].
    \]
    Given a product operation, positive integer powers are, as usual,
    recursively defined $M^{[1]} = M$ and $M^{[k]} = M * M^{[k-1]}$, and
    similarly for $V$.  Taking Hadamard powers `commutes' with taking the
    Zariski closure.

    The variety $V_n^1$ is the first secant variety of the \emph{Segre
    embedding} of the product of $n$ projective lines $\Proj^1 \times \cdots
    \times \Proj^1$ into $\Proj^{2^n-1}$.  The Segre embedding of the Cartesian
    product of two subvarieties $X \subset \Proj^m$ and $Y \subset \Proj^n$ is
    the image of the map
    \[
        X \times Y \to \Proj^{(m+1)(n+1) - 1},
        \quad
        (x, y) \mapsto
        [x_0y_0: x_0 y_0 : \cdots : x_my_n]
    \]
    where the indices are in lexicographic order; the Segre embedding of the
    product of multiple varieties is defined analogously.
    
    The corresponding statistical model $M_n^1$ with one hidden value is a
    mixture of two independence models; both independence models and mixture
    models will be discussed in more detail in later sections.

\subsection{The Nested Sequence of RBM Models}

    It seems that adding more hidden units should not decrease the
    representational power of the Restricted Boltzmann Machine.  Indeed, we have
    the bounded nested sequence of models $\{M_n^k\}_{k=0}^\infty$ where
    \[
        M_n^0 \subset M_n^1 \subset \cdots 
        \qquad\text{and}\qquad
        M_n^k \subset \Delta_{2^n-1}
        \qquad\text{for all $k \in \Z^+$}.
    \]
    Indeed, any model in $M_n^k$ with parameters $w_{ij}$, $b_i$, and $c_j$ is
    contained in $M_n^{k+1}$ where the corresponding parameters are the same
    except for $w_{1,k+1} = \cdots = w_{n, k+1} = 0$  and $c_{k+1} = 0$.

    Some facts are already known about this sequence.
    \begin{theorem}[\cite{MA10}] \label{thm:approximator}
    With $k = 2^{n-1} - 1$, the model $M_n^k$ is dense in $\Delta_{2^n} - 1$.
    \end{theorem}
    In the cited paper, this result is phrased as ``Any distribution on
    $\{0,1\}^n$ can be approximated arbitrarily well by an RBM with $2^{n-1} -
    1$ hidden units''.  From the machine learning point of view, it is important
    to quantify the `representational power' of the RBM.

    In the absence of a defect, one would expect that both $M_n^k$ and $V_n^k$
    have the same dimension as the parametrizing space $\R^{nk + n + k}$.
    (Here, dimension is defined as usual for varieties.) In most cases, this in
    fact holds.
    \begin{theorem}[\cite{CMS09}] \label{thm:dimension}
    The Restricted Boltzmann Machine has the expected dimension
    $\min\{nk+n+k, 2^n-1\}$ when $k \le 2^{n-\lceil \log_2(n+1)\rceil}$ and
    when $k \ge 2^{n-\lfloor\log_2(n+1)\rfloor}$.
    \end{theorem}
    Notice that light of this dimensionality result, the bound given in the
    previous theorem is not necessarily sharp; there exist smaller $k$ such that
    the models have the requisite full dimension.

\subsection{Open Problems}

    A question naturally suggested by the universal approximation result in
    Theorem \ref{thm:approximator} and the dimensionality result in Theorem
    \ref{thm:dimension} is whether the bound given in the former is sharp.  That
    is, does there exist $k < 2^{n-1}-1$ such that $M_n^k$ is dense in
    $\Delta_{2^n-1}$?

    Less concretely, it is desirable to get a better handle on the geometry of
    the RBM, keeping in view the goal of training RBMs of practical sizes.  A
    typical RBM used in a Deep Belief Network might have several hundred to
    several thousand visible and hidden units \cite{Hin07}.  While algorithmic
    innovations in the machine learning community have made training RBMs
    practical, the run time of such algorithms is still measured in days, rather
    than seconds.
    
    Even though the rational parametrization of the RBM given in
    \eqref{eq:rbm-alg-par} places the task of likelihood maximization within an
    algebro-geometric framework, there are still severe computational
    difficulties with that approach.  In \cite{CMS09}, the authors turn to
    tropical techniques.  For a model with a mere four visible units and two
    hidden units, the task of computing the defining polynomial of the Zariski
    closure of the model (which has co-dimension 1) required a team to resort
    to creative techniques detailed in \cite{CTY10}.  

    Thus, any progress toward an algorithm leveraging algebro-geometric
    techniques to do either exact or approximate maximum likelihood
    more efficiently on the RBM would be of extreme interest.


\section{Components of the RBM}

    We saw in Section \ref{sec:rbm-def} that given a fixed value $h \in
    \{0,1\}^k$ for the hidden units of a RBM, the visible units are all
    independent in the conditional distribution $P( \cdot\mid h )$.  Breaking
    the conditional probability out, probability of a state $v \in \{0,1\}^n$
    for the visible units is
    \[
        P( v ) = \sum_{h \in \{0,1\}^k} P( h ) P( v \mid h ).
    \]
    There are $2^n$ different hidden states $h$, so the visible distribution is
    a weighted sum of $2^n$ distributions, such that in each component
    distribution the visible units are all independent.  The weights $P(h)$ and
    the independence distributions $P(\cdot \mid h)$ are, however, not
    arbitrary.  The range of possibilities for both determine the structure of
    the RBM.

    In this section, we consider the space of strictly positive distributions,
    denoted by $\intr(\Delta_{N-1})$.  This space contains distributions for
    which the probability of every event is nonzero, and consists of the
    interior of the simplex.

\subsection{The Hadamard Product}

    In the RBM, each hidden units influences the visible units.  The effects of
    the hidden units are combined through the Hadamard product.  (cf. Theorem
    \ref{thm:factor}).  Recall that for $p, q \in \intr(\Delta)_{N-1}$ the
    Hadamard product is defined
    \[
        (p * q)_i = \frac{p_i q_i}{\sum_{j=1}^N p_j q_j}.
    \]
    This is essentially a normalized coordinate-wise product.  The Hadamard
    product yields a straightforward algebraic structure.

    \begin{theorem}
    The space $(\intr(\Delta_{N-1}), *)$ of strictly positive distributions
    equipped with the Hadamard product is an Abelian group.
    \end{theorem}
    \begin{proof}
    By the construction of the Hadamard product, $p * q \in
    \intr(\Delta_{N-1})$, and it is obvious that the product is commutative.
    The uniform distribution
    \[
        u = \pdil*{\frac 1 N, \ldots, \frac 1 N}
    \]
    is the identity element.  The inverse of a distribution $p \in
    \intr(\Delta_{N-1})$ is 
    \[
        \inv p = \frac{1}{\sum_{j=1}^N \frac 1 {p_j}}
        \cdot\pdil*{\frac 1 {p_1}, \ldots, \frac 1 {p_N}}
    \]
    from which we see that the stipulation that distributions be strictly
    positive is necessary.
    \end{proof}

    Through a particularly nice parametrization of $\intr(\Delta_{N-1})$, we see
    that the group has a familiar structure.
    \begin{theorem} \label{thm:dist-par}
    There is an isomorphism of (Lie) groups $(\intr(\Delta_{N-1}), *) \iso
    (\R^{N-1}, +)$, where the latter group is the real vector space with the
    usual addition operation.
    \end{theorem}
    \begin{proof}
    Consider the vector subspace
    \[
        S = \pdil*{\cdil*{x \in \R^N \;\Big\vert\; \sum_{j=1}^N x_i = 0 }, +}
    \]
    as a group with the usual addition operation.  Consider the parametrization
    \[
        \varphi : S \to \intr(\Delta_{N-1})
        \qquad
        x \mapsto \frac{1}{\sum_{j=1}^N e^{x_j}} 
        \pdil*{e^{x_1}, \ldots, e^{x_N}}.
    \]
    We claim that $\varphi$ is a group isomorphism.  It is clear that $\varphi$
    is surjective.  To see that it is injective, notice that if $\varphi(x) =
    \varphi(y)$, then $(e^{x_1}, \ldots, e^{x_N}) = (\lambda e^{y_1}, \ldots,
    \lambda e^{y_N})$ for some scalar $\lambda$.  The condition $\sum x_j = \sum
    y_j = 0$, however, implies that $\prod e^{x_j} = \prod e^{y_j} = 1$, so
    $\lambda^N = 1$ and $\lambda = 1$.  Thus $\varphi$ is a bijection.

    The map $\varphi$ is also a group homomorphism.  In the computation below,
    note that both the map $\varphi$ and the Hadamard product $*$ normalize
    their values to sum to 1 to fall within $\intr(\Delta_{N-1})$, so we can
    essentially ignore a scalar (`projective') factor.  We have
    \[
        \varphi(x + y)
        \propto
        \pdil*{e^{x_1 + y_1}, \ldots, e^{x_N + y_N}}
        =
        \pdil*{e^{x_1}e^{y_1}, \ldots, e^{x_N}e^{y_N}}
        \propto
        \varphi(x) * \varphi(y)
    \]
    and $\varphi(x + y) \propto \varphi(x) * \varphi(y)$ implies that $\varphi(x
    + y) = \varphi(x) + \varphi(y)$ when both lie in $\intr(\Delta_{N-1})$.
    \end{proof}
    

\subsection{The Binary Independence Model}

    We formally introduce the binary independence model.  Throughout this
    section, let $N = 2^n$.
    \begin{definition}
    Identify the space of strictly positive distributions over $\{0,1\}^n$ with
    the probability simplex $\intr(\Delta_{N-1})$.  The coordinates of
    $\Delta_{N-1}$ may be associated with the binary vectors of length $n$
    ordered lexicographically.  The \emph{binary independence model} $\Bin
    \subset \Delta_N$ consists of the distributions that factor as
    \[
        P(v) = P(v_1) \cdots P(v_n)
    \]
    for binary vectors $v = (v_1, \ldots, v_n) \in \{0,1\}^n$.
    \end{definition}

    Notice that the binary independence model is parametrized by $(\lambda_1,
    \ldots, \lambda_n) \in (0,1)^n$ as
    \begin{equation} \label{eq:bin}
        P(v) 
        = \lambda_1^{v_1}(1 - \lambda_1)^{1 - v_1}
        \cdots \lambda_n^{v_n}(1 - \lambda_n)^{1 - v_n}.
    \end{equation}
    In fact, it is clear that this parametrization $(0,1)^n \to \Bin$ is a
    bijection.

    The binary independence model interacts well with the Hadamard product.
    \begin{theorem}\label{thm:bin-par}
    The binary independence model $\Bin$ with the Hadamard product is a subgroup
    of the space $(\intr(\Delta_{N-1}), *)$ of all strictly positive
    distributions.  Furthermore, $(\Bin, *)$ is  isomorphic to $(\R^n, +)$.
    \end{theorem}
    \begin{proof}
    For $1 \le k \le N$, let $v(k)$ be the $k$th binary vector of length $n$,
    ordered lexicographically.  Let $S$ and $\varphi$ be as in the proof of
    Theorem \ref{thm:dist-par}.  Recall that $S$ is a hyperplane in $\R^N$;
    associate the $k$th coordinate of $S \subset \R^N$ with the probability that
    the event $v(k)$ occurs.  
    
    We construct a parametrization of $\Bin$ via the previous parametrization
    $\varphi : S \to \intr(\Delta_{N-1})$. Consider the linear transformation 
    \[
        \rho: \R^n \to \R^N
        \qquad\text{given by the matrix}\qquad
        \begin{bmatrix*}[r]
            -1 & \cdots & -1 & -1 & -1 \\
            -1 & \cdots & -1 & -1 &  1 \\
            -1 & \cdots & -1 &  1 & -1 \\
            \vdots & \ddots &  & \vdots &  \\
             1 & \cdots &  1 &  1 &  1
        \end{bmatrix*}
    \]
    where the rows are the $N$ possible rows of length $n$ containing only $-1$
    and $1$, ordered lexicographically.  Several things are straightforward to
    verify.  First, $\rho(\R^n) \subset S$, as the columns of the matrix all sum
    to 0.  More importantly, $(\varphi \circ \rho)(\R^n) = \Bin$, and $\varphi
    \circ \rho$ is a bijection onto its image $\Bin$.  One can compute that a
    distribution in the binary independence model given by parameters
    $\lambda_1, \ldots, \lambda_n$ as in \eqref{eq:bin} has the unique preimage
    \[
        x \in \R^n
        \qquad
        x_i = \frac 1 2 \log \pdil*{\frac{\lambda_i}{1 - \lambda_i}}
    \]
    under $\varphi \circ \rho$.  
    
    This amounts to a re-parametrization of $\Bin$, with the advantage that
    $\rho$ is a linear map.  The matrix has full rank $n$, so $\rho$ is an
    isomorphism.  The subspace $\rho(\R^n) \subset S$ is then a subgroup of $S$
    isomorphic to both $\R^n$ and $\Bin$.
    \end{proof}

\subsection{Hidden Variables and Mixture Models}
    This section follows Chapter 4 of \cite{DSS08}.

    Suppose that a hidden variable $Y$ influences a visible variable $X$.  As
    usual, we assume that probability distributions are discrete.  If $Y$
    follows the probability distribution $\pi$, then the joint distribution of
    $X$ and $Y$ is 
    \[
        P(X = i, Y = j) = \pi_j \cdot p_i^{(j)}
    \]
    for some distributions $p^{(j)}$.  Because we consider $Y$ as the hidden
    variable, we can only observe the marginal distribution of $X$.  The
    marginal probability is the sum over the possible hidden values
    \[
        P(X = i) = \sum_{j} \pi_j \cdot p_i^{(j)}.
    \]
    A hidden variable model allows for the creation of complex model out of
    simple components $p^{(j)}$.  In particular, if the distributions $p^{(j)}$
    are required to lie in some model $\Mod \subset \Delta_{N-1}$, then we have
    a mixture model.
    \begin{definition}
    Suppose that $V_1, \ldots, V_m$ are subsets of the vector space $\R^N$.  The
    \emph{mixture} of those sets is 
    \[
        \Mixt(V_1, \ldots, V_n) = \cdil*{
            \lambda_1 v_1 + \cdots + \lambda_n v_m
            \;\big\vert\;
            v_i \in V_i, \lambda_j \ge 0, \sum_{j=1}^m \lambda_j = 1
        }.
    \]
    A \emph{mixture model} is the mixture of statistical models $\Mod_1, \ldots,
    \Mod_m \subset \Delta_{N-1}$.  
    \end{definition}
    Note that because $\Delta_{N-1}$ is a convex set, the mixture of statistical
    models is itself a statistical model.  The mixture of $m$ models contains
    the distributions that can be constructed with a hidden variable with $m$
    states, where each state induces a visible distribution taken from the
    corresponding model.

    \begin{example}
    A \emph{Gaussian mixture model} simulates a multi-modal distribution.  It is
    relatively easy to work with and is useful in a number of machine learning
    tasks.  See, for example, Sections 6.8 and 8.5 of \cite{EOSL} for an
    overview of Gaussian mixtures and the EM algorithm, commonly used to train
    the model.  A Gaussian distribution on the real line with parameters $(\mu,
    \sigma)$ has probability density
    \[
        p_{\mu, \sigma}(x) = \frac{1}{\sqrt{2\pi\sigma^2}} 
        \exp\pdil*{-\frac{(x - \mu)^2}{2\sigma^2}}.
    \]
    A mixture of two Gaussian distributions is a weighted sum $p(x) = \lambda
    p_{\mu_1, \sigma_1}(x) + (1 - \lambda)p_{\mu_2, \sigma_2}(x)$.  
    \begin{center}
    \scalebox{1}{ \includegraphics[scale=0.5]{images/mixture.png} }
    \end{center}
    Mixtures of more Gaussians in higher dimensions are defined analogously.
    Note that because the space of distributions over the real line is not
    finite dimensional, this example does not formally fit under our framework;
    the constructions, however, generalize easily.
    \end{example}

    Mixture models correspond to a relatively well-studied object in algebraic
    geometry.  (For definitions of terms, see Appendix \ref{sec:varieties}.)
    \begin{definition}
    The \emph{secant variety} of the affine variety $V \subset \F^n$ is
    \[
        \Sec(V) = \overline{\cdil[\big]{
        \lambda u + (1 - \lambda) v \vbar
        u,v \in V
        \text{ and }
        \lambda \in \F
        }}
    \]
    where the line indicates the Zariski closure of the set.  
    \end{definition}
    \begin{proposition}[\cite{DSS08}]
    If $M$ is a semialgebraic set, then the secant variety
    $\Sec\pdil[\big]{\overline{M}}$ is the Zariski closure of the mixture
    $\Mixt(M, M)$.
    \end{proposition}

    For a more thorough exposition on mixture models and secant varieties, we
    refer the reader to Chapter 4 of \cite{DSS08}.

\subsection{The RBM as a Mixture Model}

    We claim that a distribution in the RBM model $M_n^K$ is the weighted sum of
    $2^k$ distributions from the binary independence model, where both the
    weights and the component distributions are subject to certain restrictions.
    From the characterization of the RBM model given in Theorem
    \ref{thm:factor}, we can make this precise.
    \begin{proposition}
    A distribution is in the RBM model $M_n^k$ if and only if it is of the form
    \[
        \sum_{h \in \{0,1\}^k} \mu_h v_h
        \qquad
        \mu_h \in \R, \;
        v_h \in \Bin
    \]
    where for all binary vectors $h = (h_1, \ldots, h_k)$, the weight $\mu_h$
    and distribution $v_h$ are as follows.  There are $\lambda_1, \ldots,
    \lambda_k \in (0,1)$ such that
    \[
        \mu_h = \lambda_1^{h_1}(1 - \lambda_1)^{1-h_1} \cdots
        \lambda_k^{h_k}(1-\lambda_k)^{1-h_k}.
    \]
    There are distributions $t \in \Bin$ and $u_1, \ldots, u_k \in \Bin$ such
    that
    \[
        v_h = t * u_{i_1} * \cdots * u_{i_m}
    \]
    where $u_{i_j}$ is included in the product only when $h_{i_j} = 1$.
    \end{proposition}

    We make the following observation.
    \begin{proposition}
        For any distribution $p \in M_n^k$ where $k \ge 1$, there exists a
        distribution $q \in M_n^{k-1}$ such that
        \[
            p = \lambda q + (1 - \lambda)(q * u)
        \]
        for some weighting scalar $\lambda \in (0,1)$ and independence
        distribution $u \in \Bin$.
    \end{proposition}
    Using the above observation inductively, we see that we can then reach any
    distribution in $M_n^k$ in a series of $k$ `steps', where we begin from a
    distribution $t \in M_n^0 = \Bin$.

\subsection{Bounding the Growth of Nested RBM Models}

    We now move to more speculative areas, in which progress could be made

    

    % Let $\rho$ and $\varphi$ be as in the proof to Theorem \ref{thm:bin-par}.
    % We can consider the binary vector $h \in \{0,1\}^k$ as in the space $\R^k$;
    % the set of all $h$ consists of the corners of the standard $k$-dimensional
    % hypercube.  Then for a given distribution in the RBM model, there exists
    % some fixed affine map $\tau: \R^k \to \R^n$ (i.e. linear map with
    % translation) such that $v_h = (\varphi \circ \rho \circ \tau)(h)$ for all
    % $h$.

    % in terms of the BIM and mixtures
    % remember the affine image of a hypercube!



\appendix

\section{A Little Algebraic Geometry}

    The field of algebraic geometry grew out of the study of curves and surfaces
    determined by polynomials.  In the mid-twentieth century, the foundations of
    the subject were reformulated by Serre, Grothendieck, and others.  The
    techniques of the field are notoriously both abstract and powerful; the
    following quote by the algebraic geometer David Mumford is perhaps telling
    \cite{Mum99}.
    \begin{quote}
        Algebraic geometry seems to have acquired the reputation of being
        esoteric, exclusive, and very abstract, with adherents who are secretly
        plotting to take over all the rest of mathematics.  In one respect this
        last point is accurate.
    \end{quote}
    More recently, the development of algorithmic techniques, e.g. Gröbner
    bases, has spurred interest in the field of computational algebraic
    geometry.  A standard introduction to algebraic geometry from a
    computational perspective is \cite{CLO97}, which covers algebraic varieties
    and Gröbner bases algorithms while assuming a relatively small amount of
    prerequisite knowledge.  The same authors have a graduate text \cite{CLO05}.

    Here, we introduce some classical algebro-geometric objects.  There are many
    references in which these objects and their importance is explicated; one is
    \cite{Sha}.

\subsection{Affine and Projective Varieties}
    \label{sec:varieties}
    Throughout, let $\F$ be a field.  It is useful to think of $\F$ being either
    $\R$ or $\C$.

    Let $\A^n$ be the affine space of dimension $n$ over $\F$ (that is, the
    vector space $\F^n$ where we view neither the choice of basis nor the choice
    of origin as canonical).  A polynomial $f \in \F[x_1, \ldots, x_n]$ can be
    considered as a function on $\A^n$ by simply evaluating the polynomial on
    any given point.

    \begin{definition}
        An \emph{affine algebraic set} is a subset of the affine space $\A^n$
        of the form
        \[
            V(f_1, \ldots, f_m)
            = \cdil[\big]{x \in \A^n \vbar f_i(x) = 0 \quad\text{for all $i$}}
        \]
        for some finite set of polynomials $f_i \in \F[x_1, \ldots, x_n]$.

        An \emph{affine variety} is an irreducible affine algebraic set, i.e. an
        affine algebraic set that cannot be written as the union of two proper
        affine algebraic subsets.
    \end{definition}
    \noindent Instead of a finite set of polynomials, we can equivalently
    consider a finitely generated ideal.  In fact, $\F[x_1,\ldots,x_n]$ is a
    Noetherian ring, so the condition that the ideal be finitely generated
    condition is redundant.

    Given a subset $X \subset \A^n$, we may consider the ideal of polynomials
    vanishing on the subset
    \[
        I(X) = \cdil[\big]{f \in \F[x_1, \ldots, x_n] \vbar f(x) = 0 \quad\text{for
        all $x \in X$}}.
    \]
    Under certain conditions (e.g. if the field is algebraically closed and the
    ideal is \emph{radical}), the operations $V$ and $I$ are inverses of each other.

    \begin{definition}
        The \emph{projective space} of dimension $n$ (usually denoted $\Proj^n$)
        is the space $\F^{n+1}\setminus \{0\}$ under the equivalence relation $x
        \sim \lambda x$ for $x \in \F^{n+1}$ and $0 \ne \lambda \in \F$.  The
        usual affine coordinate system on $\F^{n+1}$ modulo scaling gives
        \emph{homogeneous coordinates} $[x_0: \cdots: x_n]$ on $\Proj^n$.  If
        $\F = \R$ (resp.  $\C$), the homogeneous coordinates give $\Proj^n$ the
        structure of a real (resp. complex) manifold.
    \end{definition}

    It also turns out that projective space behaves particularly nicely for the
    purposes of algebraic geometry, for reasons that we will not describe here.
    Instead of arbitrary polynomials, a different collection of `functions' is
    used on projective space.
    \begin{definition}
        A \emph{homogeneous polynomial} in $n+1$ variables of degree $d$ is a
        polynomial of the form $F(x) = \sum a_I x^I$ where $a_I \in \F$ and the
        multi-index ranges over $I = (i_0, \ldots, i_n)$ such that $i_0 + \cdots
        + i_n = d$.  The monomials are defined to be $x^I = x_0^{i_0}\cdots
        x_n^{i_n}$.  A \emph{homogeneous ideal} is an ideal of $\F[x_0, \ldots,
        x_n]$ generated by homogeneous polynomials.
    \end{definition}
    Notice that if $F$ is homogeneous polynomial of degree $d$, then $F(\lambda
    x) = \lambda^d F(x)$ for $\lambda \in \F$.  The zero set of $F$ in $\Proj^n$
    is then well-defined.  It follows that we can define the projective analogue
    of an affine variety.
    \begin{definition}
        A \emph{projective algebraic set} is a subset of the projective space
        $\Proj^n$ of the form
        \[
            V(f_1, \ldots, f_m)
            = \cdil[\big]{x \in \Proj^n \vbar f_i(x) = 0 \quad\text{for all $i$}}
        \]
        for some finite set of homogeneous polynomials $f_i \in \F[x_0,
        \ldots,x_n]$.  A \emph{projective variety} is an irreducible projective
        algebraic set.
    \end{definition}
    As in the affine case, there is a well-behaved correspondence between the
    homogeneous ideals of $\F[x_0, \ldots, x_n]$ and projective varieties.

    Using these varieties, we may define the \emph{Zariski topology} on $\A^n$
    and $\Proj^n$;  the affine (or projective) varieties are the closed sets of
    the topology.  The Zariski topology is a bit unusual as, unless the field is
    finite, no variety is ever a Hausdorff space.

    For the applications encountered in algebraic statistics, we will need to
    venture into real algebraic geometry, the primary objects of which are
    slightly less well-behaved.
    \begin{definition}
        A \emph{semialgebraic set} is any subset of $\R^n$ of the form
        \[
            V = \cdil[\big]{ x \in \R^n \vbar
                f_i(x) = 0, g_j(x) > 0
            \quad\text{for all $i,j$}}
        \]
        for some finite collection of polynomials $f_i, g_j \in \R[x_1, \ldots,
        x_n]$.
    \end{definition}

    A more in-depth investigation into algebraic geometry reveals the central
    importance of rings of functions on spaces, and the need to consider
    arbitrary commutative rings.  This road leads to the theory of schemes,
    which we will neither need nor pursue here.

\section{Machine Learning: Goals, Techniques, and Trends}
    \label{sec:ML}

    The goal of machine learning is to algorithmically use data in order to
    perform specified tasks better.  The emphasis of the field, tends to be
    toward large data sets, efficient algorithms, and `non-parametric' models
    with few assumptions.  Machine learning is studied both by computer
    scientists and statisticians.  The text \cite{EOSL} is an excellent overview
    of machine learning from the latter perspective.

\subsection{Statistical Classification}

    One common task is the classification problem.  Here, observations are
    points in some large, complicated, or high-dimensional space $X$, and the
    task is to assign a class label to an observation $x \in X$.  Class labels
    come from finite set $C$, and have some meaning.  Classification can also be
    viewed as an attempt to approximate a function $f : X \to C$ given a set of
    observations $\{(x_i, f(x_i)\}_{i=1}^n$ as training data.  For this task to
    be feasible, we cannot consider arbitrary functions; there must be a model.

    \begin{example} 
    The MNIST dataset of handwritten digits \cite{MNIST} is commonly seen in the
    machine learning literature.
    \begin{center}
    \scalebox{0.8}{
    \includegraphics{images/mnist_0.png}\,
    \includegraphics{images/mnist_1.png}\,
    \includegraphics{images/mnist_2.png}\,
    \includegraphics{images/mnist_3.png}\,
    \includegraphics{images/mnist_4.png}
    }
    \end{center}
    Each $28\times28$ pixel image depicts a handwritten digit $0, \ldots, 9$.
    The training set contains 60,000 images and the test set contains 10,000
    images.  An algorithm under evaluation is given the training set with
    correct labels; the task is to correctly label a high percentage of the
    remaining test set.
    \end{example}

    The majority of algorithms in use are in some sense `only a step or two away
    from linear'.  For instance the \emph{perceptron}, a binary linear
    classifier, places observations (which are real vectors) into two classes by
    learning a hyperplane which separates the classes.  The \emph{support vector
    machine} takes a modified approach, as it maps the observed data to a very
    high-dimensional space, and learns a separating hyperplane there.  Another
    common approach is to construct an approximating function as a weighted
    linear combination of a family of basis functions.

    For some problems, however, these techniques may not be sufficient.  Within
    the past ten years, there has been increased interest in so-called `deep
    learning' methods.  An overview of the motivating problems of deep learning
    is contained in \cite{Ben09}.  Perhaps the most influential technique put
    forward so far is the Deep Belief Network.


\subsection{The Deep Belief Network}
    A Deep Belief Network (DBN) is a generative model consisting essentially a
    stack of RBMs, trained greedily.  The paper \cite{Hin07} introduced a
    technique known as `contrastive divergence' that allowed RBMs, and in turn
    DBNs, to be trained efficiently on problems of practical interest.
    \begin{definition}
    A \emph{Deep Belief Network} is a model on multiple layers of hidden
    variables, built out of RBMs.  Specifically, let $h^k \in \{0,1\}^{n_k}$
    denote the binary state vector of the $k^{\text{th}}$ layer for $0 \le k \le
    m$.  The layer $h^0$ is the visible layer.  The joint distribution of the
    DBN is
    \begin{align*}
        P(v,h^1, \ldots, h^m) &= P(h^{m-1}, h^m) \prod_{k=0}^{m-2} P(h^k \mid h^{k+1})\\
        P(h^k\mid h^{k+1}) &\propto \exp\pdil*{(h^k)^Tb^k + (h^k)^T W^{k+1} h^{k+1}}\\
        P(h^{m-1}, h^{m}) &\propto  \exp\pdil*{(h^{m-1})^T b^k + (h^{m-1})^T W^m
        h^m + (h^m)^T b^m}
    \end{align*}
    for some collection of parameters $b^k$ and $W^k$.
    \end{definition}

    The conditional independence structure of the DBN is described by a graph
    with undirected connections between the top two layers and directed
    connections between all other adjacent layers.
    \begin{center}
    \scalebox{0.6}{\includegraphics{images/DBN3.png}}
    \end{center}
    For classification, the network is greedily trained to represent the input
    data.  The top layer $h^m$ is then trained to classify the inputs with a
    one-hot encoding.  The cited paper reported very low error rates (1.25\%) on
    the MNIST dataset using a Deep Belief Network.

% \nocite{*}
\bibliographystyle{annotate}
\bibliography{mid}

\end{document}
