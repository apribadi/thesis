\documentclass[11pt,titlepage]{article}
\usepackage{pset}

\newcommand*{\X}{\mathfrak{X}}
\newcommand*{\Mod}{\mathcal{M}}
\newcommand*{\Bin}{\mathcal{B}}
\newcommand*{\vbar}{\;\big\vert\;}
\newcommand*{\mle}{\theta_{\text{mle}}}
\DeclareMathOperator{\Mixt}{Mixt}
\DeclareMathOperator{\intr}{int}
\DeclareMathOperator{\Sec}{Sec}
\DeclareMathOperator*{\argmax}{arg\ max}
\newcommand*{\parop}[1]{\frac{\partial}{\partial #1}}
\usepackage{float}

\numberwithin{equation}{section}

\title{Midyear Report: The Restricted Boltzmann Machine}
\author{Aaron Pribadi}
\date{Fall 2011}

\begin{document}

\maketitle

\tableofcontents

\pagebreak

\section{What is Algebraic Statistics?}

    Algebraic statistics is a relatively new field that examines statistical
    questions using algebraic geometry and commutative algebra.  Once a problem
    has been cast in the language of algebra, a number of computational tools
    can be brought to bear.  For example, one of the early papers in the field
    analyzed contingency tables using Monte Carlo sampling computed with Gr√∂bner
    bases \cite{DS98}.

    Algebraic statistics also offers a geometric point of view on statistical
    models; the tendency is toward intrinsically defined objects in lieu of
    explicit coordinate systems.  In this light, algebraic statistics might be
    seen as in the tradition of information geometry, a field pioneered in the
    1980s that applied the techniques of Riemannian geometry to probability
    models (see for example \cite{Ama}).

    An introduction to the field of algebraic statistics may be found in the
    collection of lecture notes \cite{DSS08}.  Algebraic statistics has also
    been applied to computational biology \cite{ASCB}.

    Our goal is to examine a particular statistical model, the Restricted
    Boltzmann Machine, from the point of view of algebraic statistics building
    from prior work in that direction, especially \cite{CMS09}.  The Restricted
    Boltzmann Machine has recently become the centerpiece of certain
    developments in the machine learning community \cite{Hin07}, and basic
    questions about its geometry remain unanswered.  
    
    We begin in Section 2 by explaining the geometric framework of our approach.
    In Section 3 we present the Restricted Boltzmann Machine and some known
    results, and in Section 4 we develop our own characterization of the model
    in the hope that it will prove illuminating.

\section{The Geometry of Statistical Models}
    \subsection{The Probability Simplex}

    We consider probability distributions over finite sets.  The space of all
    such distributions corresponds to a geometric object.
    
    \begin{definition} The \emph{probability simplex} of dimension $N$ (also
    called the \emph{standard simplex}) is the space
    \[
        \Delta_N = 
        \left\{(x_1, \ldots, x_{N+1}) \vbar \sum_{i=1}^{N+1} x_i= 1, x_i \ge 0 \right\} 
        \subset
        \R^{N+1}.
    \]
    If the appropriate dimension is either clear in context or irrelevant we
    may simply write $\Delta$, omitting the subscript.
    \end{definition}
    A \emph{simplex} in general is the image of a standard simplex under any
    affine transformation.  Low-dimensional simplices are familiar objects;
    $\Delta_0$ is a point, $\Delta_1$ is a line segment, $\Delta_2$ is a
    triangle, and $\Delta_3$ is a tetrahedron.
    \begin{figure}[H]
        \centering
        \scalebox{1}{ \includegraphics[scale=0.5]{images/simplices.png} }
        \caption{Low-dimensional simplices.}
    \end{figure}
    \noindent A random variable $X$ with $N+1$ possible values corresponds to a point $x =
    (x_1, \ldots, x_{N+1})$ contained in the simplex $\Delta_N$ in a natural
    way; if $X$ takes values in $\{1, \ldots, N+1\}$, we set $P(X = i) = x_i$.
    We usually identify a probability distribution with its corresponding
    point in the probability simplex.  
    
    Statisticians are often concerned with families of probability
    distributions.  We identify such families with geometric spaces.
    \begin{definition}
    By the phrase \emph{statistical model}, we mean a subset $\Mod \subset
    \Delta$ of a probability simplex.
    \end{definition}
    \noindent A statistical model may be parametrized by a map $f: U \to
    \Delta$, for some space $U$ of parameters.  Usually, the space of parameters
    is a subset of a real affine space $\R^d$, and parametrizations are
    differentiable almost everywhere.  In the context of algebraic statistics,
    the parametrization is usually a rational function.
    \begin{example}
    A random variable $X$ following a binomial distribution with size $N$ and
    parameter $\lambda \in [0,1]$ takes a value $k \in \{0,\ldots, N\}$ with
    probability
    \[
        P_\lambda[X = k] = {N \choose k} \lambda^k(1-\lambda)^{n-k}.
    \]
    This is the number of heads produced by $n$ `coin tosses', where $\lambda$
    is the probability of a head.  The map $\lambda \mapsto P_\lambda$
    determines a parametrized statistical model $\{P_\lambda : \lambda \in [0,1]
    \} \subset \Delta_N$. The statistical model is a curve, i.e. a
    one-dimensional subspace, of the simplex.  

    Take $N=2$, which simulates two coin flips.  The three coordinates of a
    point in the model measure the probabilities that zero, one, and two heads
    will occur, respectively.
    \begin{figure}[H]\label{fig:binomial}
        \centering
        \vspace*{-0.2cm}
        \scalebox{1}{ \includegraphics[scale=0.7]{images/binomial.pdf} }
        \vspace*{-0.5cm}
        \caption{The binomial statistical model for $N=2$.}
    \end{figure}
    \noindent As the parameter $\lambda$ varies over $[0,1]$, the statistical
    model traces out the curve 
    \[
        \lambda \longmapsto \big((1-\lambda)^2, 2\lambda(1-\lambda), \lambda^2\big)
    \]
    in the simplex $\Delta_2$.  The final coordinate
    $P[X=2]$ does not need to be displayed, because we can compute it from the
    first two coordinates.  
    \end{example}
    
\subsection{Maximum Likelihood Estimation}

    Suppose that some data are taken from an unknown distribution, and that we
    want to model the data using some statistical model $\Mod \subset \Delta$.
    A distribution matches the data well if the data are relatively probable
    given the distribution.  Throughout this section we use discrete
    distributions, though our definitions can be made more general.

    Let $\Mod = \{P_\theta : \theta \in U\}$ be a parametrized statistical
    model. A random variable $X$ distributed according to some $P_\theta$ has
    the probability mass function $p_\theta(x) = P_\theta[X = x]$.  Suppose that
    we have data $Z = \{z_1, \ldots, z_n\}$.  The data are generally assumed to
    be independent and identically distributed according to some unknown true
    distribution in $\Mod$.
    
    \begin{definition}
    The \emph{likelihood function} of $\theta \in U$ given the data $Z$ is
    \[
        L(\theta; Z) = \prod_{i=1}^n p_\theta(z_i).
    \]
    It is the probability that the observed data $Z$ would occur if the
    observations were independent and identically distributed and following
    $P_\theta$.  The \emph{log-likelihood function} is
    \[
        l(\theta; Z) = \sum_{i=1}^n \log p_\theta(z_i)
    \]
    and is often used in place of the likelihood function because it is additive.
    \end{definition}
    \begin{definition}
    The \emph{maximum likelihood estimate} of the true parameter is the
    parameter 
    \[
        \mle = \argmax_{\theta \in U} L(\theta; Z)
    \]
    that maximizes the likelihood (or equivalently the log-likelihood) of the
    data.
    \end{definition}

    The maximum likelihood estimate is not always well-defined.  The estimate
    might not be unique, as the maximal likelihood could be attained multiple
    times.  The estimate  might not even exist, as the model could be a
    non-closed subset of the simplex.  In practice, it is possible that an
    optimal parameter is not necessary and that a merely good one is sufficient.
    Standard numerical optimization techniques, e.g. those based on gradient
    descent or Newton's method, can handle many models.  A more precise
    knowledge of the geometry of a statistical model, however, can make maximum
    likelihood estimation easier.

\subsection{Solutions to the Likelihood Equations}

    Here, we give a flavor of how algebro-geometric techniques can be applied to
    maximum likelihood estimation.  Our exposition follows Section 3.3 of
    \cite{ASCB} and Section 2.1 of \cite{DSS08}.

    Suppose that $g: U \to \Delta_{N-1}$ is a rational parametrization of a
    statistical model, where $U$ is an open subset of $\R^d$.  By rational, we
    mean that each component of $g(\theta) = (g_1(\theta), \ldots, g_N(\theta))$
    is a rational function with rational coefficients in its argument $\theta
    \in U \subset \R^d$.  Maximum likelihood estimation then amounts to
    maximizing the function
    \[
        l(\theta) = \sum_{i=1}^N u_i \log g_i(\theta)
    \] 
    where $u_i$ is the number of times event $i$ has occurred in the observed
    data.

    Every local and global maximum $\theta \in U$ is a solution to the
    likelihood equations
    \begin{equation}\label{eq:lik}
        \frac{\partial l}{\partial\theta_j}
        =
        \sum_{i=1}^N 
        \frac{u_i}{g_i} 
        \cdot
        \frac{\partial g_i}{\partial \theta_j}
        = 0
        \qquad
        \text{for $j = 1,\ldots,d$}.
    \end{equation}
    The likelihood equations in \eqref{eq:lik} are again rational functions of
    $\theta$.  Solving the equations involves `clearing denominators' and
    solving the polynomial equations
    \begin{equation}\label{eq:lik-clear}
        \sum_{i=1}^N 
        u_i \cdot g_1 \cdots \widehat{g_i} \cdots g_N
        \frac{\partial g_i}{\partial \theta_j}
        = 0
        \qquad
        \text{for $j = 1, \ldots, d$}
    \end{equation}
    where $\widehat{g_i}$ indicates that the $i$-th factor is omitted.  The
    equations \eqref{eq:lik-clear}, however, introduce extraneous solutions, for
    example when $g_a(\theta) = g_b(\theta) = 0$ for any $a \ne b$.

    Algebraic geometry offers a principled way to solve the likelihood
    equations.  (For a brief introduction to affine and projective varieties, see
    Section \ref{sec:varieties} of the appendix.)  Consider the ideal $I \subset \R[\theta_1,
    \ldots, \theta_d]$ generated by the polynomial expressions in \eqref{eq:lik-clear}
    \[
        I = \adil[\Bigg]{
            \sum_{i=1}^N u_i \cdot g_1 \cdots \widehat{g_i} \cdots g_N
            \frac{\partial g_i}{\partial \theta_j}
        }_{j=1}^d
        .
    \]
    Let $h$ be the product of all polynomials appearing in the denominators of
    the rational equations in \eqref{eq:lik}.  The \emph{saturation ideal} of
    $I$ with respect to $h$ is defined
    \[
        (I : h^\infty) = \cdil[\big]{
            f \in \R[\theta_1, \ldots, \theta_d]
            \;\big\vert\;
            fh^k \in I
            \quad
            \text{for some non-negative integer $k$}
        }.
    \]
    We can then consider the variety corresponding to the saturation ideal.  The
    points in $V(I : h^\infty) \cap U$ are all solutions to the likelihood
    equations.  In the case that there are only finitely many solutions, passing
    to the saturation ideal removes all extraneous solutions.

    The referenced works \cite{ASCB} and \cite{DSS08} include examples of
    computing the variety $V(I : h^\infty)$ with the software package
    \texttt{Singular}.  An overview of the techniques used to solve such
    polynomial equations, e.g. Gr√∂bner bases, resultants, and elimination, is
    given in the text \cite{CLO05} on computational algebraic geometry.
% 
% \subsection{Implicit Models}

    % In the context of algebraic statistics, we usually look at statistical
    % models that are semialgebraic sets.

    % If a statistical model is a semi algebraic set, then we can pass to its
    % Zariski closure in affine space, or we can embed it in projective space
    % and takes its Zariski closure there.

    % A simplex is the locus of a finite collection of polynomial equations
    % and polynomial inequalities and therefore is a semialgebraic set.

    % Then we can solve the likelihood equations, given the defining ideal?


\section{Introducing the Restricted Boltzmann Machine}

    We now focus on the Restricted Boltzmann Machine, a statistical model that
    has recently become important to the machine learning community in its the
    pursuit of so-called deep learning architectures.  In particular, this
    model is the key component of the Deep Belief Network which has achieved
    considerable success at a number of machine learning tasks \cite{Hin07}.  In
    our brief overview of machine learning in Appendix \ref{sec:ML}, we describe
    the motivation behind this application of the Restricted Boltzmann Machine.

\subsection{Markov Random Fields}
    \label{sec:rbm-def}

    The Restricted Boltzmann Machine is an instance of a \emph{graphical model}.
    A graphical model is a probabilistic model for which a graph (directed or
    undirected) represents the conditional independence structure between random
    variables.  Several types of graphical models have become popular for
    applications in machine learning; Chapter 17 of \cite{EOSL} is an overview
    of undirected graphical models and additionally contains references to the
    large body of literature.  From the algebro-geometric point of view,
    graphical models in general are discussed in Chapter 3 of \cite{DSS08} and
    directed graphical models, known as \emph{Bayesian networks}, are discussed
    in \cite{GSS}.  In our exposition we only introduce undirected models.

    \begin{definition}
        We describe a \emph{Markov random field}, i.e. an undirected graphical
        model.  Let $G$ be an undirected graph with vertex set $V$.  Let
        $\{X_\alpha\}_{\alpha \in V}$ be random variables indexed by the
        vertices.  The joint probability of $(X_\alpha)_{\alpha \in V}$ is said
        to factor according to $G$ if for any pair of vertices $\beta, \gamma
        \in V$ that are not adjacent, the random variables $X_\beta, X_\gamma$
        are conditionally independent given the variables at the other vertices.
        That is,
        \[
            \text{$X$ and $Y$ not adjacent}
            \Longleftrightarrow
            X_\beta \perp X_\gamma \mid 
            \{X_\alpha; \text{for } \alpha \ne \beta, \alpha \ne \gamma\}.
        \]
        A Markov random field is any such collection of random variables that
        factor according to an undirected graph.
    \end{definition}

    Roughly speaking, the edges in the graph record which variables influence
    which other variables.  If the graph is relatively sparse, then there are
    strong restrictions on the interactions between variables.  For example, a
    totally disconnected graph indicates that the variables are all independent.
    In contrast, a complete graph places no restrictions at all on the
    variables' joint distribution.
    
    With the assumption that distributions are strictly positive, there is an
    equivalent characterization of a graphical model.

    \begin{theorem}[Hammersley-Clifford]
        The random variables $(X_\alpha)_\alpha$ indexed by the vertices of an
        undirected graph $G$ factor according to $G$ if and only if their
        joint probability density factors as
        \[
            p(x) = \prod_{S \in C(G)} f_S(x_S)
        \]
        where the $S$ are maximal complete subgraphs (cliques) of $G$, $x_S$ is
        the restriction of $x$ to $S$, and $f_S$ is a function on the variables in
        $S$.
    \end{theorem}

    The graphical models that we consider have a particularly simple structure;
    the variables have binary values and all interactions are pairwise.
    Following the machine learning literature, we refer to binary-valued random
    variables and their corresponding vertices as `units'.

    \begin{definition}
        The \emph{Boltzmann Machine} (also called the \emph{Ising model}) with
        binary-valued units $X_1, \ldots, X_n$ has a joint probability density
        on $x \in \{0,1\}^n$ given by
        \[
            p(x)
            = \frac 1 Z \exp\pdil*{\sum_{(i,j) \in E} \theta_{i,j} x_i x_j +
            \sum_{k=1,\ldots,n} b_k x_k}
        \]
        for some collection of real-valued parameters $\theta_{ij}$ and $b_k$,
        where $E$ denotes the edge set of an undirected graph.  The normalizing
        constant
        \[
            Z = \sum_{x \in \{0,1\}^n} 
            \exp\pdil*{\sum_{(i,j) \in E} \theta_{i,j} x_i x_j +
                        \sum_{k=1,\ldots,n} b_k x_k}
        \]
        is called the \emph{partition function}.
    \end{definition}

    From the criterion given by the Hammersley-Clifford theorem, it follows that
    the Boltzmann Machine is in fact a graphical model.  The reader familiar
    with statistical mechanics may recognize the model as a Boltzmann
    distribution with energy $H(x) = -\log p(x)$.  That is, units and pairs of
    units contribute additively to the total energy $H$, and the probability of
    a state is proportional to $e^{-H}$.
    
    Now we arrive at the central object of our investigation, a Boltzmann
    Machine with a complete bipartite graph.

    \begin{definition}
    The \emph{Restricted Boltzmann Machine} (RBM) with $n$ visible units and $k$
    hidden units has binary states of the form $(v, h)$, where $v \in \{0,1\}^n$
    and $h \in \{0,1\}^k$ are binary vectors.  It has real parameters $w_{ij}$,
    $b_i$, and $c_j$, with indices ranging $1 \le i \le n$ and $1 \le j \le k$.
    Its unnormalized joint distribution is
    \begin{equation}\label{eq:rbm-unnorm}
        \psi(v, h) = \exp\pdil*{
            \sum_{ij} w_{ij} v_i h_j + \sum_i b_i v_i + \sum_j c_j h_j
        }
    \end{equation}
    and its actual joint distribution is 
    \[
        p(v, h) = \frac{\psi(v,h)}{Z}
    \] 
    where the partition function is $Z = \sum_{v,h} \psi(v, h)$.  The
    distribution over its visible units is
    \[
        p(v) = \sum_{h \in \{0,1\}^k} p(v,h)
    \]
    a \emph{marginalization} over the hidden states.
    \end{definition}
    Essentially, the visible units and hidden units influence each other, but
    there are no connections among the visible units or among the hidden units.
    \begin{figure}[H]
        \centering
        \scalebox{1.0}{ \includegraphics[scale=0.8]{images/rbm.png} }
        \caption{The graph of an RBM with four visible and three hidden units.}
    \end{figure}
    \noindent Notice also that for a fixed value of the hidden units, the
    visible units are all independent; removing the hidden units completely
    disconnects the graph.
    
\subsection{The Algebraic Geometry of the RBM}
    The recent paper \cite{CMS09} examines the Restricted Boltzmann Machine from
    an algebro-geometric perspective.  We outline a small portion of its
    approach.
    
    In the context of algebraic statistics, we want a rational parametrization
    of the model.  To that aim, we replace the parameters $w_{ij}$, $b_i$, and
    $c_j$ used in Section \ref{sec:rbm-def} with
    \[
        \omega_{ij} = \exp(w_{ij})
        \qquad
        \beta_i = \exp(b_i)
        \qquad
        \gamma_j = \exp(c_j)
    \]
    so that $\omega_{ij}$, $\beta_i$, and $\gamma_j$ range over the strictly
    positive real values.  That is, the parameter space is $\R_{>0}^{nk+n+k}$.
    Under this parametrization, the unnormalized probability distribution
    \eqref{eq:rbm-unnorm} becomes
    \[
        \psi(v, h) = 
            \pdil*{ \prod_{i=1}^k \prod_{j=1}^n \omega_{ij}^{h_i v_j} }
            \pdil*{ \prod_{i=1}^n \beta_i^{v_i} }
            \pdil*{ \prod_{j=1}^k \gamma_j^{h_j} }
    \]
    which is a polynomial in the variables $\omega_{ij}$, $\beta_i$, and
    $\gamma$ for any particular value of the binary vectors $(v,h)$.  The
    marginalized distribution over the visible variables factors as
    \begin{equation} \label{eq:rbm-alg-par}
        p(v) = \frac 1 Z
        \beta_1^{v_1} \beta_2^{v_2} \cdots \beta_n^{v_n} 
        \prod_{i=1}^k
        (1 + \gamma_i \omega_{i1}^{v_1} \omega_{i2}^{v_2} \cdots
        \omega_{in}^{v_n}).
    \end{equation}
    The partition function $Z = \sum_{v,h} \psi(v,h)$ is also a polynomial in
    the new parameters $\omega_{ij}$, $\beta_i$, and $\gamma_j$,  so the full
    parametrization $\R_{>0}^{nk+n+k} \to \Delta_{2^n-1}$ of the model is a
    rational map.  

    Let $M_n^k \subset \Delta_{2^n-1}$ denote the image of the parametrization,
    and let $V_n^k$ denote the Zariski closure of $M_n^k$ in the complex
    projective space $\Proj^{2^n-1}$.  The model $M_n^k$ is a semialgebraic set.
    The referenced paper gives a structural result about the model $M_n^k$ and
    the variety $V_n^k$.
    \begin{theorem}[\cite{CMS09}] \label{thm:factor}
    The RBM variety and model factor as the Hadamard powers
    \[
        V_n^k = (V_n^1)^{[k]}
        \qquad\text{and}\qquad
        M_n^k = (M_n^1)^{[k]}
    \]
    of the variety and model with one hidden unit.
    \end{theorem}
    \noindent The Hadamard product of two statistical models $M, N \subset
    \Delta_{N-1}$ (consisting of strictly positive distributions) is defined to
    be
    \[
        M * N = \cdil[\big]{p * q \mid p \in M, q \in N}
        \qquad
        \text{where}
        \qquad
        (p * q)_i = \frac{p_i\,q_i}{\sum_{j=1}^N p_j\,q_j}.
    \]
    The Hadamard product $p*q$ may be thought of as the normalized
    coordinate-wise product of the two distributions.  The Hadamard product $X *
    Y$ of two subvarieties $X$ and $Y$ of a projective space $\Proj^{N-1}$ is
    defined to be the closure under the Zariski topology of the image of the rational map
    \[
        X \times Y \dashrightarrow \Proj^m,
        \quad
        (x, y) \mapsto [x_1y_1 : x_2y_2\cdots : x_N y_N].
    \]
    Given a product operation, positive integer powers are, as usual,
    recursively defined $M^{[1]} = M$ and $M^{[k]} = M * M^{[k-1]}$, and
    similarly for $V$.  Taking Hadamard powers `commutes' with taking the
    closure under the Zariski topology.

    The variety $V_n^1$ is the first secant variety of the \emph{Segre
    embedding} of the product of $n$ projective lines $\Proj^1 \times \cdots
    \times \Proj^1$ into $\Proj^{2^n-1}$.  The Segre embedding of the Cartesian
    product of two subvarieties $X \subset \Proj^m$ and $Y \subset \Proj^n$ is
    the image of the map
    \[
        X \times Y \to \Proj^{(m+1)(n+1) - 1},
        \quad
        (x, y) \mapsto
        [x_0y_0: x_0 y_0 : \cdots : x_my_n]
    \]
    where the indices are in lexicographic order; the Segre embedding of the
    product of multiple varieties is defined analogously.
    
    The corresponding statistical model $M_n^1$ with one hidden value is a
    mixture of two independence models; both independence models and mixture
    models will be discussed in more detail in later sections.

\subsection{The Nested Sequence of RBM Models}

    Adding more hidden units should not decrease the representational power of
    the Restricted Boltzmann Machine.  That is, we have the bounded nested
    sequence of models $\{M_n^k\}_{k=0}^\infty$ where
    \[
        M_n^0 \subset M_n^1 \subset M_n^2 \subset \cdots 
        \qquad\text{and}\qquad
        M_n^k \subset \Delta_{2^n-1}
        \qquad\text{for all $k \in \Z^+$}.
    \]
    Indeed, any model in $M_n^k$ with parameters $w_{ij}$, $b_i$, and $c_j$ is
    contained in $M_n^{k+1}$ where the corresponding parameters are the same
    except for $w_{1,k+1} = \cdots = w_{n, k+1} = 0$  and $c_{k+1} = 0$.  While
    none of these constructions are new, we do not think that anybody has before
    drawn attention to the nested sequence itself as an object of interest.

    Some properties of this sequence are already known.
    \begin{theorem}[from \cite{MA10}, in our notation] \label{thm:approximator}
    With $k = 2^{n-1} - 1$, the model $M_n^k$ is dense in $\Delta_{2^n - 1}$.
    \end{theorem}
    In the cited paper, this result is phrased as ``Any distribution on
    $\{0,1\}^n$ can be approximated arbitrarily well by an RBM with $2^{n-1} -
    1$ hidden units''.  From the machine learning point of view, this is a first
    step towards quantifying how well the Restricted Boltzmann Machine can
    represent distributions.

    The Restricted Boltzmann Machine has $nk+n+k$ real parameters.  If none of
    the parameters are `redundant', then one would expect that both $M_n^k$ and
    $V_n^k$ have the same dimension as the parametrizing space.  (Here,
    dimension is defined as usual for varieties.) In most cases, this holds.
    \begin{theorem}[\cite{CMS09}] \label{thm:dimension}
    The Restricted Boltzmann Machine has the expected dimension
    $\min\{nk+n+k, 2^n-1\}$ when $k \le 2^{n-\lceil \log_2(n+1)\rceil}$ and
    when $k \ge 2^{n-\lfloor\log_2(n+1)\rfloor}$.
    \end{theorem}
    \noindent The RBM model $M_n^k$ is embedded in a simplex of very high
    dimension, i.e. $\dim \Delta = 2^n-1$.  For sufficiently small $k$, the
    model $M_n^k$ has the same dimension $nk+n+k$ as its parametrizing space.
    For sufficiently large $k$, the model $M_n^k$ reaches the same dimension
    $2^n-1$ as the ambient simplex, and cannot increase in dimension any
    further.

    Notice that in light of this dimensionality result, the bound given in
    Theorem \ref{thm:dimension} is not necessarily sharp.  For $k = 2^{n-1}-1$
    and $n \ge 2$, we have $nk+n+k > 2^n-1$, so there are lesser values of $k$
    for which the model $M_n^k$ has the requisite full dimension.

\subsection{Open Problems}

    A question naturally suggested by the universal approximation result in
    Theorem \ref{thm:approximator} and the dimensionality result in Theorem
    \ref{thm:dimension} is whether the bound given in the former is sharp.  That
    is, does there exist $k < 2^{n-1}-1$ such that $M_n^k$ is dense in
    $\Delta_{2^n-1}$?  The proof of the dimensionality result required passing
    from the model, a semialgebraic set, to the corresponding projective variety
    and tropical variety.  When dealing with a mixture model (as we will see is
    the case for the RBM), there is a loss in information in the passage from a
    semialgebraic set to a variety; Example 4.1.5 in \cite{DSS08} illustrates
    this phenomenon in a different model.

    Less concretely, it is desirable to get a better handle on the geometry of
    the Restricted Boltzmann Machine, keeping in view the model's use in
    practical applications.  A typical RBM used in a Deep Belief Network might
    have several hundred to several thousand visible and hidden units
    \cite{Hin07}.  Such an RBM would be trained with a data set containing tens
    of thousands of examples.  While algorithmic innovations in the machine
    learning community have made training RBMs practical, the run time of such
    algorithms is still measured in days, rather than seconds.
    
    Even though the rational parametrization of the RBM given in
    \eqref{eq:rbm-alg-par} places the task of likelihood maximization within an
    algebro-geometric framework, there are still severe computational
    difficulties with that approach.  In \cite{CMS09}, the authors turn to
    tropical techniques.  For a model with a mere four visible units and two
    hidden units, the task of computing the defining polynomial of the Zariski
    closure of the model (which has co-dimension 1) required a team to resort
    to creative techniques detailed in \cite{CTY10}.  

    Thus, any progress toward an algorithm leveraging algebro-geometric
    techniques to compute (approximate) maximum likelihood estimates for the RBM
    more efficiently  would be of extreme interest.


\section{The Structure of the Restricted Boltzmann Machine}

    We examine the Restricted Boltzmann Machine as a mixture model of
    independence distributions.  With the exception of subsection 4.3, the
    material in this section is original.

    We saw in Section \ref{sec:rbm-def} that given a fixed value $h \in
    \{0,1\}^k$ for the hidden units of a RBM, the visible units are all
    independent in the conditional distribution $P( \cdot\mid h )$.  Breaking
    the conditional probability out, probability of a state $v \in \{0,1\}^n$
    for the visible units is
    \[
        P( v ) = \sum_{h \in \{0,1\}^k} P( h ) P( v \mid h ).
    \]
    There are $2^n$ different hidden states $h$, so the visible distribution is
    a weighted sum of $2^n$ distributions, such that in each component
    distribution the visible units are all independent.  The weights $P(h)$ and
    the independence distributions $P(\cdot \mid h)$ are, however, not
    arbitrary.  The range of possibilities for both determine the structure of
    the RBM.

    In this section, we restrict ourselves to the space of strictly positive
    distributions, denoted by $\intr(\Delta_{N-1})$.  This space contains
    distributions for which the probability of every event is nonzero, and
    consists of the interior of the simplex.

\subsection{The Hadamard Product}

    In the RBM, each hidden unit influences the visible units.  The effects of
    the hidden units are combined through the Hadamard product.  (cf. Theorem
    \ref{thm:factor}).  Recall that for $p, q \in \intr(\Delta)_{N-1}$ the
    Hadamard product is defined
    \[
        (p * q)_i = \frac{p_i q_i}{\sum_{j=1}^N p_j q_j}.
    \]
    This is essentially a normalized coordinate-wise product.  The Hadamard
    product yields a straightforward algebraic structure.

    \begin{theorem}
    The space $(\intr(\Delta_{N-1}), *)$ of strictly positive distributions
    equipped with the Hadamard product is an Abelian group.
    \end{theorem}
    \begin{proof}
    By the construction of the Hadamard product, $p * q \in
    \intr(\Delta_{N-1})$, and it is obvious that the product is commutative.
    The uniform distribution
    \[
        u = \pdil*{\frac 1 N, \ldots, \frac 1 N}
    \]
    is the identity element.  The inverse of a distribution $p \in
    \intr(\Delta_{N-1})$ is 
    \[
        \inv p = \frac{1}{\sum_{j=1}^N \frac 1 {p_j}}
        \cdot\pdil*{\frac 1 {p_1}, \ldots, \frac 1 {p_N}}
    \]
    from which we see that the stipulation that distributions be strictly
    positive is necessary.
    \end{proof}

    Through a particularly nice parametrization of $\intr(\Delta_{N-1})$, we see
    that the group has a familiar structure.
    \begin{theorem} \label{thm:dist-par}
    There is an isomorphism of (Lie) groups $(\intr(\Delta_{N-1}), *) \iso
    (\R^{N-1}, +)$, where the latter group is the real vector space with the
    usual addition operation.
    \end{theorem}
    \begin{proof}
    Consider the vector subspace
    \[
        S = \pdil*{\cdil*{x \in \R^N \;\Big\vert\; \sum_{j=1}^N x_i = 0 }, +}
    \]
    as a group with the usual addition operation.  Consider the parametrization
    \[
        \varphi : S \to \intr(\Delta_{N-1})
        \qquad
        x \mapsto \frac{1}{\sum_{j=1}^N e^{x_j}} 
        \pdil*{e^{x_1}, \ldots, e^{x_N}}.
    \]
    We claim that $\varphi$ is a group isomorphism.  It is clear that $\varphi$
    is surjective.  To see that it is injective, notice that if $\varphi(x) =
    \varphi(y)$, then $(e^{x_1}, \ldots, e^{x_N}) = (\lambda e^{y_1}, \ldots,
    \lambda e^{y_N})$ for some scalar $\lambda$.  The condition $\sum x_j = \sum
    y_j = 0$, however, implies that $\prod e^{x_j} = \prod e^{y_j} = 1$, so
    $\lambda^N = 1$ and $\lambda = 1$.  Thus $\varphi$ is a bijection.

    The map $\varphi$ is also a group homomorphism.  In the computation below,
    note that both the map $\varphi$ and the Hadamard product $*$ normalize
    their values to sum to 1 to fall within $\intr(\Delta_{N-1})$, so we can
    essentially ignore a scalar (`projective') factor.  We have
    \[
        \varphi(x + y)
        \propto
        \pdil*{e^{x_1 + y_1}, \ldots, e^{x_N + y_N}}
        =
        \pdil*{e^{x_1}e^{y_1}, \ldots, e^{x_N}e^{y_N}}
        \propto
        \varphi(x) * \varphi(y)
    \]
    and $\varphi(x + y) \propto \varphi(x) * \varphi(y)$ implies that $\varphi(x
    + y) = \varphi(x) + \varphi(y)$ when both lie in $\intr(\Delta_{N-1})$.
    \end{proof}
    

\subsection{The Binary Independence Model}

    We formally introduce the binary independence model.  Throughout this
    section, let $N = 2^n$.
    \begin{definition}
    Identify the space of strictly positive distributions over the $N$ states
    $\{0,1\}^n$ with the probability simplex $\intr(\Delta_{N-1})$.  The
    coordinates of $\Delta_{N-1}$ may be associated with the binary vectors of
    length $n$ ordered lexicographically.  The \emph{binary independence model}
    $\Bin \subset \Delta_N$ consists of the distributions that factor as
    \[
        P(v) = P(v_1) \cdots P(v_n)
    \]
    for binary vectors $v = (v_1, \ldots, v_n) \in \{0,1\}^n$.
    \end{definition}

    Notice that the binary independence model is parametrized by $(\lambda_1,
    \ldots, \lambda_n) \in (0,1)^n$ as
    \begin{equation} \label{eq:bin}
        P(v) 
        = \lambda_1^{v_1}(1 - \lambda_1)^{1 - v_1}
        \cdots \lambda_n^{v_n}(1 - \lambda_n)^{1 - v_n}.
    \end{equation}
    In fact, it is clear that this parametrization $(0,1)^n \to \Bin$ is a
    bijection.

    The binary independence model interacts well with the Hadamard product.
    \begin{theorem}\label{thm:bin-par}
    The binary independence model $\Bin$ with the Hadamard product is a subgroup
    of the space $(\intr(\Delta_{N-1}), *)$ of all strictly positive
    distributions.  Furthermore, $(\Bin, *)$ is  isomorphic to $(\R^n, +)$.
    \end{theorem}
    \begin{proof}
    For $1 \le k \le N$, let $v(k)$ be the $k$th binary vector of length $n$,
    ordered lexicographically.  Let $S$ and $\varphi$ be as in the proof of
    Theorem \ref{thm:dist-par}.  Recall that $S$ is a hyperplane in $\R^N$;
    associate the $k$th coordinate of $S \subset \R^N$ with the probability that
    the event $v(k)$ occurs.  
    
    We construct a parametrization of $\Bin$ via the previous parametrization
    $\varphi : S \to \intr(\Delta_{N-1})$. Consider the linear transformation 
    \[
        \rho: \R^n \to \R^N
        \qquad\text{given by the matrix}\qquad
        \begin{bmatrix*}[r]
            -1 & \cdots & -1 & -1 & -1 \\
            -1 & \cdots & -1 & -1 &  1 \\
            -1 & \cdots & -1 &  1 & -1 \\
            \vdots & \ddots &  & \vdots &  \\
             1 & \cdots &  1 &  1 &  1
        \end{bmatrix*}
    \]
    where the rows are the $N$ possible rows of length $n$ containing only $-1$
    and $1$, ordered lexicographically.  Several things are straightforward to
    verify.  First, $\rho(\R^n) \subset S$, as the columns of the matrix all sum
    to 0.  More importantly, $(\varphi \circ \rho)(\R^n) = \Bin$, and $\varphi
    \circ \rho$ is a bijection onto its image $\Bin$.  One can compute that a
    distribution in the binary independence model given by parameters
    $\lambda_1, \ldots, \lambda_n$ as in \eqref{eq:bin} has the unique preimage
    \[
        x \in \R^n
        \qquad
        x_i = \frac 1 2 \log \pdil*{\frac{\lambda_i}{1 - \lambda_i}}
    \]
    under $\varphi \circ \rho$.  
    
    This amounts to a re-parametrization of $\Bin$, with the advantage that
    $\rho$ is a linear map.  The matrix has full rank $n$, so $\rho$ is an
    isomorphism.  The subspace $\rho(\R^n) \subset S$ is then a subgroup of $S$
    isomorphic to both $\R^n$ and $\Bin$.
    \end{proof}

\subsection{Hidden Variables and Mixture Models}
    This section follows Chapter 4 of \cite{DSS08}.

    Suppose that a hidden variable $Y$ influences a visible variable $X$.  As
    usual, we assume that probability distributions are discrete.  If $Y$
    follows the probability distribution $\pi$, then the joint distribution of
    $X$ and $Y$ is 
    \[
        P(X = i, Y = j) = \pi_j \cdot p_i^{(j)}
    \]
    for some distributions $p^{(j)}$.  Because we consider $Y$ as the hidden
    variable, we can only observe the marginal distribution of $X$.  The
    marginal probability is the sum over the possible hidden values
    \[
        P(X = i) = \sum_{j} \pi_j \cdot p_i^{(j)}.
    \]
    A hidden variable model allows for the creation of complex model out of
    simple components $p^{(j)}$.  In particular, if the distributions $p^{(j)}$
    are required to lie in some model $\Mod \subset \Delta_{N-1}$, then we have
    a mixture model.
    \begin{definition}
    Suppose that $V_1, \ldots, V_m$ are subsets of the vector space $\R^N$.  The
    \emph{mixture} of those sets is 
    \[
        \Mixt(V_1, \ldots, V_n) = \cdil*{
            \lambda_1 v_1 + \cdots + \lambda_n v_m
            \;\big\vert\;
            v_i \in V_i, \lambda_j \ge 0, \sum_{j=1}^m \lambda_j = 1
        }.
    \]
    A \emph{mixture model} is the mixture of statistical models $\Mod_1, \ldots,
    \Mod_m \subset \Delta_{N-1}$.  
    \end{definition}
    Note that because $\Delta_{N-1}$ is a convex set, the mixture of statistical
    models is itself a statistical model.  The mixture of $m$ models contains
    the distributions that can be constructed with a hidden variable with $m$
    states, where each state induces a visible distribution taken from the
    corresponding model.

    \begin{example}
    A \emph{Gaussian mixture model} simulates a multi-modal distribution.  It is
    relatively easy to work with and is useful in a number of machine learning
    tasks.  See, for example, Sections 6.8 and 8.5 of \cite{EOSL} for an
    overview of Gaussian mixtures and the EM algorithm, commonly used to train
    the model.  A Gaussian distribution on the real line with parameters $(\mu,
    \sigma)$ has probability density
    \[
        p_{\mu, \sigma}(x) = \frac{1}{\sqrt{2\pi\sigma^2}} 
        \exp\pdil*{-\frac{(x - \mu)^2}{2\sigma^2}}.
    \]
    A mixture of two Gaussian distributions is a weighted sum $p(x) = \lambda
    p_{\mu_1, \sigma_1}(x) + (1 - \lambda)p_{\mu_2, \sigma_2}(x)$.  
    \begin{figure}[H]
        \centering
        \scalebox{1}{ \includegraphics[scale=0.5]{images/mixture.png} }
        \caption{A mixture of two Gaussian distributions, with components shown
        dashed.}
    \end{figure}
    Mixtures of more Gaussian distributions in higher dimensions are defined
    analogously.  Note that because the space of distributions over the real
    line is not finite dimensional, this example does not formally fit under our
    framework; the relevant constructions, however, generalize easily.
    \end{example}

    Mixture models correspond to a relatively well-studied object in algebraic
    geometry.  (For definitions of terms, see Appendix \ref{sec:varieties}.)
    \begin{definition}
    The \emph{secant variety} of the affine variety $V \subset \F^n$ is
    \[
        \Sec(V) = \overline{\cdil[\big]{
        \lambda u + (1 - \lambda) v \vbar
        u,v \in V
        \text{ and }
        \lambda \in \F
        }}
    \]
    where the line indicates the closure of the set under the Zariski topology.  
    \end{definition}
    \begin{proposition}[\cite{DSS08}]
    If $M$ is a semialgebraic set, then the secant variety
    $\Sec\pdil[\big]{\overline{M}}$ is the Zariski closure of the mixture
    $\Mixt(M, M)$.
    \end{proposition}

    For a more thorough exposition on mixture models and secant varieties, we
    refer the reader to Chapter 4 of \cite{DSS08}.

\subsection{The RBM as a Mixture Model}

    We claim that a distribution in the RBM model $M_n^K$ is the weighted sum of
    $2^k$ distributions from the binary independence model, where both the
    weights and the component distributions are subject to certain restrictions.
    From the characterization of the RBM model given in Theorem
    \ref{thm:factor}, we can make this precise.
    \begin{proposition}\label{prop:rbm-mix}
    A distribution is in the RBM model $M_n^k$ if and only if it is of the form
    \[
        \sum_{h \in \{0,1\}^k} \mu_h v_h
        \qquad
        \mu_h \in \R, \;
        v_h \in \Bin
    \]
    where for all binary vectors $h = (h_1, \ldots, h_k)$, the weight $\mu_h$
    and distribution $v_h$ are as follows.  There are $\lambda_1, \ldots,
    \lambda_k \in (0,1)$ such that
    \[
        \mu_h = \lambda_1^{h_1}(1 - \lambda_1)^{1-h_1} \cdots
        \lambda_k^{h_k}(1-\lambda_k)^{1-h_k}.
    \]
    There are distributions $t \in \Bin$ and $u_1, \ldots, u_k \in \Bin$ such
    that
    \[
        v_h = t * u_{i_1} * \cdots * u_{i_m}
    \]
    where $u_{i_j}$ is included in the product only when $h_{i_j} = 1$.
    \end{proposition}
    \begin{proof}
        Write an arbitrary distribution in $M_n^k$ as the Hadamard product of
        $k$ mixture models, each a weighted sum of two independence models.
        Multiplying out yields the stated form.
    \end{proof}

    We make the following observation.
    \begin{proposition}
        For any distribution $p \in M_n^k$ where $k \ge 1$, there exists a
        distribution $q \in M_n^{k-1}$ such that
        \[
            p = \lambda q + (1 - \lambda)(q * u)
        \]
        for some weighting scalar $\lambda \in (0,1)$ and independence
        distribution $u \in \Bin$.
    \end{proposition}
    \begin{proof}
        The observation follows immediately from Proposition \ref{prop:rbm-mix}.
    \end{proof}
    Using the above observation inductively, we see that we can then reach any
    distribution in $M_n^k$ in a series of $k$ steps, where we begin from a
    distribution $t \in M_n^0 = \Bin$.  A step is determined by a weighting
    parameter $\lambda \in (0,1)$ and an independence distribution $u \in \Bin$
    and proceeds
    \[
        \text{from}\qquad q \in M_n^{k-1}
        \qquad\text{to}\qquad
        \lambda q + (1 - \lambda)(q * u) \in M_n^k.
    \]
    A step is of course equivalent to adding another hidden unit with the
    appropriately weighted connections.

\subsection{Bounding the Growth of Nested RBM Models}

    This section outlines potential avenues for further progress.

    We would like to characterize how the nested models $\Bin = M_n^0 \subset
    M_n^1 \subset M_n^2 \subset \cdots$ grow.  One possible coarse description
    would use some notion of distance in the space of distributions.
    Specifically, a metric $d(\cdot, \cdot)$ on $\Delta$ would be useful
    if for $q \in M_n^{k-1}$, $\lambda \in (0,1)$, and $u \in \Bin$ the distance
    traversed in a single step
    \begin{equation}\label{eq:step-distance}
        d(q,\, \lambda q + (1 - \lambda)(q * u))
    \end{equation}
    is relatively small.  Ideally, the `radius' of the space, i.e. the distance
    from the subset $M_n^0$ to a point in $\Delta$ as far away from the subset
    as possible, would be large relative to the distance traversed in a single
    step.  A `uniform' bound for \eqref{eq:step-distance} that does not depend
    on $q$, $\lambda$, or $u$ would imply that the maximal distance from a point
    in $M_n^k$ to the smaller model $M_n^{k-1}$
    \begin{equation}\label{eq:expansion}
        \sup_{p \in M_n^k}
        \inf_{q \in M_n^{k-1}} d(p, q)
    \end{equation}
    is bounded.  The value \eqref{eq:expansion} measures how much a model can
    `expand' in the simplex given an additional hidden unit. 
    
    There are a number of commonly used
    measures of distance between probability distributions, including the
    Kullback-Leibler divergence, the total variation distance, and the usual
    Euclidean distance on the simplex.  It is unknown whether any of these
    distances will be useful or could be modified to be useful in this
    situation.

    It may be useful to consider the space of distributions modulo the binary
    independence model with respect to the Hadamard product.  Recall that, with
    the Hadamard product, the space of distributions is an Abelian group and the
    binary independence model is a subgroup, so the quotient is well-defined.
    The following points are suggestive.
    \begin{itemize*}
        \item Every model $M_n^k$ is invariant under the action of $\Bin$, so
        it projects cleanly to the quotient.
        \item The projection of $M_n^0$ is the identity.
        \item The `path' to a distribution in $M_n^k$ is unique (disregarding
        reordering of steps) when we work in the quotient; this is not the case
        in the original space.
    \end{itemize*}
    The difficulty is in characterizing mixture models in the quotient space; this
    is a current goal.
    
    % in terms of the BIM and mixtures
    % remember the affine image of a hypercube!

    % Let $\rho$ and $\varphi$ be as in the proof to Theorem \ref{thm:bin-par}.
    % We can consider the binary vector $h \in \{0,1\}^k$ as in the space $\R^k$;
    % the set of all $h$ consists of the corners of the standard $k$-dimensional
    % hypercube.  Then for a given distribution in the RBM model, there exists
    % some fixed affine map $\tau: \R^k \to \R^n$ (i.e. linear map with
    % translation) such that $v_h = (\varphi \circ \rho \circ \tau)(h)$ for all
    % $h$.

\appendix

\section{A Little Algebraic Geometry}

    The field of algebraic geometry grew out of the study of curves and surfaces
    determined by polynomials.  In the mid-twentieth century, the foundations of
    the subject were reformulated by Serre, Grothendieck, and others.  The
    techniques of the field are notoriously both abstract and powerful; the
    following quote by the algebraic geometer David Mumford is perhaps telling
    \cite{Mum99}.
    \begin{quote}
        Algebraic geometry seems to have acquired the reputation of being
        esoteric, exclusive, and very abstract, with adherents who are secretly
        plotting to take over all the rest of mathematics.  In one respect this
        last point is accurate.
    \end{quote}
    More recently, the development of algorithmic techniques, e.g. Gr√∂bner
    bases, has spurred interest in the field of computational algebraic
    geometry.  A standard introduction to algebraic geometry from a
    computational perspective is \cite{CLO97}, which covers algebraic varieties
    and Gr√∂bner bases algorithms while assuming a relatively small amount of
    prerequisite knowledge.  The same authors have a graduate text \cite{CLO05}.

    Here, we introduce some classical algebro-geometric objects.  There are
    many good introductory textbooks for algebraic geometry, including
    \cite{Invitation}, \cite{Sha94}, \cite{Hart}, and \cite{Mum99}.

\subsection{Affine and Projective Varieties}
    \label{sec:varieties}
    Throughout, let $\F$ be a field.  It is useful to think of $\F$ being either
    $\R$ or $\C$.

    Let $\A^n$ be the affine space of dimension $n$ over $\F$ (that is, the
    vector space $\F^n$ where we view neither the choice of basis nor the choice
    of origin as canonical).  A polynomial $f \in \F[x_1, \ldots, x_n]$ can be
    considered as a function on $\A^n$ by simply evaluating the polynomial on
    any given point.

    \begin{definition}
        An \emph{affine algebraic set} is a subset of the affine space $\A^n$
        of the form
        \[
            V(f_1, \ldots, f_m)
            = \cdil[\big]{x \in \A^n \vbar f_i(x) = 0 \quad\text{for all $i$}}
        \]
        for some finite set of polynomials $f_i \in \F[x_1, \ldots, x_n]$.

        An \emph{affine variety} is an irreducible affine algebraic set, i.e. an
        affine algebraic set that cannot be written as the union of two proper
        affine algebraic subsets.
    \end{definition}
    \noindent Instead of a finite set of polynomials, we can equivalently
    consider a finitely generated ideal.  In fact, $\F[x_1,\ldots,x_n]$ is a
    Noetherian ring, so the condition that the ideal be finitely generated
    condition is redundant.

    Given a subset $X \subset \A^n$, we may consider the ideal of polynomials
    vanishing on the subset
    \[
        I(X) = \cdil[\big]{f \in \F[x_1, \ldots, x_n] \vbar f(x) = 0 \quad\text{for
        all $x \in X$}}.
    \]
    Under certain conditions (e.g. if the field is algebraically closed and the
    ideal is \emph{radical}), the operations $V$ and $I$ are inverses of each other.

    \begin{definition}
        The \emph{projective space} of dimension $n$ (usually denoted $\Proj^n$)
        is the space $\F^{n+1}\setminus \{0\}$ under the equivalence relation $x
        \sim \lambda x$ for $x \in \F^{n+1}$ and $0 \ne \lambda \in \F$.  The
        usual affine coordinate system on $\F^{n+1}$ modulo scaling gives
        \emph{homogeneous coordinates} $[x_0: \cdots: x_n]$ on $\Proj^n$.  If
        $\F = \R$ (resp.  $\C$), the homogeneous coordinates give $\Proj^n$ the
        structure of a real (resp.\ complex) manifold.
    \end{definition}

    It also turns out that projective space behaves particularly nicely for the
    purposes of algebraic geometry, for reasons that we will not describe here.
    Instead of arbitrary polynomials, a different collection of `functions' is
    used on projective space.
    \begin{definition}
        A \emph{homogeneous polynomial} in $n+1$ variables of degree $d$ is a
        polynomial of the form $F(x) = \sum a_I x^I$ where $a_I \in \F$ and the
        multi-index ranges over $I = (i_0, \ldots, i_n)$ such that $i_0 + \cdots
        + i_n = d$.  The monomials are defined to be $x^I = x_0^{i_0}\cdots
        x_n^{i_n}$.  A \emph{homogeneous ideal} is an ideal of $\F[x_0, \ldots,
        x_n]$ generated by homogeneous polynomials.
    \end{definition}
    Notice that if $F$ is homogeneous polynomial of degree $d$, then $F(\lambda
    x) = \lambda^d F(x)$ for $\lambda \in \F$.  The zero set of $F$ in $\Proj^n$
    is then well-defined.  It follows that we can define the projective analogue
    of an affine variety.
    \begin{definition}
        A \emph{projective algebraic set} is a subset of the projective space
        $\Proj^n$ of the form
        \[
            V(f_1, \ldots, f_m)
            = \cdil[\big]{x \in \Proj^n \vbar f_i(x) = 0 \quad\text{for all $i$}}
        \]
        for some finite set of homogeneous polynomials $f_i \in \F[x_0,
        \ldots,x_n]$.  A \emph{projective variety} is an irreducible projective
        algebraic set.
    \end{definition}
    As in the affine case, there is a well-behaved correspondence between the
    homogeneous ideals of $\F[x_0, \ldots, x_n]$ and projective varieties.

    Using these varieties, we may define the \emph{Zariski topology} on $\A^n$
    and $\Proj^n$;  the affine (or projective) varieties are the closed sets of
    the topology.  The Zariski topology is a bit unusual as, unless the field is
    finite, no variety is ever a Hausdorff space.

    For the applications encountered in algebraic statistics, we will need to
    venture into real algebraic geometry, the primary objects of which are
    slightly less well-behaved.
    \begin{definition}
        A \emph{semialgebraic set} is any subset of $\R^n$ of the form
        \[
            V = \cdil[\big]{ x \in \R^n \vbar
                f_i(x) = 0, g_j(x) > 0
            \quad\text{for all $i,j$}}
        \]
        for some finite collection of polynomials $f_i, g_j \in \R[x_1, \ldots,
        x_n]$.
    \end{definition}

    A more in-depth investigation into algebraic geometry reveals the central
    importance of rings of functions on spaces, and the need to consider
    arbitrary commutative rings.  This road leads to the theory of schemes,
    which we will neither need nor pursue here.

\section{Machine Learning: Tasks and Tools}
    \label{sec:ML}

    The goal of machine learning is to algorithmically use data in order to
    perform specified tasks better.  The emphasis of the field tends to be
    toward large data sets, efficient algorithms, and `non-parametric' models
    with few assumptions.  Machine learning is studied both by computer
    scientists and statisticians.  The text \cite{EOSL} is an excellent overview
    of machine learning from the latter perspective.

\subsection{Statistical Classification}

    One common task is the classification problem.  Here, observations are
    points in some large, complicated, or high-dimensional space $X$, and the
    task is to assign a class label to an observation $x \in X$.  Class labels
    come from finite set $C$, and have some meaning.  Classification can also be
    viewed as an attempt to approximate a function $f : X \to C$ given a set of
    observations $\{(x_i, f(x_i)\}_{i=1}^n$ as training data.  For this task to
    be feasible, we cannot consider arbitrary functions; there must be a model.

    \begin{example} 
    The MNIST data set of handwritten digits \cite{MNIST} is commonly seen in the
    machine learning literature.
    \begin{figure}[H]
        \centering
    \scalebox{0.8}{
    \includegraphics{images/mnist_0.png}\,
    \includegraphics{images/mnist_1.png}\,
    \includegraphics{images/mnist_2.png}\,
    \includegraphics{images/mnist_3.png}\,
    \includegraphics{images/mnist_4.png}
    }
        \caption{Some digits from the MNIST data set.}
    \end{figure}
    \noindent Each $28\times28$ pixel image depicts a handwritten digit $0,
    \ldots, 9$.  The training set contains 60,000 images and the test set
    contains 10,000 images.  An algorithm under evaluation is given the training
    set with correct labels; the task is to correctly label a high percentage of
    the remaining test set.
    \end{example}

    The majority of algorithms in use are in some sense `only a step or two away
    from linear'.  For instance the \emph{perceptron}, a binary linear
    classifier, places observations (which are real vectors) into two classes by
    learning a hyperplane which separates the classes.  The \emph{support vector
    machine} takes a modified approach, as it maps the observed data to a very
    high-dimensional space, and learns a separating hyperplane there.  Another
    common approach is to construct an approximating function as a weighted
    linear combination of a family of basis functions.

    For some problems, however, these techniques may not be sufficient.  Within
    the past ten years, there has been increased interest in so-called `deep
    learning' methods.  An overview of the motivating problems of deep learning
    is contained in \cite{Ben09}.  Perhaps the most influential technique put
    forward so far is the Deep Belief Network.


\subsection{The Deep Belief Network}
    A Deep Belief Network (DBN) is a generative model consisting essentially a
    stack of RBMs, trained greedily.  The paper \cite{Hin07} introduced a
    technique known as `contrastive divergence' that allowed RBMs, and in turn
    DBNs, to be trained efficiently on problems of practical interest.
    \begin{definition}
    A \emph{Deep Belief Network} is a model on multiple layers of hidden
    variables, built out of RBMs.  Specifically, let $h^k \in \{0,1\}^{n_k}$
    denote the binary state vector of the $k^{\text{th}}$ layer for $0 \le k \le
    m$.  The layer $h^0$ is the visible layer.  The joint distribution of the
    DBN is
    \begin{align*}
        P(v,h^1, \ldots, h^m) &= P(h^{m-1}, h^m) \prod_{k=0}^{m-2} P(h^k \mid h^{k+1})\\
        P(h^k\mid h^{k+1}) &\propto \exp\pdil*{(h^k)^Tb^k + (h^k)^T W^{k+1} h^{k+1}}\\
        P(h^{m-1}, h^{m}) &\propto  \exp\pdil*{(h^{m-1})^T b^k + (h^{m-1})^T W^m
        h^m + (h^m)^T b^m}
    \end{align*}
    for some collection of parameters $b^k$ and $W^k$.
    \end{definition}

    The conditional independence structure of the DBN is described by a graph
    with undirected connections between the top two layers and directed
    connections between all other adjacent layers.
    \begin{figure}[H]
        \centering
        \scalebox{0.6}{\includegraphics{images/DBN3.png}}
        \caption{Layers of a Deep Belief Network.}
    \end{figure}
    \noindent For classification, the network is greedily trained to represent
    the input data.  The top layer $h^m$ is then trained to classify the inputs
    with a one-hot encoding.  The cited paper reported very low error rates
    (1.25\%) on the MNIST data set using a Deep Belief Network.

% \nocite{*}
\bibliographystyle{annotate}
\bibliography{mid}

\end{document}
