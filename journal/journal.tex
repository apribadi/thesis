\documentclass[11pt]{article}
\usepackage{pset}

\newcommand*{\X}{\mathfrak{X}}
\newcommand*{\Mod}{\mathcal{M}}
\usepackage[normalem]{ulem}

\title{Thesis Journal}
\author{Aaron Pribadi}
\date{}

\begin{document}

\maketitle

\section{September 9, 2011}

\subsection{Introduction}

I would like to work on a problem in algebraic statistics, and more specifically
a problem about graphical models.  I do not yet have a coherent `story' for the
background of a problem, so I will instead discuss some topics in a close
neighborhood of what I want to work on.


\subsection{Graphical models}

Independence conditions arise naturally in many probabilistic models.  A
graphical model is a probabilistic model for which a graph (directed or
undirected) represents the conditional independence structure between random
variables. 

Let $G$ be a graph with vertex set $V$.  Suppose that $(X_\alpha)_{\alpha \in
V}$ are random variables taking values in probability spaces
$(\X_\alpha)_{\alpha \in V}$.  Usually the $X_\alpha$ are either discrete or
real-valued.  The joint probability of $X = (X_\alpha)$ is said to
factor with respect to $G$ if the probability density is
\[
    P[X_1, \ldots, X_n] = 
        \prod_\alpha P[X_\alpha \mid X_\beta, \beta \text{ is a parent of }
        \alpha]
\]
that is, the dependence of a random variable on the other variables is
completely captured by its dependence on its immediate parents (i.e. on the
variables having directed edges into it).  We can also formulate an equivalent
statement in terms of conditional independence statements.

In the case of an undirected graph, it is known (the Hammersleyâ€“Clifford
theorem) that the probability density factors as
\[
    P[X_1, \ldots, X_n] = 
        \prod_{S \in C(G)} P[X_\alpha \mid X_\beta, \beta \in S]
\]
where the $S$ are complete subgraphs (cliques) of $G$.

Graphical models turn up in a number of places -- some include Bayesian networks
(directed) and Markov random fields (undirected) in statistics, Boltzmann
machines in machine learning, and Ising models (particle spins in a lattice) in
statistical mechanics.

\subsection{Boltzmann machines}

A Boltzmann machine factors with respect to an undirected graph, and in addition
assumes that the variables only have pairwise interactions.

Concretely, let the random variable $X = (X_i)_{1 \le i \le n}$ take values in
$\{0,1\}^n$, and let $\theta = (\theta_{ij})$ be a symmetric $n \times n$
matrix.  Then $\theta$ parametrizes a family of probability distributions.

We can regard 
\[
    H_\theta(X) = \sum_{i < j} \theta_{ij} X_i X_j + \sum_i \theta_i X_i
\]
as the energy of the system.  Then the probability of $X$ is
\[
    P_\theta[X] = \frac{\exp(- H_\theta(X))}{Z_\theta}
\]
where $Z_\theta$ is the normalizing factor
\[
    Z_\theta = \sum_{X \in \{0,1\}^n} \exp(-H_\theta(X))))
\]
known as the partition function.

Notice that the `exponent of sum' form of the probability density directly
corresponds to a `product of terms' form, from which we can derive conditional
independence statements if some of the entries $\theta_{ij}$ are zero.

The Boltzmann machine is inspired by ideas statistical mechanics, where the
distribution is known as the Ising model.  Also, restricted Boltzmann machines
(RBMs) have recently become a building block for an important technique in
machine learning -- deep belief networks.

A RBM has some number of `observed' variables $X_1, \ldots, X_m$ and hidden
variables $X_{m+1}, \ldots, X_n$.  The graph for the RBM is the complete
bipartite graph, where every edge is between an observed and hidden variable,
i.e. $\theta_{ij} = 0$ if $i,j \le m$ or $i,j \ge m+1$.

Because the hidden variables are not observed, in order to fit the model's
parameters to the data we consider the marginalized probabilities
\[
    P_\theta[X_1, \ldots, X_m] = \sum_{(X_{m+1}, \ldots, X_n) \in \{0,1\}^{n-m}}
        P_\theta[X_1, \ldots, X_n].
\]
With a fitted parameter $\theta$ and an observed value $X_1, \ldots, X_m$, we can
also compute a probability distribution over the hidden variables; in some
sense, the hidden variables constitute an `explanation' for the data.


\subsection{The (algebraic) geometry of probability models}

A probability distribution over a finite set of size $n$ corresponds with a
point in the \emph{probability simplex}
\[
    \Delta_{n-1} = 
    \left\{(x_1, \ldots, x_{n}) \mid \sum_{i=1}^n x_i= 1, x_i \ge 0 \right\} 
    \cong 
    \left\{(x_1, \ldots, x_n) \mid \sum_{i=1}^{n-1} x_i \le 1, x_i \ge 0 \right\}
\]
of dimension $n-1$.  We can think of a probability model as a subspace 
$\Mod \subset \Delta_{n-1}$.

One way to select a particular probability distribution $p$ from a model $\Mod$
is by \emph{maximum likelihood}.  That is, given observations $X^{(1)}, \ldots,
X^{(N)}$ we attempt to maximize
\[
    L(p \mid X^{(1)}, \ldots, X^{(N)}) = \prod_{i=1}^N p(X^{(i)}
\]
though in practice the negative log-likelihood $l(p, X^{(i)}) = -\log L(p, X^{(i)}$ is
often used (this is a `loss' function, and the losses of the observations are
summed).

From the viewpoint of algebraic geometry, we will usually stipulate that models
are semialgebraic sets, that is, subsets of $\R^n$ that are determined by
polynomial equations and polynomial inequalities.  Real algebraic geometry
(which studies these objects) can be difficult, so sometimes the field is
extended to the complex numbers and/or the Zariski closure of the set is
considered.

Oftentimes, to compute things we will want some rational parametrization $\R^n
\supset U \to \Mod$ of the model.

Using the computational tools available to algebraic geometry (and with problem
sizes, e.g. dimension, that are extremely small) it is sometimes possible to
answer questions like `what is the maximum likelihood parameter' exactly.

The overall hope of approaches like this is (from what I can see) to apply the
tools of geometry to the problems of statistics.  (Compare with `information
geometry', the application of the techniques of differential (esp. Riemannian)
geometry to problems in probability theory, pioneered in the 1980s -- I don't
know how much it has caught on.)  One potential gain from this approach is that
`coordinate-invariance' comes more naturally than usual.


\subsection{Machine learning (and deep learning)}

Machine learning involves algorithmically taking data in order to perform better
at some task.  Abstractly, its goal is similar to that of statistics -- to learn
from data.  The emphasis, however, tends to be toward large data sets, efficient
algorithms (for those large data sets), and fewer assumptions on models
(`non-parametric' statistics).  

Machine learning is studied both by computer scientists and statisticians (who
sometimes call it statistical learning).  (A very good introduction, and one
from which I have taken much of my viewpoint on ML, is \textit{The Elements of
Statistical Learning} by Hastie, Tibshirani, and Friedman -- three
statisticians.)

One very common task is the classification problem.  Here, observations are
points in some (complicated, high-dimensional) space $X$, and the goal is to
assign an observation $x \in X$ to a class label from some finite set $\{A, B,
\ldots \}$.  It can be viewed as a problem where we are attempting to
approximate a function $f : X \to \{A, B, \ldots\}$ given a number of observed
class labels $f(x^{(i)})$.  Of course, this problem is impossible without some very
strong constraints on the type of function (the `model').  You can transform the
classification problem into a problem of estimating a probability distribution
by simply considering the joint distribution over observations and labels.

Most successful algorithms are in some sense `only a step or two away from
linear'.  A basic approach (to, say, binary classification) is to try to
separate the points in the two classes by some hyperplane.  A modification (used
by support vector machines (SVMs)) is to transform the data to some even-higher
dimensional space (infinite, perhaps), and to separate by a hyperplane there.
Another common approach is to have some family of basis functions, and to
consider linear combinations of them.

However, for some problems these techniques may not be enough.  This has
prompted (with some success only in the last ten years?) the creation of
so-called `deep learning' methods, which have `many layers of non-linearity'.
One very important method (perhaps the \emph{most important} so far) is called
the deep belief network and involves stacking restricted Boltzmann machines and
training them greedily.  (Professor Keller, I think, has done some work with
deep belief networks.)  

Deep belief networks can be, however, extremely difficult to optimize.
Monte-Carlo-Markov-Chain (MCMC) techniques are necessary, and esoteric sounding
things like `importance sampling' and `simulated annealing' are used to improve
performance.  (The general subject area is called `stochastic optimization', I
think.  Remember that we do not need exactly optimal solutions, but only
solutions that are `good enough'.)

These and similar methods have performed well on some problems.  One `standard'
example that shows up a lot in papers is the MNIST dataset of handwritten
digits.  In some cases, these algorithms solve an even harder problem than what
humans do -- the `permutation invariant' problem delivers the pixels of the
image as a vector with no a priori knowledge of the geometry of the image.

\subsection{Prior work}

For the algebraic geometry of graphical models, one paper is `On the toric
algebra of graphical models' (Geiger, Meek, Sturmfels -- 2006).  A book-length
account that covers graphical models, among other topics in algebraic
statistics, is `Lectures on Algebraic Statistics' (Drton, Sturmfels, Sullivant
-- 2009).  

Papers that have focused on specific types of graphical models include
`Algebraic Geometry of Bayesian Networks' (Garcia, Stillman, Sturmfels -- 2003)
and `Geometry of the restricted Boltzmann Machine' (Cueto, Morton, and Sturmfels
-- 2009).  

Inspired by questions in phylogenetics, tropical geometry has been applied to
questions in algebraic statistics (incl. graphical models).  See the paper
`Tropical Geometry of Statistical Models' (Pachter, Sturmfels -- 2003) and the
book `Algebraic Statistics for Computational Biology' (Pachter, Sturmfels --
2005).

Another book, which builds theory for considering optimization problems from the
viewpoint of algebraic statistics is `Algebraic Geometry and Statistical
Learning Theory' (Watanabe 2009).

From the machine learning side, questions in deep learning have spawned a large
number of papers, but an early and important one is `A fast learning algorithm for
deep belief nets' (Hinton 2007).  A good survey paper on the need for and
approaches to deep learning is `Learning deep architectures for AI' (Bengio
2009).

One very recent type of graphical model that encompasses models with tractable
partition functions was introduced in `Sum-Product Networks: A New Deep
Architecture' (Domingos, Poon 2011).

\subsection{Where to go from here}

One possibility for a problem to work on is to simply extend Sturmfels' work on
the restricted Boltzmann machine.  There is some distance yet between examining
its geometry and having something useful to say about the optimization problem.

\section{September 16, 2011}


\subsection{The restricted Boltzmann machine}

We describe a restricted Boltzmann machine (RBM); some of this is a repeat from
earlier.

An RBM with $n$ visible variables and $k$ hidden variables is a model with
states $(v, h)$ where $v \in \{0,1\}^n$ and $h \in \{0,1\}^k$.  It is
parametrized by a real $k \times n$ matrix and real vectors $b \in \R^n$, $c \in
\R^k$ (so it has a total of $nk + n + k$ parameters).  The unnormalized joint
distribution is
\[
    \psi(v, h) = \exp(h^T W v + b^Tv + c^T h).
\]
The probability distribution over the visible variables is 
\[
    p(v) = \frac 1 Z \cdot \sum_{h \in \{0,1\}^k} \psi(v, h)
\]
where $Z = \sum_{v,h} \psi(v, h)$ is the \emph{partition function}.  (This
procedure, of defining a joint probability distribution and then
\emph{marginalizing} over hidden variables, is a common one.)

Because the only interactions between variables are pairwise between one visible
and one hidden, the model factors according to the complete bipartite graph
$K_{n,k}$.  (Notice that the terms in the sum inside the $\exp$ become
multiplicative factors.  Thus terms that only involve some subset of variables
correspond to multiplicative factors that only involve those variables --- from
which we can derive the appropriate independence conditions.)

\subsection{Summary of `Geometry of the restricted Boltzmann machine'}

Now to the paper.  In order to move to an algebro-geometric presentation, the
paper uses the parametrization
\[
    \gamma_i = \exp(c_i)
    \qquad
    \omega_{ij} = \exp(W_{ij})
    \qquad
    \beta_j = \exp(b_j)
\]
with which
\[
    \psi(v, h) = \prod_{i=1}^k \gamma_i^{h_i}
        \cdot 
        \prod_{i=1}^k \prod_{j=1}^n \omega_{ij}^{h_i v_j}
        \cdot
        \prod_{j=1}^n \beta_j^{v_j}.
\]
The above is a monomial, and is not actually very scary.  The partition $Z$ is
also polynomial in the parameters, so the map $\R^{nk+n+k} \to
\Delta_{2^n-1}$ from parameters to probability distributions given by $p(v)$ is
then a rational map.  The image $M_n^k \subset \Delta_{2^n - 1}$ of the map is a
semialgebraic set.

The author considers both the semialgebraic set $M_n^k \subset \Delta_{2^n - 1}$
and the variety $V_n^k$ which is the Zariski closure of $M_n^k$ in complex
projective space.  Some results and conjectures on dimension are given.

In the absence of `problems', one would expect that $M_n^k$ has the same
dimension as its preimage $\R^{nk + n + k}$.  The dimension of $M_n^k$ of course
cannot exceed the dimension of its containing space, $2^n - 1$.  Thus the author
conjectures that
\[
    \dim M_n^k = \min\{nk+n+k, 2^n -1\}
\]
in $\Delta_{2^n-1}$, and proves the conjecture for most usual values of $n,k$,
i.e. for $k \le 2^{n - \lceil \log_2(n+1) \rceil}$.

The author also shows that the varieties $M_n^k$ and $V_n^k$ `factor'.
Specifically, they factor as \emph{Hadamard powers}
\[
    V_n^k = (V_n^1)^{[k]}
    \qquad
    M_n^k = (M_n^1)^{[k]}.
\]
Then $M_n^1$ and $V_n^1$ are examined.  One result proved is that $V_n^1$
conincides with the first secant variety of the \emph{Segre embedding} of the
product of projective lines $(\Proj^1)^n$ into $\Proj^{2^n-1}$.  Also, $M_n^1$
can be seen as a \emph{mixture model} of two independence models and has a
`star-shaped' independence graph.

(Both the Hadamard product and the Segre embedding are fairly standard
constructions.)

Another way to look at an RBM (and, indeed, the reason that it is useful for
deep belief networks) is as an `inference function'.  You give it an observed
value for the visible nodes, and you get a conditional probability distribution
over the hidden nodes $p(h \mid v)$.  The value $h \in \{0,1\}^k$ of maximal
likelihood is an `explanation' of the observed datum.

It turns out that the components $h_i$ of $h$ are, viewed as functions of $v$,
`linear threshold functions'.  Basically, the values of $v$ are vertices of a
hypercube, and the decision boundary of whether $h_i = 0$ or $1$ is a hyperplane
through the hypercube.

The author also examines the RBM using tropical geometry, but I am not familiar
enough with it to give a good explanation.

\subsection{Summary of Montufar}

I haven't written this up yet

\subsection{Questions}

How does tropical geometry fit into the picture?  The paper \cite{PS03} may help.

Can any of these results (incl. tropical) help with the training being done with
DBNs, i.e. with stochastic optimization on networks with a large number of
nodes?

As far as where to go with this, one direction is simply to `stack up' the RBMs:
what is the (algebraic) geometry of the deep belief network?  It also looks like
the mixture model perspective helps to understand the RBM -- can in also, from
the alg-geo point of view, help with the DBN?

The variety $\Proj^1 \times \cdots \times \Proj^1$ appears to play an important
role with the RBM \dots.

Another thing to look at:  It appears that the bounds on the number of hidden
units for universal approximators require a model with \emph{much much} more
parameters than are required to solely match the dimension requirements.  When
the RBM/DBN is at full dimension but does not cover the whole range of possible
distributions, what do the `holes' look like?

\section{September 23, 2011}

Short (10 min) presentation to thesis group was last Tuesday.

\subsection{Thoughts on an expository paper}

The presentation I did on Tuesday (with \sout{a few} many expanded topics) could
be one good ordering with which to approach the topic somewhat pedagogically.
That is, the order
\begin{itemize}
\item Brief introduction to algebraic statistics and algebraic geometry (what
are they?)
\item The probability simplex (making probability geometric)
\item Example: the independence model
\item Example: mixture models and hidden values
\item Statistical learning -- models, maximum likelihood, and model selection
criteria
\item Statistical learning -- common optimization algorithms (?? optional ??)
\item The RBM (restricted Boltzmann machine) -- definition
\item The RBM -- marginalization and the interpretation as an inference function
\item The RBM -- its algebraic geometry
\item Graphical models -- a taxonomy, applications (?? optional ??)
\item Graphical models -- computational concerns, and algebraic techniques (?? optional ??)
\item Machine learning -- goals, techniques, and trends (`deep learning')
\item The DBN (deep belief network) -- definition, applications, training techniques
\item The DBN -- the state of the art (unsharp universal approximation results)
\end{itemize}
In particular, independence models and mixture models (in addition to being nice
examples) lead directly to the geometry of the RBM with one hidden node.  The
book \cite{DSS09} is going to be referenced pretty heavily for the more
general material.  I don't know if I'm going to be able to work in
\cite{Wat09} for the model selection stuff -- I probabily will not have the
time to understand it beforehand.

Next week's thesis-ing should mostly be occupied with writing the paper.  Much
of it is already written up in some form or another.  The major parts that are
left are a precise description of a DBN, a precise description of universal
approximation results, and algorithmic things.  I'd like to include enough of
the algorithms to at least get a rough flavor of what's being used in practice
-- I think that that's going to mean at least EM and gradient descent.  

I still also need to get a better handle on the computational algebra portions
-- perhaps read the `toric algebra' paper.  The presence of computational
algebra in the expository paper will likely be pretty light at this point,
however.

As indicated, I'm not sure how much I want to include on other types of
graphical models.  Certainly enough to say `the RBM factors w.r.t a complete
bipartite graph', but more might not be necessary (?).

I should also dig up some more open problems that I'm \emph{not} going to work
on.


\subsection{A closer look at `Geometry of the RBM'}

I've been looking more closely at the proofs in \cite{CMS09}, so I now understand
better his parametrization of the RBM with one hidden node (generating
functions!), and why taking Hadamard powers corresponds to adding new nodes.
Also, the Segre embedding of a product is a natural construction.  I'm still a
bit shaky on secant varieties.

Also, I don't understand enough about some of the computational aspects, and
related algebraic techniques.  E.g. determinental varieties, and why do matrix
minors turn up?

Also, I don't know tropical geometry.

I have a copy of Cox, Little, and O'Shea's \emph{Using Algebraic Geometry}, but
will probably have to cherry-pick topics to study.  There's a chapter on
polytopes and resultants that might (?) be useful.

\subsection{Paper hopping and other miscellany}

Referenced in \cite{CMS09} is the paper \cite{B2} about the geometry binary-valued
graphical tree models  (It's mentioned specifically in the context of the
one-hidden-node RBM, which is a tree.)  Haven't looked too closely yet.

Something I didn't realize before: the directed connections in a DBN are pointed
\emph{towards} the visible nodes.  I guess that makes sense in a way -- the goal
is to get an interesting distribution over the visible nodes, and `more hidden'
layers should correspond to `more abstract' features, which influence layers of
abstraction beneath them.  This direction also plays nicely with marginalizing
away the hidden variables.  Also, it makes sense for generation -- a one-hot
encoding on the classification variables induces a distribution over the visible
nodes, so we have a generative model.

From a greedy-training standpoint, it also makes sense (as it should).  We're
looking (at each layer) for parameters that make the layers we already have more
likely.  Then the causal arrows are pointing towards what we already have.

\subsection{Questions}

Another question -- is model selection even relevant here?  After all, we might
going for something that's a universal approximator, or close.  Or is the
`factor analysis'-like structure the important part?  Or perhaps with the
(sort-of) decomposition of the model, we can say something about model selection
over the constituent parts?  Do these questions even make sense?

What effect do the one-way-directional influences have on a DBN, versus the lack
of them in a deep Boltzmann machine?

No answers yet (for last week's stuff). :-(


\subsection{Goals for next week}
\begin{itemize}
\item Write expository paper (bulk of time)
\item Write precise description of DBN (incl.\ in above)
\item Continue to digest Sturmfels' RBM geometry paper
\item Read toric algebra paper (?)
\end{itemize}

\section{October 14, 2011}

After the expository draft.

For now, it looks like a good idea to focus solely on the RBM (ignoring, e.g.
the DBN).  What useful things can we say about the model?

Should be on the lookout for computational opportunities.  For example, can we
take an RBM for some values of $n$ and $k$ and sample to find out what sort of
volume it occupies in the probability simplex?  

One option would be to sample points from the simplex and then compute whether
or not they lie within the model.  It would be convenient, if, say, we only had
to check a finite number of polynomial equations and polynomial inequalities.
However, we run into the problem of \textbf{implicitization}.  For small models,
it's already a significant challenge.  In fact, \cite{CTY10} is a whole paper on
the techniques necessary to implicitize the model with four visible variables
and two hidden variables (disregarding the inequalities, it's a hypersurface,
i.e. has codimension 1).

Another option would be to sample from the parameter space.  The problem there
is that you don't know what the mapping (from parameters to the model) does to
the closeness of points.  You would (I think) want to be able to compute the
gradient of the map.  However, the difficulty of computing the gradient is
\textbf{precisely} what makes training RBMs take as long as it does.
(Contrastive divergence, etc. is for estimating the gradient, to do gradient
descent).

A problem with all of these approaches (sampling a space to empirically
determine its geometry) is the curse of dimensionality.  Basically, as the
dimension increases, you need exponentially more points to get points to become
close to each other (using the usual euclidian distance).  In this case, the
dimension itself is increasing exponentially!

Another line of attack is to consider simplifications or restrictions of the
model.  For example, if there existed some natural sequence of models $M_1
\subset M_2 \subset \cdots \subset M_n = M$, we could possibly leverage that.

\section{October 21, 2011}

Learning about toric varieties (\cite{CLS09} and \cite{CLO98}) and real
algebraic geometry (\cite{Cos09}) is hard.  Perhaps something to put off till
later?

I think that I better understand the results in \cite{Mon10} on the sizes of
RBMs required for arbitrarily approximating any distribution.  The upper bounds
are by parameter (dimension) counting, which is pretty straightforward.  The
lower bounds are by construction.  Independence models can produce arbitrary
distributions with support over two states, where the two states are bit vectors
that differ by one bit.  There is then a procedure for starting with a
distribution over two of those states, and then with each additional hidden node
extending the distribution to two more states (that differ by one bit).

Montufar's NIPS paper also has results about constructing arbitrary
distributions out of mixtures of independence models.  With arbitrary weights,
it is sufficient to have $2^{n-1}$ factorizing distributions.  This value is
sharp; there exist distributions that require all $2^{n-1}$ components.
It is interesting to note that each one of these components can be very simple;
they're the previously mentioned arbitrary distributions with support over two
states that differ by one bit.

Thus, if we view the visible distribution as a marginalization over hidden
variables, the hidden variables must have at least $2^{n-1}$ states.  You can
check that each conditional probability $p(v \mid h)$ is, in fact, a factorizing
distribution.  Basically, $p(v) \propto \exp(hWv + bv)$, which factors into
parts dependent on components of $v$.

The construction given, however, requires $2^{n-1} - 1$ hidden nodes, which have
$2^{2^{n-1} - 1}$ hidden states, which grows much more rapidly then the
theoretical bound of $2^{n-1}$.  Contributing to this discrepancy is the fact
that neither the factorizing models nor the weights are arbitrary.

Some thing(s) that I did!

From the description of the RBM given in \cite{CMS09}, it is possible to express
the model as a weighted sum of factorizing models, with certain restrictions on
both the weights and the models.  For $i = 1, \ldots, k$, $l = 0,1$, let $a_k^l$
be a factorizing distribution over $n$ binary variables (i.e. $a_k^l$ is in the
independence model $M \subset \Delta_{2^n - 1}$).  Then for $l = (l_1, \ldots,
l_k) \in \{0,1\}^k$, let $b_l$ be the distribution
\[
    b_l = a_1^{l_1} * a_2^{l_1} * \cdots * a_k^{l_k}
\]
wher `$*$' is the Hadamard product (multiply component-wise, then renormalize).
It is fairly straightforward to show that $b_l$ factorizes as an independence
model.  Then any point in the RBM model is of the form
\[
    \sum_{l \in \{0,1\}^k} \pdil*{\prod_{i=1}^k \lambda_i^{l_i} (1 -
    \lambda_i)^{1 - l_i}} b_l
\]
where $\lambda_i \in (0,1)$ are weights.  It's almost as if we have a bunch of
independence models that are weighted together using another independence model.
So there are $2^k$ mixture components $b_l$, but both the components and the
weights are constrained.

In fact, if we take $n=k$ and take $a_i^j$ be the model
\[
    a_i^j(v) = \begin{cases}
        \frac{2}{2^n} & \text{if $v_i = j$}\\
        0 & \text{if $v_i \ne j$}
    \end{cases}
\]
then 
\[
    b_l(v) = \begin{cases}
        1 & \text{if $v = l$}\\
        0 & \text{if $v \ne l$}
    \end{cases}
\]
i.e. the $b_l$ are the delta functions.  If the weights were arbitrary, then all
distributions could be had.  Alas, there are far too few possible weight
combinations; we actually recover a lone independence model (parametrized by
values of $\lambda_i$).

This perspective (parametrization in terms of mixture models) might (?) be
interesting, and we could try the to nest models from here.  Say, what happens
when we perturb one or more of the $a_i^j$'s away from purely discriminating on
one of the bits, into say
\[
    a_i^j(v) = \begin{cases}
        p\frac{2}{2^n} & \text{if $v_i = j$}\\
        (1 - p)\frac{2}{2^n} & \text{if $v_i \ne j$}
    \end{cases}.
\]
The assumption here that $n=k$ is also potentially interesting or slightly
simplifying.  In any case, it's a realistic choice of parameter, as one could
stack identically sized ones of these indefinitely.

\begin{thebibliography}{100}
    \bibitem[Ben09]{Ben09} Bengio.  Learning Deep Architectures for AI. 2009.
    \bibitem[CLO98]{CLO98} Cox, Little, O'Shea.  Using Algebraic Geometry.  1998.
    \bibitem[CLS09]{CLS09} Cox, Little, Schenck.  Toric Varieties.  2009.
    \bibitem[CMS09]{CMS09} Cueto, Morton, Sturmfels. Geometry of the Restricted Boltzmann Machine.  2009.
    \bibitem[Cos09]{Cos09} Michel Coste.  An Introduction to Semialgebraic Geometry.  2002.
    \bibitem[CTY10]{CTY10} Cueto, Tobis, Yu.  An Implicitization Challenge for Binary Factor Analysis. 2010.
    \bibitem[DS98]{DS98} Diaconis and Sturmfels.  Algebraic Algorithms for Sampling
    from Conditional Distributions. 1998.
    \bibitem[DSS09]{DSS09} Drton, Sturmfels, Sullivant. Lectures on Algebraic
    Statistics. 2009.
    \bibitem[GMS06]{GMS06} Geiger, Meed, Sturmfels.  On the Toric Algebra of Graphical Models. 2006.
    \bibitem[Hin07]{Hin07} Hinton.  A Fast Algorithm for Deep Belief Nets.  2007.
    \bibitem[Mon10]{Mon10} Montufar.  Mixture Models and Representational Power of
    RBM's, DBN's and DBM's.  2010.
    \bibitem[PS03]{PS03} Pachter, Sturmfels.  Tropical Geometry of Statistical Models. 2003.
    \bibitem[Ric09]{Ric09} Riccomagno.  A Short History of Algebraic Statistics.  2009.
    \bibitem[Wat09]{Wat09} Watanabe.  Algebraic Geometry and Statistical Learning Theory.  2009.

    \bibitem{A4} Garcia, Stillman, Sturmfels.  Algebraic Geometry of Bayesian Networks.  2003.
    \bibitem{A6} Pachter, Sturmfels.  Algebraic Statistics for Computational Biology.  2005.

    \bibitem{B1} Domingos, Poon. Sum-Product Networks: A New Deep Architecture. 2011.
    \bibitem{B2} Smith, Zwiernik.  The Geometry of Independence Tree Models with Hidden Variables.
\end{thebibliography}


\end{document}
